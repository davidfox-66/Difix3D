{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c16003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Go one level up\n",
    "parent_dir = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "\n",
    "# Add parent directory to Python path\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26cb527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import lpips\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import torchvision\n",
    "import transformers\n",
    "from torchvision.transforms.functional import crop\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from einops import rearrange\n",
    "\n",
    "import diffusers\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "import wandb\n",
    "\n",
    "from src.model import Difix, load_ckpt_from_state_dict, save_ckpt\n",
    "from src.dataset import PairedDataset\n",
    "from src.loss import gram_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e6dc6",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34dda894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d6e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"lambda_lpips\": 1.0,\n",
    "    \"lambda_l2\": 1.0,\n",
    "    \"lambda_gram\": 1.0,\n",
    "    \"gram_loss_warmup_steps\": 2000,\n",
    "    \"dataset_path\": \"data/converted_dataset_fixed.json\",\n",
    "    \"train_image_prep\": \"resized_crop_512\",\n",
    "    \"test_image_prep\": \"resized_crop_512\",\n",
    "    \"prompt\": None,\n",
    "    \"eval_freq\": 100,\n",
    "    \"num_samples_eval\": 100,\n",
    "    \"viz_freq\": 100,\n",
    "    \"tracker_project_name\": \"difix\",\n",
    "    \"tracker_run_name\": \"train\",\n",
    "    \"pretrained_model_name_or_path\": None,\n",
    "    \"revision\": None,\n",
    "    \"variant\": None,\n",
    "    \"tokenizer_name\": None,\n",
    "    \"lora_rank_vae\": 4,\n",
    "    \"timestep\": 199,\n",
    "    \"mv_unet\": False,\n",
    "    \"output_dir\": \"./outputs/difix/train\",\n",
    "    \"cache_dir\": None,\n",
    "    \"seed\": None,\n",
    "    \"resolution\": 512,\n",
    "    \"train_batch_size\": 1,\n",
    "    \"num_training_epochs\": 10,\n",
    "    \"max_train_steps\": 10000,\n",
    "    \"checkpointing_steps\": 500,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"lr_scheduler\": \"constant\",\n",
    "    \"lr_warmup_steps\": 500,\n",
    "    \"lr_num_cycles\": 1,\n",
    "    \"lr_power\": 1.0,\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.999,\n",
    "    \"adam_weight_decay\": 1e-2,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"allow_tf32\": False,\n",
    "    \"report_to\": \"wandb\",\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"enable_xformers_memory_efficient_attention\": True,\n",
    "    \"set_grads_to_none\": False,\n",
    "    \"resume\": None,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "args['output_dir'] = './outputs/difix/train'\n",
    "args['dataset_path'] = \"./data/converted_dataset_fixed.json\"\n",
    "args['max_train_steps'] = 10000 \n",
    "args['resolution'] = 512 \n",
    "args['learning_rate'] = 2e-5 \n",
    "args['train_batch_size'] = 1 \n",
    "args['dataloader_num_workers'] = 4 \n",
    "args['enable_xformers_memory_efficient_attention'] = True\n",
    "args['checkpointing_steps'] = 1000 \n",
    "args['eval_freq'] = 1000 \n",
    "args['viz_freq'] = 100 \n",
    "args['lambda_lpips'] = 1.0 \n",
    "args['lambda_l2'] = 1.0 \n",
    "args['lambda_gram'] =  1.0 \n",
    "args['gram_loss_warmup_steps'] = 2000\n",
    "args['report_to']= \"wandb\" \n",
    "args['tracker_project_name'] = \"difix\" \n",
    "args['tracker_run_name'] = \"train\" \n",
    "args['timestep'] = 199\n",
    "\n",
    "args = SimpleNamespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b5e66b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "{'variance_type', 'clip_sample_range', 'thresholding', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with random weights\n",
      "==================================================\n",
      "Number of trainable parameters in UNet: 865.91M\n",
      "Number of trainable parameters in VAE: 0.52M\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.report_to,\n",
    ")\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(os.path.join(args.output_dir, \"checkpoints\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_dir, \"eval\"), exist_ok=True)\n",
    "\n",
    "### Set Up Model ###\n",
    "net_difix = Difix(\n",
    "    lora_rank_vae=args.lora_rank_vae, \n",
    "    timestep=args.timestep,\n",
    "    mv_unet=args.mv_unet,\n",
    ")\n",
    "net_difix.set_train()\n",
    "\n",
    "if args.enable_xformers_memory_efficient_attention:\n",
    "    if is_xformers_available():\n",
    "        net_difix.unet.enable_xformers_memory_efficient_attention()\n",
    "    else:\n",
    "        raise ValueError(\"xformers is not available, please install it by running `pip install xformers`\")\n",
    "\n",
    "if args.gradient_checkpointing:\n",
    "    net_difix.unet.enable_gradient_checkpointing()\n",
    "\n",
    "if args.allow_tf32:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27fb44a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "### Set up metrics\n",
    "net_lpips = lpips.LPIPS(net='vgg').cuda()\n",
    "\n",
    "net_lpips.requires_grad_(False)\n",
    "\n",
    "net_vgg = torchvision.models.vgg16(pretrained=True).features\n",
    "for param in net_vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3cecf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/torch/_compile.py:51: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#### make the optimizer\n",
    "layers_to_opt = []\n",
    "layers_to_opt += list(net_difix.unet.parameters())\n",
    "\n",
    "for n, _p in net_difix.vae.named_parameters():\n",
    "    if \"lora\" in n and \"vae_skip\" in n:\n",
    "        assert _p.requires_grad\n",
    "        layers_to_opt.append(_p)\n",
    "layers_to_opt = layers_to_opt + list(net_difix.vae.decoder.skip_conv_1.parameters()) + \\\n",
    "    list(net_difix.vae.decoder.skip_conv_2.parameters()) + \\\n",
    "    list(net_difix.vae.decoder.skip_conv_3.parameters()) + \\\n",
    "    list(net_difix.vae.decoder.skip_conv_4.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(layers_to_opt, lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,)\n",
    "lr_scheduler = get_scheduler(args.lr_scheduler, optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=args.max_train_steps * accelerator.num_processes,\n",
    "    num_cycles=args.lr_num_cycles, power=args.lr_power,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386ce608",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up dataset\n",
    "dataset_train = PairedDataset(dataset_path='/mnt/e/Difix3d/data/converted_dataset_fixed.json',\n",
    "                              height=512,\n",
    "                              width=512,\n",
    "                              split=\"train\",\n",
    "                              tokenizer=net_difix.tokenizer)\n",
    "dl_train = torch.utils.data.DataLoader(dataset_train, batch_size=args.train_batch_size, shuffle=True, num_workers=args.dataloader_num_workers)\n",
    "dataset_val = PairedDataset(dataset_path='/mnt/e/Difix3d/data/converted_dataset_fixed.json',\n",
    "                            height=512,\n",
    "                            width=512,\n",
    "                            split=\"test\",\n",
    "                            tokenizer=net_difix.tokenizer)\n",
    "random.Random(42).shuffle(dataset_val.img_ids)\n",
    "dl_val = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40170aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up components on accelerator\n",
    "weight_dtype = torch.float32\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "# Move al networksr to device and cast to weight_dtype\n",
    "net_difix.to(accelerator.device, dtype=weight_dtype)\n",
    "net_lpips.to(accelerator.device, dtype=weight_dtype)\n",
    "net_vgg.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "net_difix, optimizer, dl_train, lr_scheduler = accelerator.prepare(\n",
    "    net_difix, optimizer, dl_train, lr_scheduler\n",
    ")\n",
    "net_lpips, net_vgg = accelerator.prepare(net_lpips, net_vgg)\n",
    "# renorm with image net statistics\n",
    "t_vgg_renorm =  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "548ce092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./outputs/difix/train/wandb/run-20250805_212420-riixlqht</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/davidfox-university-of-bristol/difix/runs/riixlqht' target=\"_blank\">train</a></strong> to <a href='https://wandb.ai/davidfox-university-of-bristol/difix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/davidfox-university-of-bristol/difix' target=\"_blank\">https://wandb.ai/davidfox-university-of-bristol/difix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/davidfox-university-of-bristol/difix/runs/riixlqht' target=\"_blank\">https://wandb.ai/davidfox-university-of-bristol/difix/runs/riixlqht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# tracker and progress bar\n",
    "if accelerator.is_main_process:\n",
    "    init_kwargs = {\n",
    "        \"wandb\": {\n",
    "            \"name\": args.tracker_run_name,\n",
    "            \"dir\": args.output_dir,\n",
    "        },\n",
    "    }        \n",
    "    tracker_config = dict(vars(args))\n",
    "    accelerator.init_trackers(args.tracker_project_name, config=tracker_config, init_kwargs=init_kwargs)\n",
    "\n",
    "progress_bar = tqdm(range(0, args.max_train_steps), initial=0, desc=\"Steps\",\n",
    "    disable=not accelerator.is_local_main_process,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56465805",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563cf1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 1/10000 [00:07<22:03:06,  7.94s/it, loss_gram=0, loss_l2=0.027, loss_lpips=0.512]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Data passed to `wandb.Image` should consist of values in the range [0, 255], image data will be normalized to this range, but behavior will be removed in a future version of wandb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   0%|          | 2/10000 [01:54<183:19:50, 66.01s/it, loss_gram=0, loss_l2=0.0309, loss_lpips=0.318]\n",
      "Steps:   0%|          | 3/10000 [02:02<109:33:28, 39.45s/it, loss_gram=0, loss_l2=0.00927, loss_lpips=0.363]\n",
      "Steps:   0%|          | 4/10000 [02:11<75:58:16, 27.36s/it, loss_gram=0, loss_l2=0.0105, loss_lpips=0.314]  \n",
      "Steps:   0%|          | 5/10000 [02:20<57:46:20, 20.81s/it, loss_gram=0, loss_l2=0.016, loss_lpips=0.304] \n",
      "Steps:   0%|          | 6/10000 [02:29<46:10:44, 16.63s/it, loss_gram=0, loss_l2=0.0233, loss_lpips=0.29]\n",
      "Steps:   0%|          | 7/10000 [02:38<39:17:21, 14.15s/it, loss_gram=0, loss_l2=0.015, loss_lpips=0.484]\n",
      "Steps:   0%|          | 8/10000 [02:46<34:28:40, 12.42s/it, loss_gram=0, loss_l2=0.00506, loss_lpips=0.314]\n",
      "Steps:   0%|          | 9/10000 [02:55<31:16:56, 11.27s/it, loss_gram=0, loss_l2=0.0241, loss_lpips=0.428] \n",
      "Steps:   0%|          | 10/10000 [03:04<28:49:29, 10.39s/it, loss_gram=0, loss_l2=0.00846, loss_lpips=0.239]\n",
      "Steps:   0%|          | 11/10000 [03:12<27:26:57,  9.89s/it, loss_gram=0, loss_l2=0.0128, loss_lpips=0.271] \n",
      "Steps:   0%|          | 12/10000 [03:21<26:06:55,  9.41s/it, loss_gram=0, loss_l2=0.0267, loss_lpips=0.326]\n",
      "Steps:   0%|          | 13/10000 [03:30<25:44:33,  9.28s/it, loss_gram=0, loss_l2=0.032, loss_lpips=0.288] \n",
      "Steps:   0%|          | 14/10000 [03:38<24:59:10,  9.01s/it, loss_gram=0, loss_l2=0.0281, loss_lpips=0.393]\n",
      "Steps:   0%|          | 15/10000 [03:47<25:05:26,  9.05s/it, loss_gram=0, loss_l2=0.0222, loss_lpips=0.346]\n",
      "Steps:   0%|          | 16/10000 [03:56<24:32:35,  8.85s/it, loss_gram=0, loss_l2=0.00926, loss_lpips=0.228]\n",
      "Steps:   0%|          | 17/10000 [04:08<27:18:28,  9.85s/it, loss_gram=0, loss_l2=0.0201, loss_lpips=0.347] \n",
      "Steps:   0%|          | 18/10000 [04:19<28:16:12, 10.20s/it, loss_gram=0, loss_l2=0.0362, loss_lpips=0.366]\n",
      "Steps:   0%|          | 19/10000 [04:31<29:49:06, 10.76s/it, loss_gram=0, loss_l2=0.0184, loss_lpips=0.321]\n",
      "Steps:   0%|          | 20/10000 [04:42<30:24:01, 10.97s/it, loss_gram=0, loss_l2=0.0215, loss_lpips=0.2]  \n",
      "Steps:   0%|          | 21/10000 [04:54<31:22:10, 11.32s/it, loss_gram=0, loss_l2=0.0265, loss_lpips=0.263]\n",
      "Steps:   0%|          | 22/10000 [05:05<31:04:52, 11.21s/it, loss_gram=0, loss_l2=0.0282, loss_lpips=0.253]\n",
      "Steps:   0%|          | 23/10000 [05:17<31:49:49, 11.49s/it, loss_gram=0, loss_l2=0.0119, loss_lpips=0.201]\n",
      "Steps:   0%|          | 24/10000 [05:29<31:49:08, 11.48s/it, loss_gram=0, loss_l2=0.0344, loss_lpips=0.223]\n",
      "Steps:   0%|          | 25/10000 [05:41<32:19:06, 11.66s/it, loss_gram=0, loss_l2=0.00746, loss_lpips=0.271]\n",
      "Steps:   0%|          | 26/10000 [05:52<32:09:03, 11.60s/it, loss_gram=0, loss_l2=0.00378, loss_lpips=0.312]\n",
      "Steps:   0%|          | 27/10000 [06:05<32:41:46, 11.80s/it, loss_gram=0, loss_l2=0.0234, loss_lpips=0.275] \n",
      "Steps:   0%|          | 28/10000 [06:16<32:16:30, 11.65s/it, loss_gram=0, loss_l2=0.0145, loss_lpips=0.258]\n",
      "Steps:   0%|          | 29/10000 [06:28<32:43:11, 11.81s/it, loss_gram=0, loss_l2=0.00535, loss_lpips=0.229]\n",
      "Steps:   0%|          | 30/10000 [06:40<32:34:30, 11.76s/it, loss_gram=0, loss_l2=0.0154, loss_lpips=0.339] \n",
      "Steps:   0%|          | 31/10000 [06:52<33:01:56, 11.93s/it, loss_gram=0, loss_l2=0.0196, loss_lpips=0.248]\n",
      "Steps:   0%|          | 32/10000 [07:04<32:51:41, 11.87s/it, loss_gram=0, loss_l2=0.0197, loss_lpips=0.237]\n",
      "Steps:   0%|          | 33/10000 [07:16<33:03:27, 11.94s/it, loss_gram=0, loss_l2=0.0135, loss_lpips=0.17] \n",
      "Steps:   0%|          | 34/10000 [07:27<32:12:22, 11.63s/it, loss_gram=0, loss_l2=0.021, loss_lpips=0.195]\n",
      "Steps:   0%|          | 35/10000 [07:39<32:50:02, 11.86s/it, loss_gram=0, loss_l2=0.00691, loss_lpips=0.153]\n",
      "Steps:   0%|          | 36/10000 [07:51<32:26:15, 11.72s/it, loss_gram=0, loss_l2=0.0149, loss_lpips=0.203] \n",
      "Steps:   0%|          | 37/10000 [08:03<32:35:25, 11.78s/it, loss_gram=0, loss_l2=0.0238, loss_lpips=0.336]\n",
      "Steps:   0%|          | 38/10000 [08:14<32:11:57, 11.64s/it, loss_gram=0, loss_l2=0.0237, loss_lpips=0.221]\n",
      "Steps:   0%|          | 39/10000 [08:26<32:41:57, 11.82s/it, loss_gram=0, loss_l2=0.00555, loss_lpips=0.288]\n",
      "Steps:   0%|          | 40/10000 [08:38<32:36:25, 11.79s/it, loss_gram=0, loss_l2=0.0259, loss_lpips=0.235] \n",
      "Steps:   0%|          | 41/10000 [08:50<32:38:13, 11.80s/it, loss_gram=0, loss_l2=0.0413, loss_lpips=0.269]\n",
      "Steps:   0%|          | 42/10000 [09:01<32:06:49, 11.61s/it, loss_gram=0, loss_l2=0.0103, loss_lpips=0.271]\n",
      "Steps:   0%|          | 43/10000 [09:13<32:36:26, 11.79s/it, loss_gram=0, loss_l2=0.00253, loss_lpips=0.236]\n",
      "Steps:   0%|          | 44/10000 [09:24<32:12:01, 11.64s/it, loss_gram=0, loss_l2=0.0154, loss_lpips=0.182] \n",
      "Steps:   0%|          | 45/10000 [09:37<32:41:36, 11.82s/it, loss_gram=0, loss_l2=0.0266, loss_lpips=0.258]\n",
      "Steps:   0%|          | 46/10000 [09:48<32:07:47, 11.62s/it, loss_gram=0, loss_l2=0.016, loss_lpips=0.207] \n",
      "Steps:   0%|          | 47/10000 [10:00<32:50:17, 11.88s/it, loss_gram=0, loss_l2=0.0229, loss_lpips=0.214]\n",
      "Steps:   0%|          | 48/10000 [10:12<32:57:57, 11.93s/it, loss_gram=0, loss_l2=0.011, loss_lpips=0.177] \n",
      "Steps:   0%|          | 49/10000 [10:25<33:29:51, 12.12s/it, loss_gram=0, loss_l2=0.0297, loss_lpips=0.27]\n",
      "Steps:   0%|          | 50/10000 [10:37<33:13:18, 12.02s/it, loss_gram=0, loss_l2=0.0362, loss_lpips=0.245]\n",
      "Steps:   1%|          | 51/10000 [10:50<34:11:06, 12.37s/it, loss_gram=0, loss_l2=0.0394, loss_lpips=0.258]\n",
      "Steps:   1%|          | 52/10000 [11:02<33:48:28, 12.23s/it, loss_gram=0, loss_l2=0.036, loss_lpips=0.24]  \n",
      "Steps:   1%|          | 53/10000 [11:15<34:27:30, 12.47s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.172]\n",
      "Steps:   1%|          | 54/10000 [11:27<34:01:32, 12.32s/it, loss_gram=0, loss_l2=0.00199, loss_lpips=0.177]\n",
      "Steps:   1%|          | 55/10000 [11:40<34:49:32, 12.61s/it, loss_gram=0, loss_l2=0.0185, loss_lpips=0.211] \n",
      "Steps:   1%|          | 56/10000 [11:52<34:05:28, 12.34s/it, loss_gram=0, loss_l2=0.0124, loss_lpips=0.213]\n",
      "Steps:   1%|          | 57/10000 [12:04<34:00:27, 12.31s/it, loss_gram=0, loss_l2=0.0202, loss_lpips=0.194]\n",
      "Steps:   1%|          | 58/10000 [12:15<32:43:02, 11.85s/it, loss_gram=0, loss_l2=0.0076, loss_lpips=0.173]\n",
      "Steps:   1%|          | 59/10000 [12:27<32:44:11, 11.86s/it, loss_gram=0, loss_l2=0.0357, loss_lpips=0.253]\n",
      "Steps:   1%|          | 60/10000 [12:38<32:32:04, 11.78s/it, loss_gram=0, loss_l2=0.0379, loss_lpips=0.237]\n",
      "Steps:   1%|          | 61/10000 [12:50<32:42:39, 11.85s/it, loss_gram=0, loss_l2=0.0145, loss_lpips=0.232]\n",
      "Steps:   1%|          | 62/10000 [13:02<32:33:05, 11.79s/it, loss_gram=0, loss_l2=0.00386, loss_lpips=0.176]\n",
      "Steps:   1%|          | 63/10000 [13:14<32:43:04, 11.85s/it, loss_gram=0, loss_l2=0.014, loss_lpips=0.206]  \n",
      "Steps:   1%|          | 64/10000 [13:25<32:09:12, 11.65s/it, loss_gram=0, loss_l2=0.0211, loss_lpips=0.222]\n",
      "Steps:   1%|          | 65/10000 [13:38<32:48:01, 11.89s/it, loss_gram=0, loss_l2=0.012, loss_lpips=0.138] \n",
      "Steps:   1%|          | 66/10000 [13:53<35:24:36, 12.83s/it, loss_gram=0, loss_l2=0.0355, loss_lpips=0.211]\n",
      "Steps:   1%|          | 67/10000 [14:12<40:31:17, 14.69s/it, loss_gram=0, loss_l2=0.0156, loss_lpips=0.167]\n",
      "Steps:   1%|          | 68/10000 [14:33<45:37:43, 16.54s/it, loss_gram=0, loss_l2=0.0043, loss_lpips=0.112]\n",
      "Steps:   1%|          | 69/10000 [14:54<49:48:47, 18.06s/it, loss_gram=0, loss_l2=0.03, loss_lpips=0.203]  \n",
      "Steps:   1%|          | 70/10000 [15:14<51:35:37, 18.70s/it, loss_gram=0, loss_l2=0.0854, loss_lpips=0.408]\n",
      "Steps:   1%|          | 71/10000 [15:33<51:40:27, 18.74s/it, loss_gram=0, loss_l2=0.00927, loss_lpips=0.218]\n",
      "Steps:   1%|          | 72/10000 [15:53<52:46:14, 19.14s/it, loss_gram=0, loss_l2=0.00507, loss_lpips=0.1]  \n",
      "Steps:   1%|          | 73/10000 [16:13<53:33:43, 19.42s/it, loss_gram=0, loss_l2=0.00234, loss_lpips=0.177]\n",
      "Steps:   1%|          | 74/10000 [16:33<53:50:52, 19.53s/it, loss_gram=0, loss_l2=0.0265, loss_lpips=0.173] \n",
      "Steps:   1%|          | 75/10000 [16:52<53:16:05, 19.32s/it, loss_gram=0, loss_l2=0.00762, loss_lpips=0.128]\n",
      "Steps:   1%|          | 76/10000 [17:12<53:36:52, 19.45s/it, loss_gram=0, loss_l2=0.0021, loss_lpips=0.0977]\n",
      "Steps:   1%|          | 77/10000 [17:33<54:51:39, 19.90s/it, loss_gram=0, loss_l2=0.0195, loss_lpips=0.216] \n",
      "Steps:   1%|          | 78/10000 [17:55<56:50:41, 20.63s/it, loss_gram=0, loss_l2=0.00197, loss_lpips=0.126]\n",
      "Steps:   1%|          | 79/10000 [18:14<55:44:58, 20.23s/it, loss_gram=0, loss_l2=0.012, loss_lpips=0.134]  \n",
      "Steps:   1%|          | 80/10000 [18:36<56:40:37, 20.57s/it, loss_gram=0, loss_l2=0.00707, loss_lpips=0.164]\n",
      "Steps:   1%|          | 81/10000 [18:57<57:22:36, 20.82s/it, loss_gram=0, loss_l2=0.015, loss_lpips=0.178]  \n",
      "Steps:   1%|          | 82/10000 [19:17<56:13:33, 20.41s/it, loss_gram=0, loss_l2=0.007, loss_lpips=0.162]\n",
      "Steps:   1%|          | 83/10000 [19:37<56:03:10, 20.35s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.149]\n",
      "Steps:   1%|          | 84/10000 [19:58<56:49:19, 20.63s/it, loss_gram=0, loss_l2=0.0128, loss_lpips=0.21] \n",
      "Steps:   1%|          | 85/10000 [20:18<56:31:38, 20.52s/it, loss_gram=0, loss_l2=0.0108, loss_lpips=0.195]\n",
      "Steps:   1%|          | 86/10000 [20:39<56:00:04, 20.34s/it, loss_gram=0, loss_l2=0.00919, loss_lpips=0.26]\n",
      "Steps:   1%|          | 87/10000 [20:58<55:42:35, 20.23s/it, loss_gram=0, loss_l2=0.01, loss_lpips=0.188]  \n",
      "Steps:   1%|          | 88/10000 [21:19<56:05:43, 20.37s/it, loss_gram=0, loss_l2=0.0116, loss_lpips=0.234]\n",
      "Steps:   1%|          | 89/10000 [21:40<57:00:37, 20.71s/it, loss_gram=0, loss_l2=0.00511, loss_lpips=0.123]\n",
      "Steps:   1%|          | 90/10000 [22:00<55:47:38, 20.27s/it, loss_gram=0, loss_l2=0.00114, loss_lpips=0.1]  \n",
      "Steps:   1%|          | 91/10000 [22:21<56:28:47, 20.52s/it, loss_gram=0, loss_l2=0.0159, loss_lpips=0.202]\n",
      "Steps:   1%|          | 92/10000 [22:41<56:06:48, 20.39s/it, loss_gram=0, loss_l2=0.0171, loss_lpips=0.149]\n",
      "Steps:   1%|          | 93/10000 [23:01<55:46:43, 20.27s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.146]\n",
      "Steps:   1%|          | 94/10000 [23:19<54:01:18, 19.63s/it, loss_gram=0, loss_l2=0.00309, loss_lpips=0.114]\n",
      "Steps:   1%|          | 95/10000 [23:41<56:22:24, 20.49s/it, loss_gram=0, loss_l2=0.0194, loss_lpips=0.169] \n",
      "Steps:   1%|          | 96/10000 [24:01<56:10:29, 20.42s/it, loss_gram=0, loss_l2=0.00442, loss_lpips=0.242]\n",
      "Steps:   1%|          | 97/10000 [24:21<55:31:36, 20.19s/it, loss_gram=0, loss_l2=0.0709, loss_lpips=0.179] \n",
      "Steps:   1%|          | 98/10000 [24:41<54:43:15, 19.89s/it, loss_gram=0, loss_l2=0.00603, loss_lpips=0.165]\n",
      "Steps:   1%|          | 99/10000 [25:02<56:00:45, 20.37s/it, loss_gram=0, loss_l2=0.00617, loss_lpips=0.118]\n",
      "Steps:   1%|          | 100/10000 [25:22<56:09:13, 20.42s/it, loss_gram=0, loss_l2=0.00907, loss_lpips=0.117]\n",
      "Steps:   1%|          | 101/10000 [25:42<55:42:16, 20.26s/it, loss_gram=0, loss_l2=0.0198, loss_lpips=0.195] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   1%|          | 102/10000 [26:05<57:02:15, 20.75s/it, loss_gram=0, loss_l2=0.00172, loss_lpips=0.134]\n",
      "Steps:   1%|          | 103/10000 [26:25<57:10:25, 20.80s/it, loss_gram=0, loss_l2=0.0134, loss_lpips=0.125] \n",
      "Steps:   1%|          | 104/10000 [26:46<57:09:27, 20.79s/it, loss_gram=0, loss_l2=0.0194, loss_lpips=0.149]\n",
      "Steps:   1%|          | 105/10000 [27:07<57:38:42, 20.97s/it, loss_gram=0, loss_l2=0.00184, loss_lpips=0.157]\n",
      "Steps:   1%|          | 106/10000 [27:27<56:17:20, 20.48s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.142] \n",
      "Steps:   1%|          | 107/10000 [27:48<57:06:40, 20.78s/it, loss_gram=0, loss_l2=0.00433, loss_lpips=0.147]\n",
      "Steps:   1%|          | 108/10000 [28:09<56:54:11, 20.71s/it, loss_gram=0, loss_l2=0.00706, loss_lpips=0.101]\n",
      "Steps:   1%|          | 109/10000 [28:28<55:41:17, 20.27s/it, loss_gram=0, loss_l2=0.0111, loss_lpips=0.148] \n",
      "Steps:   1%|          | 110/10000 [28:47<54:40:51, 19.90s/it, loss_gram=0, loss_l2=0.00485, loss_lpips=0.196]\n",
      "Steps:   1%|          | 111/10000 [29:09<56:17:53, 20.49s/it, loss_gram=0, loss_l2=0.0188, loss_lpips=0.175] \n",
      "Steps:   1%|          | 112/10000 [29:30<56:40:24, 20.63s/it, loss_gram=0, loss_l2=0.00979, loss_lpips=0.154]\n",
      "Steps:   1%|          | 113/10000 [29:50<56:48:37, 20.69s/it, loss_gram=0, loss_l2=0.0364, loss_lpips=0.154] \n",
      "Steps:   1%|          | 114/10000 [30:11<56:25:30, 20.55s/it, loss_gram=0, loss_l2=0.0496, loss_lpips=0.229]\n",
      "Steps:   1%|          | 115/10000 [30:31<56:05:47, 20.43s/it, loss_gram=0, loss_l2=0.00369, loss_lpips=0.104]\n",
      "Steps:   1%|          | 116/10000 [30:52<56:05:44, 20.43s/it, loss_gram=0, loss_l2=0.0186, loss_lpips=0.171] \n",
      "Steps:   1%|          | 117/10000 [31:11<55:43:53, 20.30s/it, loss_gram=0, loss_l2=0.00308, loss_lpips=0.114]\n",
      "Steps:   1%|          | 118/10000 [31:32<56:11:16, 20.47s/it, loss_gram=0, loss_l2=0.00509, loss_lpips=0.129]\n",
      "Steps:   1%|          | 119/10000 [31:52<55:55:50, 20.38s/it, loss_gram=0, loss_l2=0.0151, loss_lpips=0.21]  \n",
      "Steps:   1%|          | 120/10000 [32:14<57:04:46, 20.80s/it, loss_gram=0, loss_l2=0.00229, loss_lpips=0.197]\n",
      "Steps:   1%|          | 121/10000 [32:35<57:12:05, 20.84s/it, loss_gram=0, loss_l2=0.0198, loss_lpips=0.192] \n",
      "Steps:   1%|          | 122/10000 [32:56<57:00:28, 20.78s/it, loss_gram=0, loss_l2=0.00834, loss_lpips=0.168]\n",
      "Steps:   1%|          | 123/10000 [33:15<55:26:41, 20.21s/it, loss_gram=0, loss_l2=0.0315, loss_lpips=0.185] \n",
      "Steps:   1%|          | 124/10000 [33:34<54:52:18, 20.00s/it, loss_gram=0, loss_l2=0.0218, loss_lpips=0.158]\n",
      "Steps:   1%|▏         | 125/10000 [33:54<55:07:27, 20.10s/it, loss_gram=0, loss_l2=0.0406, loss_lpips=0.163]\n",
      "Steps:   1%|▏         | 126/10000 [34:14<54:20:38, 19.81s/it, loss_gram=0, loss_l2=0.0322, loss_lpips=0.222]\n",
      "Steps:   1%|▏         | 127/10000 [34:34<54:52:05, 20.01s/it, loss_gram=0, loss_l2=0.00378, loss_lpips=0.111]\n",
      "Steps:   1%|▏         | 128/10000 [34:55<55:37:50, 20.29s/it, loss_gram=0, loss_l2=0.0122, loss_lpips=0.129] \n",
      "Steps:   1%|▏         | 129/10000 [35:16<56:07:12, 20.47s/it, loss_gram=0, loss_l2=0.00651, loss_lpips=0.141]\n",
      "Steps:   1%|▏         | 130/10000 [35:36<56:12:12, 20.50s/it, loss_gram=0, loss_l2=0.00907, loss_lpips=0.0996]\n",
      "Steps:   1%|▏         | 131/10000 [35:57<55:55:11, 20.40s/it, loss_gram=0, loss_l2=0.00407, loss_lpips=0.095] \n",
      "Steps:   1%|▏         | 132/10000 [36:18<56:34:42, 20.64s/it, loss_gram=0, loss_l2=0.0138, loss_lpips=0.163] \n",
      "Steps:   1%|▏         | 133/10000 [36:39<56:48:53, 20.73s/it, loss_gram=0, loss_l2=0.00186, loss_lpips=0.219]\n",
      "Steps:   1%|▏         | 134/10000 [37:00<57:04:25, 20.83s/it, loss_gram=0, loss_l2=0.00833, loss_lpips=0.0948]\n",
      "Steps:   1%|▏         | 135/10000 [37:19<55:46:44, 20.36s/it, loss_gram=0, loss_l2=0.0123, loss_lpips=0.114]  \n",
      "Steps:   1%|▏         | 136/10000 [37:39<55:32:41, 20.27s/it, loss_gram=0, loss_l2=0.00307, loss_lpips=0.0885]\n",
      "Steps:   1%|▏         | 137/10000 [37:58<54:10:36, 19.77s/it, loss_gram=0, loss_l2=0.0182, loss_lpips=0.241]  \n",
      "Steps:   1%|▏         | 138/10000 [38:17<53:32:13, 19.54s/it, loss_gram=0, loss_l2=0.0159, loss_lpips=0.193]\n",
      "Steps:   1%|▏         | 139/10000 [38:36<52:55:42, 19.32s/it, loss_gram=0, loss_l2=0.0312, loss_lpips=0.147]\n",
      "Steps:   1%|▏         | 140/10000 [38:57<54:18:38, 19.83s/it, loss_gram=0, loss_l2=0.00652, loss_lpips=0.122]\n",
      "Steps:   1%|▏         | 141/10000 [39:16<53:49:55, 19.66s/it, loss_gram=0, loss_l2=0.0353, loss_lpips=0.177] \n",
      "Steps:   1%|▏         | 142/10000 [39:37<54:56:20, 20.06s/it, loss_gram=0, loss_l2=0.0173, loss_lpips=0.209]\n",
      "Steps:   1%|▏         | 143/10000 [39:58<56:06:53, 20.49s/it, loss_gram=0, loss_l2=0.00824, loss_lpips=0.163]\n",
      "Steps:   1%|▏         | 144/10000 [40:19<56:17:17, 20.56s/it, loss_gram=0, loss_l2=0.0391, loss_lpips=0.161] \n",
      "Steps:   1%|▏         | 145/10000 [40:41<57:19:29, 20.94s/it, loss_gram=0, loss_l2=0.0459, loss_lpips=0.285]\n",
      "Steps:   1%|▏         | 146/10000 [41:02<57:20:53, 20.95s/it, loss_gram=0, loss_l2=0.00199, loss_lpips=0.0852]\n",
      "Steps:   1%|▏         | 147/10000 [41:23<56:50:47, 20.77s/it, loss_gram=0, loss_l2=0.00136, loss_lpips=0.159] \n",
      "Steps:   1%|▏         | 148/10000 [41:43<56:49:21, 20.76s/it, loss_gram=0, loss_l2=0.0352, loss_lpips=0.104] \n",
      "Steps:   1%|▏         | 149/10000 [42:03<56:18:50, 20.58s/it, loss_gram=0, loss_l2=0.00919, loss_lpips=0.197]\n",
      "Steps:   2%|▏         | 150/10000 [42:25<56:58:50, 20.83s/it, loss_gram=0, loss_l2=0.00751, loss_lpips=0.26] \n",
      "Steps:   2%|▏         | 151/10000 [42:46<57:39:34, 21.08s/it, loss_gram=0, loss_l2=0.117, loss_lpips=0.321] \n",
      "Steps:   2%|▏         | 152/10000 [43:07<57:14:19, 20.92s/it, loss_gram=0, loss_l2=0.0101, loss_lpips=0.125]\n",
      "Steps:   2%|▏         | 153/10000 [43:27<56:24:44, 20.62s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.198]\n",
      "Steps:   2%|▏         | 154/10000 [43:47<55:52:48, 20.43s/it, loss_gram=0, loss_l2=0.0278, loss_lpips=0.19] \n",
      "Steps:   2%|▏         | 155/10000 [44:09<57:51:41, 21.16s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.14]\n",
      "Steps:   2%|▏         | 156/10000 [44:30<57:21:12, 20.97s/it, loss_gram=0, loss_l2=0.014, loss_lpips=0.208]\n",
      "Steps:   2%|▏         | 157/10000 [44:51<56:48:21, 20.78s/it, loss_gram=0, loss_l2=0.0216, loss_lpips=0.163]\n",
      "Steps:   2%|▏         | 158/10000 [45:13<57:55:27, 21.19s/it, loss_gram=0, loss_l2=0.00488, loss_lpips=0.15]\n",
      "Steps:   2%|▏         | 159/10000 [45:35<58:45:29, 21.49s/it, loss_gram=0, loss_l2=0.0173, loss_lpips=0.194]\n",
      "Steps:   2%|▏         | 160/10000 [45:56<58:38:26, 21.45s/it, loss_gram=0, loss_l2=0.0224, loss_lpips=0.179]\n",
      "Steps:   2%|▏         | 161/10000 [46:17<58:04:41, 21.25s/it, loss_gram=0, loss_l2=0.0109, loss_lpips=0.149]\n",
      "Steps:   2%|▏         | 162/10000 [46:36<56:31:07, 20.68s/it, loss_gram=0, loss_l2=0.0106, loss_lpips=0.141]\n",
      "Steps:   2%|▏         | 163/10000 [46:57<56:20:58, 20.62s/it, loss_gram=0, loss_l2=0.00608, loss_lpips=0.227]\n",
      "Steps:   2%|▏         | 164/10000 [47:19<57:48:33, 21.16s/it, loss_gram=0, loss_l2=0.00791, loss_lpips=0.123]\n",
      "Steps:   2%|▏         | 165/10000 [47:40<57:07:05, 20.91s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.138] \n",
      "Steps:   2%|▏         | 166/10000 [47:59<56:13:39, 20.58s/it, loss_gram=0, loss_l2=0.00418, loss_lpips=0.109]\n",
      "Steps:   2%|▏         | 167/10000 [48:20<56:07:50, 20.55s/it, loss_gram=0, loss_l2=0.00252, loss_lpips=0.19] \n",
      "Steps:   2%|▏         | 168/10000 [48:41<56:44:48, 20.78s/it, loss_gram=0, loss_l2=0.015, loss_lpips=0.193] \n",
      "Steps:   2%|▏         | 169/10000 [49:01<56:03:54, 20.53s/it, loss_gram=0, loss_l2=0.0128, loss_lpips=0.118]\n",
      "Steps:   2%|▏         | 170/10000 [49:20<54:46:22, 20.06s/it, loss_gram=0, loss_l2=0.0137, loss_lpips=0.163]\n",
      "Steps:   2%|▏         | 171/10000 [49:40<54:56:45, 20.12s/it, loss_gram=0, loss_l2=0.00862, loss_lpips=0.115]\n",
      "Steps:   2%|▏         | 172/10000 [50:03<56:35:21, 20.73s/it, loss_gram=0, loss_l2=0.0114, loss_lpips=0.164] \n",
      "Steps:   2%|▏         | 173/10000 [50:22<55:16:08, 20.25s/it, loss_gram=0, loss_l2=0.00398, loss_lpips=0.0946]\n",
      "Steps:   2%|▏         | 174/10000 [50:43<55:47:02, 20.44s/it, loss_gram=0, loss_l2=0.0251, loss_lpips=0.187]  \n",
      "Steps:   2%|▏         | 175/10000 [51:06<57:49:56, 21.19s/it, loss_gram=0, loss_l2=0.0195, loss_lpips=0.201]\n",
      "Steps:   2%|▏         | 176/10000 [51:24<55:25:54, 20.31s/it, loss_gram=0, loss_l2=0.0148, loss_lpips=0.139]\n",
      "Steps:   2%|▏         | 177/10000 [51:44<55:43:54, 20.42s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.223]\n",
      "Steps:   2%|▏         | 178/10000 [52:06<55:57:15, 20.51s/it, loss_gram=0, loss_l2=0.00702, loss_lpips=0.132]\n",
      "Steps:   2%|▏         | 179/10000 [52:27<57:05:02, 20.92s/it, loss_gram=0, loss_l2=0.0281, loss_lpips=0.18]  \n",
      "Steps:   2%|▏         | 180/10000 [52:48<57:21:48, 21.03s/it, loss_gram=0, loss_l2=0.00209, loss_lpips=0.134]\n",
      "Steps:   2%|▏         | 181/10000 [53:10<57:43:33, 21.16s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.144] \n",
      "Steps:   2%|▏         | 182/10000 [53:30<57:20:08, 21.02s/it, loss_gram=0, loss_l2=0.0152, loss_lpips=0.133]\n",
      "Steps:   2%|▏         | 183/10000 [53:50<56:20:41, 20.66s/it, loss_gram=0, loss_l2=0.0135, loss_lpips=0.202]\n",
      "Steps:   2%|▏         | 184/10000 [54:12<56:47:32, 20.83s/it, loss_gram=0, loss_l2=0.00795, loss_lpips=0.156]\n",
      "Steps:   2%|▏         | 185/10000 [54:32<56:19:49, 20.66s/it, loss_gram=0, loss_l2=0.0079, loss_lpips=0.193] \n",
      "Steps:   2%|▏         | 186/10000 [54:51<55:19:48, 20.30s/it, loss_gram=0, loss_l2=0.00456, loss_lpips=0.0906]\n",
      "Steps:   2%|▏         | 187/10000 [55:14<57:02:44, 20.93s/it, loss_gram=0, loss_l2=0.00408, loss_lpips=0.167] \n",
      "Steps:   2%|▏         | 188/10000 [55:34<57:01:42, 20.92s/it, loss_gram=0, loss_l2=0.00407, loss_lpips=0.111]\n",
      "Steps:   2%|▏         | 189/10000 [55:53<55:10:13, 20.24s/it, loss_gram=0, loss_l2=0.00962, loss_lpips=0.155]\n",
      "Steps:   2%|▏         | 190/10000 [56:15<56:26:58, 20.72s/it, loss_gram=0, loss_l2=0.00915, loss_lpips=0.116]\n",
      "Steps:   2%|▏         | 191/10000 [56:36<57:00:03, 20.92s/it, loss_gram=0, loss_l2=0.0201, loss_lpips=0.144] \n",
      "Steps:   2%|▏         | 192/10000 [56:57<56:31:24, 20.75s/it, loss_gram=0, loss_l2=0.0132, loss_lpips=0.131]\n",
      "Steps:   2%|▏         | 193/10000 [57:18<56:42:51, 20.82s/it, loss_gram=0, loss_l2=0.0184, loss_lpips=0.122]\n",
      "Steps:   2%|▏         | 194/10000 [57:39<57:20:13, 21.05s/it, loss_gram=0, loss_l2=0.0616, loss_lpips=0.215]\n",
      "Steps:   2%|▏         | 195/10000 [58:03<58:20:02, 21.42s/it, loss_gram=0, loss_l2=0.00846, loss_lpips=0.103]\n",
      "Steps:   2%|▏         | 196/10000 [58:29<63:35:35, 23.35s/it, loss_gram=0, loss_l2=0.0275, loss_lpips=0.168] \n",
      "Steps:   2%|▏         | 197/10000 [58:55<65:41:47, 24.13s/it, loss_gram=0, loss_l2=0.00456, loss_lpips=0.116]\n",
      "Steps:   2%|▏         | 198/10000 [59:20<65:59:55, 24.24s/it, loss_gram=0, loss_l2=0.0201, loss_lpips=0.232] \n",
      "Steps:   2%|▏         | 199/10000 [59:46<67:13:31, 24.69s/it, loss_gram=0, loss_l2=0.009, loss_lpips=0.117] \n",
      "Steps:   2%|▏         | 200/10000 [1:00:11<67:58:53, 24.97s/it, loss_gram=0, loss_l2=0.0287, loss_lpips=0.148]\n",
      "Steps:   2%|▏         | 201/10000 [1:00:32<64:50:00, 23.82s/it, loss_gram=0, loss_l2=0.0251, loss_lpips=0.199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   2%|▏         | 202/10000 [1:00:41<52:37:03, 19.33s/it, loss_gram=0, loss_l2=0.00697, loss_lpips=0.151]\n",
      "Steps:   2%|▏         | 203/10000 [1:00:50<44:01:52, 16.18s/it, loss_gram=0, loss_l2=0.00725, loss_lpips=0.188]\n",
      "Steps:   2%|▏         | 204/10000 [1:00:58<37:44:18, 13.87s/it, loss_gram=0, loss_l2=0.00578, loss_lpips=0.143]\n",
      "Steps:   2%|▏         | 205/10000 [1:01:07<33:36:13, 12.35s/it, loss_gram=0, loss_l2=0.0208, loss_lpips=0.147] \n",
      "Steps:   2%|▏         | 206/10000 [1:01:16<30:20:24, 11.15s/it, loss_gram=0, loss_l2=0.00121, loss_lpips=0.0914]\n",
      "Steps:   2%|▏         | 207/10000 [1:01:24<28:14:39, 10.38s/it, loss_gram=0, loss_l2=0.00926, loss_lpips=0.0989]\n",
      "Steps:   2%|▏         | 208/10000 [1:01:33<26:40:19,  9.81s/it, loss_gram=0, loss_l2=0.00221, loss_lpips=0.0842]\n",
      "Steps:   2%|▏         | 209/10000 [1:01:41<25:46:22,  9.48s/it, loss_gram=0, loss_l2=0.0203, loss_lpips=0.221]  \n",
      "Steps:   2%|▏         | 210/10000 [1:01:50<24:56:28,  9.17s/it, loss_gram=0, loss_l2=0.00979, loss_lpips=0.161]\n",
      "Steps:   2%|▏         | 211/10000 [1:01:59<24:35:33,  9.04s/it, loss_gram=0, loss_l2=0.0057, loss_lpips=0.142] \n",
      "Steps:   2%|▏         | 212/10000 [1:02:07<24:05:40,  8.86s/it, loss_gram=0, loss_l2=0.0077, loss_lpips=0.167]\n",
      "Steps:   2%|▏         | 213/10000 [1:02:16<24:01:47,  8.84s/it, loss_gram=0, loss_l2=0.00232, loss_lpips=0.104]\n",
      "Steps:   2%|▏         | 214/10000 [1:02:24<23:47:19,  8.75s/it, loss_gram=0, loss_l2=0.000776, loss_lpips=0.137]\n",
      "Steps:   2%|▏         | 215/10000 [1:02:33<23:42:40,  8.72s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.156]  \n",
      "Steps:   2%|▏         | 216/10000 [1:02:41<23:32:29,  8.66s/it, loss_gram=0, loss_l2=0.00901, loss_lpips=0.133]\n",
      "Steps:   2%|▏         | 217/10000 [1:02:50<23:30:43,  8.65s/it, loss_gram=0, loss_l2=0.0094, loss_lpips=0.109] \n",
      "Steps:   2%|▏         | 218/10000 [1:02:59<23:22:28,  8.60s/it, loss_gram=0, loss_l2=0.00162, loss_lpips=0.126]\n",
      "Steps:   2%|▏         | 219/10000 [1:03:07<23:23:49,  8.61s/it, loss_gram=0, loss_l2=0.0205, loss_lpips=0.129] \n",
      "Steps:   2%|▏         | 220/10000 [1:03:16<23:17:16,  8.57s/it, loss_gram=0, loss_l2=0.0053, loss_lpips=0.0952]\n",
      "Steps:   2%|▏         | 221/10000 [1:03:24<23:18:54,  8.58s/it, loss_gram=0, loss_l2=0.0162, loss_lpips=0.172] \n",
      "Steps:   2%|▏         | 222/10000 [1:03:33<23:17:26,  8.57s/it, loss_gram=0, loss_l2=0.00921, loss_lpips=0.137]\n",
      "Steps:   2%|▏         | 223/10000 [1:03:41<23:19:55,  8.59s/it, loss_gram=0, loss_l2=0.00784, loss_lpips=0.127]\n",
      "Steps:   2%|▏         | 224/10000 [1:03:50<23:18:17,  8.58s/it, loss_gram=0, loss_l2=0.0178, loss_lpips=0.211] \n",
      "Steps:   2%|▏         | 225/10000 [1:03:59<23:19:57,  8.59s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.188]\n",
      "Steps:   2%|▏         | 226/10000 [1:04:07<23:16:06,  8.57s/it, loss_gram=0, loss_l2=0.0278, loss_lpips=0.198]\n",
      "Steps:   2%|▏         | 227/10000 [1:04:16<23:24:20,  8.62s/it, loss_gram=0, loss_l2=0.0162, loss_lpips=0.163]\n",
      "Steps:   2%|▏         | 228/10000 [1:04:24<23:19:12,  8.59s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.151]\n",
      "Steps:   2%|▏         | 229/10000 [1:04:33<23:30:41,  8.66s/it, loss_gram=0, loss_l2=0.022, loss_lpips=0.189] \n",
      "Steps:   2%|▏         | 230/10000 [1:04:42<23:20:05,  8.60s/it, loss_gram=0, loss_l2=0.00499, loss_lpips=0.13]\n",
      "Steps:   2%|▏         | 231/10000 [1:04:50<23:29:03,  8.65s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.143]\n",
      "Steps:   2%|▏         | 232/10000 [1:04:59<23:22:20,  8.61s/it, loss_gram=0, loss_l2=0.00474, loss_lpips=0.119]\n",
      "Steps:   2%|▏         | 233/10000 [1:05:08<23:30:53,  8.67s/it, loss_gram=0, loss_l2=0.0206, loss_lpips=0.151] \n",
      "Steps:   2%|▏         | 234/10000 [1:05:16<23:24:36,  8.63s/it, loss_gram=0, loss_l2=0.00558, loss_lpips=0.158]\n",
      "Steps:   2%|▏         | 235/10000 [1:05:25<23:29:58,  8.66s/it, loss_gram=0, loss_l2=0.0349, loss_lpips=0.224] \n",
      "Steps:   2%|▏         | 236/10000 [1:05:34<23:23:50,  8.63s/it, loss_gram=0, loss_l2=0.101, loss_lpips=0.23]  \n",
      "Steps:   2%|▏         | 237/10000 [1:05:42<23:26:09,  8.64s/it, loss_gram=0, loss_l2=0.0229, loss_lpips=0.165]\n",
      "Steps:   2%|▏         | 238/10000 [1:05:51<23:19:34,  8.60s/it, loss_gram=0, loss_l2=0.00506, loss_lpips=0.135]\n",
      "Steps:   2%|▏         | 239/10000 [1:05:59<23:23:49,  8.63s/it, loss_gram=0, loss_l2=0.00574, loss_lpips=0.1]  \n",
      "Steps:   2%|▏         | 240/10000 [1:06:08<23:15:50,  8.58s/it, loss_gram=0, loss_l2=0.0246, loss_lpips=0.158]\n",
      "Steps:   2%|▏         | 241/10000 [1:06:17<23:21:39,  8.62s/it, loss_gram=0, loss_l2=0.0391, loss_lpips=0.209]\n",
      "Steps:   2%|▏         | 242/10000 [1:06:25<23:10:25,  8.55s/it, loss_gram=0, loss_l2=0.0079, loss_lpips=0.158]\n",
      "Steps:   2%|▏         | 243/10000 [1:06:34<23:17:36,  8.59s/it, loss_gram=0, loss_l2=0.00929, loss_lpips=0.101]\n",
      "Steps:   2%|▏         | 244/10000 [1:06:42<23:09:04,  8.54s/it, loss_gram=0, loss_l2=0.0225, loss_lpips=0.192] \n",
      "Steps:   2%|▏         | 245/10000 [1:06:51<23:19:48,  8.61s/it, loss_gram=0, loss_l2=0.00522, loss_lpips=0.087]\n",
      "Steps:   2%|▏         | 246/10000 [1:06:59<23:09:42,  8.55s/it, loss_gram=0, loss_l2=0.0301, loss_lpips=0.138] \n",
      "Steps:   2%|▏         | 247/10000 [1:07:08<23:16:47,  8.59s/it, loss_gram=0, loss_l2=0.00834, loss_lpips=0.139]\n",
      "Steps:   2%|▏         | 248/10000 [1:07:17<23:15:34,  8.59s/it, loss_gram=0, loss_l2=0.00664, loss_lpips=0.151]\n",
      "Steps:   2%|▏         | 249/10000 [1:07:25<23:23:16,  8.63s/it, loss_gram=0, loss_l2=0.0156, loss_lpips=0.118] \n",
      "Steps:   2%|▎         | 250/10000 [1:07:34<23:11:42,  8.56s/it, loss_gram=0, loss_l2=0.0182, loss_lpips=0.198]\n",
      "Steps:   3%|▎         | 251/10000 [1:07:42<23:15:00,  8.59s/it, loss_gram=0, loss_l2=0.00525, loss_lpips=0.0979]\n",
      "Steps:   3%|▎         | 252/10000 [1:07:51<23:09:24,  8.55s/it, loss_gram=0, loss_l2=0.00939, loss_lpips=0.153] \n",
      "Steps:   3%|▎         | 253/10000 [1:08:00<23:15:33,  8.59s/it, loss_gram=0, loss_l2=0.0062, loss_lpips=0.121] \n",
      "Steps:   3%|▎         | 254/10000 [1:08:08<23:08:38,  8.55s/it, loss_gram=0, loss_l2=0.00227, loss_lpips=0.184]\n",
      "Steps:   3%|▎         | 255/10000 [1:08:17<23:11:21,  8.57s/it, loss_gram=0, loss_l2=0.0123, loss_lpips=0.128] \n",
      "Steps:   3%|▎         | 256/10000 [1:08:25<23:03:47,  8.52s/it, loss_gram=0, loss_l2=0.0034, loss_lpips=0.0842]\n",
      "Steps:   3%|▎         | 257/10000 [1:08:34<23:18:35,  8.61s/it, loss_gram=0, loss_l2=0.0248, loss_lpips=0.211] \n",
      "Steps:   3%|▎         | 258/10000 [1:08:42<23:09:32,  8.56s/it, loss_gram=0, loss_l2=0.00402, loss_lpips=0.0818]\n",
      "Steps:   3%|▎         | 259/10000 [1:08:51<23:15:13,  8.59s/it, loss_gram=0, loss_l2=0.00262, loss_lpips=0.105] \n",
      "Steps:   3%|▎         | 260/10000 [1:08:59<23:10:18,  8.56s/it, loss_gram=0, loss_l2=0.00743, loss_lpips=0.239]\n",
      "Steps:   3%|▎         | 261/10000 [1:09:08<23:18:46,  8.62s/it, loss_gram=0, loss_l2=0.00906, loss_lpips=0.2]  \n",
      "Steps:   3%|▎         | 262/10000 [1:09:17<23:16:07,  8.60s/it, loss_gram=0, loss_l2=0.0207, loss_lpips=0.124]\n",
      "Steps:   3%|▎         | 263/10000 [1:09:25<23:15:09,  8.60s/it, loss_gram=0, loss_l2=0.00687, loss_lpips=0.161]\n",
      "Steps:   3%|▎         | 264/10000 [1:09:34<23:15:31,  8.60s/it, loss_gram=0, loss_l2=0.012, loss_lpips=0.145]  \n",
      "Steps:   3%|▎         | 265/10000 [1:09:43<23:20:48,  8.63s/it, loss_gram=0, loss_l2=0.00696, loss_lpips=0.145]\n",
      "Steps:   3%|▎         | 266/10000 [1:09:51<23:07:58,  8.56s/it, loss_gram=0, loss_l2=0.00359, loss_lpips=0.128]\n",
      "Steps:   3%|▎         | 267/10000 [1:10:00<23:15:04,  8.60s/it, loss_gram=0, loss_l2=0.0125, loss_lpips=0.233] \n",
      "Steps:   3%|▎         | 268/10000 [1:10:08<23:11:12,  8.58s/it, loss_gram=0, loss_l2=0.0459, loss_lpips=0.2]  \n",
      "Steps:   3%|▎         | 269/10000 [1:10:17<23:13:06,  8.59s/it, loss_gram=0, loss_l2=0.00681, loss_lpips=0.164]\n",
      "Steps:   3%|▎         | 270/10000 [1:10:25<23:02:41,  8.53s/it, loss_gram=0, loss_l2=0.0191, loss_lpips=0.158] \n",
      "Steps:   3%|▎         | 271/10000 [1:10:34<23:15:00,  8.60s/it, loss_gram=0, loss_l2=0.0162, loss_lpips=0.152]\n",
      "Steps:   3%|▎         | 272/10000 [1:10:43<23:12:13,  8.59s/it, loss_gram=0, loss_l2=0.0364, loss_lpips=0.173]\n",
      "Steps:   3%|▎         | 273/10000 [1:10:51<23:22:15,  8.65s/it, loss_gram=0, loss_l2=0.0167, loss_lpips=0.143]\n",
      "Steps:   3%|▎         | 274/10000 [1:11:00<23:16:52,  8.62s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.119]\n",
      "Steps:   3%|▎         | 275/10000 [1:11:09<23:15:35,  8.61s/it, loss_gram=0, loss_l2=0.00932, loss_lpips=0.118]\n",
      "Steps:   3%|▎         | 276/10000 [1:11:17<23:09:46,  8.58s/it, loss_gram=0, loss_l2=0.00606, loss_lpips=0.0885]\n",
      "Steps:   3%|▎         | 277/10000 [1:11:31<27:20:43, 10.12s/it, loss_gram=0, loss_l2=0.0325, loss_lpips=0.21]   \n",
      "Steps:   3%|▎         | 278/10000 [1:11:58<41:03:22, 15.20s/it, loss_gram=0, loss_l2=0.0389, loss_lpips=0.187]\n",
      "Steps:   3%|▎         | 279/10000 [1:12:22<48:05:19, 17.81s/it, loss_gram=0, loss_l2=0.0328, loss_lpips=0.219]\n",
      "Steps:   3%|▎         | 280/10000 [1:12:43<50:35:56, 18.74s/it, loss_gram=0, loss_l2=0.007, loss_lpips=0.177] \n",
      "Steps:   3%|▎         | 281/10000 [1:13:09<56:41:12, 21.00s/it, loss_gram=0, loss_l2=0.00945, loss_lpips=0.0974]\n",
      "Steps:   3%|▎         | 282/10000 [1:13:28<54:39:45, 20.25s/it, loss_gram=0, loss_l2=0.00594, loss_lpips=0.0974]\n",
      "Steps:   3%|▎         | 283/10000 [1:13:53<58:42:07, 21.75s/it, loss_gram=0, loss_l2=0.0231, loss_lpips=0.156]  \n",
      "Steps:   3%|▎         | 284/10000 [1:14:16<59:51:33, 22.18s/it, loss_gram=0, loss_l2=0.0217, loss_lpips=0.314]\n",
      "Steps:   3%|▎         | 285/10000 [1:14:43<63:27:13, 23.51s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.17] \n",
      "Steps:   3%|▎         | 286/10000 [1:15:07<64:21:06, 23.85s/it, loss_gram=0, loss_l2=0.00829, loss_lpips=0.0977]\n",
      "Steps:   3%|▎         | 287/10000 [1:15:26<60:28:17, 22.41s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.196]  \n",
      "Steps:   3%|▎         | 288/10000 [1:15:50<61:28:07, 22.78s/it, loss_gram=0, loss_l2=0.0254, loss_lpips=0.177]\n",
      "Steps:   3%|▎         | 289/10000 [1:16:16<63:51:48, 23.68s/it, loss_gram=0, loss_l2=0.0158, loss_lpips=0.146]\n",
      "Steps:   3%|▎         | 290/10000 [1:16:41<65:09:26, 24.16s/it, loss_gram=0, loss_l2=0.0205, loss_lpips=0.162]\n",
      "Steps:   3%|▎         | 291/10000 [1:17:07<66:56:36, 24.82s/it, loss_gram=0, loss_l2=0.00995, loss_lpips=0.0989]\n",
      "Steps:   3%|▎         | 292/10000 [1:17:30<64:47:30, 24.03s/it, loss_gram=0, loss_l2=0.0285, loss_lpips=0.134]  \n",
      "Steps:   3%|▎         | 293/10000 [1:17:56<66:32:10, 24.68s/it, loss_gram=0, loss_l2=0.0039, loss_lpips=0.201]\n",
      "Steps:   3%|▎         | 294/10000 [1:18:18<64:57:14, 24.09s/it, loss_gram=0, loss_l2=0.0161, loss_lpips=0.152]\n",
      "Steps:   3%|▎         | 295/10000 [1:18:46<67:28:53, 25.03s/it, loss_gram=0, loss_l2=0.00401, loss_lpips=0.126]\n",
      "Steps:   3%|▎         | 296/10000 [1:19:11<67:17:23, 24.96s/it, loss_gram=0, loss_l2=0.00684, loss_lpips=0.148]\n",
      "Steps:   3%|▎         | 297/10000 [1:19:37<68:41:43, 25.49s/it, loss_gram=0, loss_l2=0.00602, loss_lpips=0.127]\n",
      "Steps:   3%|▎         | 298/10000 [1:20:00<66:18:42, 24.61s/it, loss_gram=0, loss_l2=0.0137, loss_lpips=0.121] \n",
      "Steps:   3%|▎         | 299/10000 [1:20:26<67:48:30, 25.16s/it, loss_gram=0, loss_l2=0.0271, loss_lpips=0.13] \n",
      "Steps:   3%|▎         | 300/10000 [1:20:49<65:42:11, 24.38s/it, loss_gram=0, loss_l2=0.0121, loss_lpips=0.118]\n",
      "Steps:   3%|▎         | 301/10000 [1:21:14<66:41:54, 24.76s/it, loss_gram=0, loss_l2=0.0221, loss_lpips=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   3%|▎         | 302/10000 [1:21:38<65:36:30, 24.35s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.106]\n",
      "Steps:   3%|▎         | 303/10000 [1:22:03<66:18:47, 24.62s/it, loss_gram=0, loss_l2=0.0122, loss_lpips=0.124]\n",
      "Steps:   3%|▎         | 304/10000 [1:22:27<65:46:38, 24.42s/it, loss_gram=0, loss_l2=0.0216, loss_lpips=0.197]\n",
      "Steps:   3%|▎         | 305/10000 [1:22:51<65:39:25, 24.38s/it, loss_gram=0, loss_l2=0.0114, loss_lpips=0.0959]\n",
      "Steps:   3%|▎         | 306/10000 [1:23:16<66:14:54, 24.60s/it, loss_gram=0, loss_l2=0.00889, loss_lpips=0.0916]\n",
      "Steps:   3%|▎         | 307/10000 [1:23:41<66:32:55, 24.72s/it, loss_gram=0, loss_l2=0.00231, loss_lpips=0.116] \n",
      "Steps:   3%|▎         | 308/10000 [1:24:06<65:59:21, 24.51s/it, loss_gram=0, loss_l2=0.0111, loss_lpips=0.14]  \n",
      "Steps:   3%|▎         | 309/10000 [1:24:29<65:32:32, 24.35s/it, loss_gram=0, loss_l2=0.0254, loss_lpips=0.196]\n",
      "Steps:   3%|▎         | 310/10000 [1:24:53<64:44:16, 24.05s/it, loss_gram=0, loss_l2=0.00878, loss_lpips=0.182]\n",
      "Steps:   3%|▎         | 311/10000 [1:25:18<65:31:17, 24.34s/it, loss_gram=0, loss_l2=0.00785, loss_lpips=0.114]\n",
      "Steps:   3%|▎         | 312/10000 [1:25:42<65:35:56, 24.38s/it, loss_gram=0, loss_l2=0.0124, loss_lpips=0.092] \n",
      "Steps:   3%|▎         | 313/10000 [1:26:07<66:12:15, 24.60s/it, loss_gram=0, loss_l2=0.0151, loss_lpips=0.102]\n",
      "Steps:   3%|▎         | 314/10000 [1:26:31<65:11:28, 24.23s/it, loss_gram=0, loss_l2=0.00314, loss_lpips=0.124]\n",
      "Steps:   3%|▎         | 315/10000 [1:26:56<66:13:01, 24.61s/it, loss_gram=0, loss_l2=0.0192, loss_lpips=0.262] \n",
      "Steps:   3%|▎         | 316/10000 [1:27:20<65:49:11, 24.47s/it, loss_gram=0, loss_l2=0.00521, loss_lpips=0.114]\n",
      "Steps:   3%|▎         | 317/10000 [1:27:46<66:44:38, 24.81s/it, loss_gram=0, loss_l2=0.00183, loss_lpips=0.106]\n",
      "Steps:   3%|▎         | 318/10000 [1:28:11<66:38:27, 24.78s/it, loss_gram=0, loss_l2=0.0067, loss_lpips=0.145] \n",
      "Steps:   3%|▎         | 319/10000 [1:28:35<66:42:16, 24.80s/it, loss_gram=0, loss_l2=0.0135, loss_lpips=0.121]\n",
      "Steps:   3%|▎         | 320/10000 [1:29:00<66:05:40, 24.58s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.153]\n",
      "Steps:   3%|▎         | 321/10000 [1:29:24<65:57:05, 24.53s/it, loss_gram=0, loss_l2=0.00984, loss_lpips=0.115]\n",
      "Steps:   3%|▎         | 322/10000 [1:29:42<60:45:42, 22.60s/it, loss_gram=0, loss_l2=0.0077, loss_lpips=0.122] \n",
      "Steps:   3%|▎         | 323/10000 [1:30:06<61:44:35, 22.97s/it, loss_gram=0, loss_l2=0.00514, loss_lpips=0.154]\n",
      "Steps:   3%|▎         | 324/10000 [1:30:20<54:57:39, 20.45s/it, loss_gram=0, loss_l2=0.00178, loss_lpips=0.149]\n",
      "Steps:   3%|▎         | 325/10000 [1:30:32<48:03:16, 17.88s/it, loss_gram=0, loss_l2=0.00922, loss_lpips=0.146]\n",
      "Steps:   3%|▎         | 326/10000 [1:30:43<42:29:50, 15.81s/it, loss_gram=0, loss_l2=0.00971, loss_lpips=0.26] \n",
      "Steps:   3%|▎         | 327/10000 [1:31:04<46:40:35, 17.37s/it, loss_gram=0, loss_l2=0.0038, loss_lpips=0.107]\n",
      "Steps:   3%|▎         | 328/10000 [1:31:25<49:21:04, 18.37s/it, loss_gram=0, loss_l2=0.00865, loss_lpips=0.152]\n",
      "Steps:   3%|▎         | 329/10000 [1:31:44<49:52:01, 18.56s/it, loss_gram=0, loss_l2=0.00926, loss_lpips=0.11] \n",
      "Steps:   3%|▎         | 330/10000 [1:32:02<49:21:35, 18.38s/it, loss_gram=0, loss_l2=0.017, loss_lpips=0.153] \n",
      "Steps:   3%|▎         | 331/10000 [1:32:24<51:50:59, 19.30s/it, loss_gram=0, loss_l2=0.0076, loss_lpips=0.147]\n",
      "Steps:   3%|▎         | 332/10000 [1:32:44<52:20:29, 19.49s/it, loss_gram=0, loss_l2=0.00556, loss_lpips=0.0908]\n",
      "Steps:   3%|▎         | 333/10000 [1:33:03<52:38:01, 19.60s/it, loss_gram=0, loss_l2=0.00664, loss_lpips=0.108] \n",
      "Steps:   3%|▎         | 334/10000 [1:33:24<53:08:33, 19.79s/it, loss_gram=0, loss_l2=0.000905, loss_lpips=0.0842]\n",
      "Steps:   3%|▎         | 335/10000 [1:33:44<54:01:08, 20.12s/it, loss_gram=0, loss_l2=0.0112, loss_lpips=0.133]   \n",
      "Steps:   3%|▎         | 336/10000 [1:34:05<54:27:23, 20.29s/it, loss_gram=0, loss_l2=0.00207, loss_lpips=0.0791]\n",
      "Steps:   3%|▎         | 337/10000 [1:34:28<56:16:32, 20.97s/it, loss_gram=0, loss_l2=0.00607, loss_lpips=0.103] \n",
      "Steps:   3%|▎         | 338/10000 [1:34:49<56:18:21, 20.98s/it, loss_gram=0, loss_l2=0.0152, loss_lpips=0.0931]\n",
      "Steps:   3%|▎         | 339/10000 [1:35:09<56:03:30, 20.89s/it, loss_gram=0, loss_l2=0.00269, loss_lpips=0.127]\n",
      "Steps:   3%|▎         | 340/10000 [1:35:28<54:41:23, 20.38s/it, loss_gram=0, loss_l2=0.00405, loss_lpips=0.112]\n",
      "Steps:   3%|▎         | 341/10000 [1:35:48<53:46:10, 20.04s/it, loss_gram=0, loss_l2=0.00169, loss_lpips=0.112]\n",
      "Steps:   3%|▎         | 342/10000 [1:36:08<53:31:20, 19.95s/it, loss_gram=0, loss_l2=0.00575, loss_lpips=0.173]\n",
      "Steps:   3%|▎         | 343/10000 [1:36:27<53:32:21, 19.96s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.128] \n",
      "Steps:   3%|▎         | 344/10000 [1:36:48<53:51:31, 20.08s/it, loss_gram=0, loss_l2=0.00346, loss_lpips=0.0717]\n",
      "Steps:   3%|▎         | 345/10000 [1:37:09<54:29:24, 20.32s/it, loss_gram=0, loss_l2=0.00438, loss_lpips=0.12]  \n",
      "Steps:   3%|▎         | 346/10000 [1:37:30<55:33:25, 20.72s/it, loss_gram=0, loss_l2=0.00504, loss_lpips=0.144]\n",
      "Steps:   3%|▎         | 347/10000 [1:37:53<56:38:40, 21.13s/it, loss_gram=0, loss_l2=0.0161, loss_lpips=0.153] \n",
      "Steps:   3%|▎         | 348/10000 [1:38:13<56:06:05, 20.92s/it, loss_gram=0, loss_l2=0.007, loss_lpips=0.165] \n",
      "Steps:   3%|▎         | 349/10000 [1:38:32<54:56:09, 20.49s/it, loss_gram=0, loss_l2=0.00705, loss_lpips=0.141]\n",
      "Steps:   4%|▎         | 350/10000 [1:38:53<54:38:17, 20.38s/it, loss_gram=0, loss_l2=0.0099, loss_lpips=0.13]  \n",
      "Steps:   4%|▎         | 351/10000 [1:39:12<53:41:02, 20.03s/it, loss_gram=0, loss_l2=0.00651, loss_lpips=0.11]\n",
      "Steps:   4%|▎         | 352/10000 [1:39:31<52:26:48, 19.57s/it, loss_gram=0, loss_l2=0.0132, loss_lpips=0.116]\n",
      "Steps:   4%|▎         | 353/10000 [1:39:52<53:45:54, 20.06s/it, loss_gram=0, loss_l2=0.00529, loss_lpips=0.0946]\n",
      "Steps:   4%|▎         | 354/10000 [1:40:11<53:50:15, 20.09s/it, loss_gram=0, loss_l2=0.0291, loss_lpips=0.163]  \n",
      "Steps:   4%|▎         | 355/10000 [1:40:32<54:13:19, 20.24s/it, loss_gram=0, loss_l2=0.0124, loss_lpips=0.119]\n",
      "Steps:   4%|▎         | 356/10000 [1:40:52<54:00:11, 20.16s/it, loss_gram=0, loss_l2=0.0283, loss_lpips=0.151]\n",
      "Steps:   4%|▎         | 357/10000 [1:41:14<54:44:32, 20.44s/it, loss_gram=0, loss_l2=0.00871, loss_lpips=0.157]\n",
      "Steps:   4%|▎         | 358/10000 [1:41:32<53:25:39, 19.95s/it, loss_gram=0, loss_l2=0.00654, loss_lpips=0.0999]\n",
      "Steps:   4%|▎         | 359/10000 [1:41:53<54:08:34, 20.22s/it, loss_gram=0, loss_l2=0.0162, loss_lpips=0.156]  \n",
      "Steps:   4%|▎         | 360/10000 [1:42:12<53:15:35, 19.89s/it, loss_gram=0, loss_l2=0.0038, loss_lpips=0.186]\n",
      "Steps:   4%|▎         | 361/10000 [1:42:32<53:23:00, 19.94s/it, loss_gram=0, loss_l2=0.0151, loss_lpips=0.0979]\n",
      "Steps:   4%|▎         | 362/10000 [1:42:51<52:43:34, 19.69s/it, loss_gram=0, loss_l2=0.00491, loss_lpips=0.175]\n",
      "Steps:   4%|▎         | 363/10000 [1:43:12<53:52:23, 20.12s/it, loss_gram=0, loss_l2=0.0114, loss_lpips=0.138] \n",
      "Steps:   4%|▎         | 364/10000 [1:43:33<54:38:58, 20.42s/it, loss_gram=0, loss_l2=0.00701, loss_lpips=0.118]\n",
      "Steps:   4%|▎         | 365/10000 [1:43:53<53:53:42, 20.14s/it, loss_gram=0, loss_l2=0.00711, loss_lpips=0.0984]\n",
      "Steps:   4%|▎         | 366/10000 [1:44:13<53:45:55, 20.09s/it, loss_gram=0, loss_l2=0.0131, loss_lpips=0.136]  \n",
      "Steps:   4%|▎         | 367/10000 [1:44:33<53:44:56, 20.09s/it, loss_gram=0, loss_l2=0.0085, loss_lpips=0.126]\n",
      "Steps:   4%|▎         | 368/10000 [1:44:54<54:08:39, 20.24s/it, loss_gram=0, loss_l2=0.00343, loss_lpips=0.157]\n",
      "Steps:   4%|▎         | 369/10000 [1:45:14<54:16:33, 20.29s/it, loss_gram=0, loss_l2=0.0148, loss_lpips=0.125] \n",
      "Steps:   4%|▎         | 370/10000 [1:45:33<53:25:10, 19.97s/it, loss_gram=0, loss_l2=0.00591, loss_lpips=0.0909]\n",
      "Steps:   4%|▎         | 371/10000 [1:45:52<52:55:31, 19.79s/it, loss_gram=0, loss_l2=0.00793, loss_lpips=0.178] \n",
      "Steps:   4%|▎         | 372/10000 [1:46:13<53:42:02, 20.08s/it, loss_gram=0, loss_l2=0.00861, loss_lpips=0.137]\n",
      "Steps:   4%|▎         | 373/10000 [1:46:33<53:28:06, 19.99s/it, loss_gram=0, loss_l2=0.00103, loss_lpips=0.109]\n",
      "Steps:   4%|▎         | 374/10000 [1:46:54<54:17:32, 20.30s/it, loss_gram=0, loss_l2=0.00795, loss_lpips=0.172]\n",
      "Steps:   4%|▍         | 375/10000 [1:47:14<53:37:53, 20.06s/it, loss_gram=0, loss_l2=0.0164, loss_lpips=0.134] \n",
      "Steps:   4%|▍         | 376/10000 [1:47:34<54:02:32, 20.22s/it, loss_gram=0, loss_l2=0.0383, loss_lpips=0.205]\n",
      "Steps:   4%|▍         | 377/10000 [1:47:54<53:54:42, 20.17s/it, loss_gram=0, loss_l2=0.00812, loss_lpips=0.144]\n",
      "Steps:   4%|▍         | 378/10000 [1:48:15<54:15:28, 20.30s/it, loss_gram=0, loss_l2=0.0143, loss_lpips=0.104] \n",
      "Steps:   4%|▍         | 379/10000 [1:48:36<55:14:48, 20.67s/it, loss_gram=0, loss_l2=0.0106, loss_lpips=0.122]\n",
      "Steps:   4%|▍         | 380/10000 [1:48:57<54:46:42, 20.50s/it, loss_gram=0, loss_l2=0.00355, loss_lpips=0.0923]\n",
      "Steps:   4%|▍         | 381/10000 [1:49:18<55:02:23, 20.60s/it, loss_gram=0, loss_l2=0.00178, loss_lpips=0.118] \n",
      "Steps:   4%|▍         | 382/10000 [1:49:38<54:48:00, 20.51s/it, loss_gram=0, loss_l2=0.00602, loss_lpips=0.137]\n",
      "Steps:   4%|▍         | 383/10000 [1:49:58<54:24:10, 20.37s/it, loss_gram=0, loss_l2=0.00576, loss_lpips=0.0979]\n",
      "Steps:   4%|▍         | 384/10000 [1:50:18<54:19:44, 20.34s/it, loss_gram=0, loss_l2=0.00476, loss_lpips=0.123] \n",
      "Steps:   4%|▍         | 385/10000 [1:50:39<55:10:43, 20.66s/it, loss_gram=0, loss_l2=0.00396, loss_lpips=0.0743]\n",
      "Steps:   4%|▍         | 386/10000 [1:51:01<55:52:38, 20.92s/it, loss_gram=0, loss_l2=0.0369, loss_lpips=0.177]  \n",
      "Steps:   4%|▍         | 387/10000 [1:51:21<55:22:28, 20.74s/it, loss_gram=0, loss_l2=0.0118, loss_lpips=0.115]\n",
      "Steps:   4%|▍         | 388/10000 [1:51:42<55:19:08, 20.72s/it, loss_gram=0, loss_l2=0.00389, loss_lpips=0.141]\n",
      "Steps:   4%|▍         | 389/10000 [1:52:03<55:39:50, 20.85s/it, loss_gram=0, loss_l2=0.00704, loss_lpips=0.157]\n",
      "Steps:   4%|▍         | 390/10000 [1:52:24<56:02:05, 20.99s/it, loss_gram=0, loss_l2=0.0206, loss_lpips=0.263] \n",
      "Steps:   4%|▍         | 391/10000 [1:52:45<55:20:32, 20.73s/it, loss_gram=0, loss_l2=0.0199, loss_lpips=0.139]\n",
      "Steps:   4%|▍         | 392/10000 [1:53:04<54:43:54, 20.51s/it, loss_gram=0, loss_l2=0.00529, loss_lpips=0.143]\n",
      "Steps:   4%|▍         | 393/10000 [1:53:25<54:43:01, 20.50s/it, loss_gram=0, loss_l2=0.00922, loss_lpips=0.153]\n",
      "Steps:   4%|▍         | 394/10000 [1:53:44<53:25:45, 20.02s/it, loss_gram=0, loss_l2=0.00491, loss_lpips=0.101]\n",
      "Steps:   4%|▍         | 395/10000 [1:54:04<53:34:36, 20.08s/it, loss_gram=0, loss_l2=0.00591, loss_lpips=0.0845]\n",
      "Steps:   4%|▍         | 396/10000 [1:54:27<55:43:27, 20.89s/it, loss_gram=0, loss_l2=0.0109, loss_lpips=0.174]  \n",
      "Steps:   4%|▍         | 397/10000 [1:54:47<55:06:21, 20.66s/it, loss_gram=0, loss_l2=0.00721, loss_lpips=0.137]\n",
      "Steps:   4%|▍         | 398/10000 [1:55:07<54:39:52, 20.49s/it, loss_gram=0, loss_l2=0.0233, loss_lpips=0.158] \n",
      "Steps:   4%|▍         | 399/10000 [1:55:27<54:23:56, 20.40s/it, loss_gram=0, loss_l2=0.00494, loss_lpips=0.135]\n",
      "Steps:   4%|▍         | 400/10000 [1:55:48<54:53:15, 20.58s/it, loss_gram=0, loss_l2=0.00659, loss_lpips=0.0967]\n",
      "Steps:   4%|▍         | 401/10000 [1:56:09<55:15:33, 20.72s/it, loss_gram=0, loss_l2=0.0147, loss_lpips=0.142]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   4%|▍         | 402/10000 [1:56:29<54:06:25, 20.29s/it, loss_gram=0, loss_l2=0.00849, loss_lpips=0.137]\n",
      "Steps:   4%|▍         | 403/10000 [1:56:49<54:07:30, 20.30s/it, loss_gram=0, loss_l2=0.0114, loss_lpips=0.139] \n",
      "Steps:   4%|▍         | 404/10000 [1:57:09<53:42:46, 20.15s/it, loss_gram=0, loss_l2=0.00394, loss_lpips=0.132]\n",
      "Steps:   4%|▍         | 405/10000 [1:57:28<53:29:05, 20.07s/it, loss_gram=0, loss_l2=0.00924, loss_lpips=0.102]\n",
      "Steps:   4%|▍         | 406/10000 [1:57:49<53:53:38, 20.22s/it, loss_gram=0, loss_l2=0.00873, loss_lpips=0.161]\n",
      "Steps:   4%|▍         | 407/10000 [1:58:11<55:18:39, 20.76s/it, loss_gram=0, loss_l2=0.0123, loss_lpips=0.139] \n",
      "Steps:   4%|▍         | 408/10000 [1:58:31<54:39:53, 20.52s/it, loss_gram=0, loss_l2=0.013, loss_lpips=0.169] \n",
      "Steps:   4%|▍         | 409/10000 [1:58:50<53:40:19, 20.15s/it, loss_gram=0, loss_l2=0.00148, loss_lpips=0.114]\n",
      "Steps:   4%|▍         | 410/10000 [1:59:10<53:26:33, 20.06s/it, loss_gram=0, loss_l2=0.00911, loss_lpips=0.122]\n",
      "Steps:   4%|▍         | 411/10000 [1:59:32<54:34:03, 20.49s/it, loss_gram=0, loss_l2=0.0074, loss_lpips=0.137] \n",
      "Steps:   4%|▍         | 412/10000 [1:59:53<54:51:42, 20.60s/it, loss_gram=0, loss_l2=0.00389, loss_lpips=0.134]\n",
      "Steps:   4%|▍         | 413/10000 [2:00:12<53:52:04, 20.23s/it, loss_gram=0, loss_l2=0.0061, loss_lpips=0.109] \n",
      "Steps:   4%|▍         | 414/10000 [2:00:32<53:16:13, 20.01s/it, loss_gram=0, loss_l2=0.00363, loss_lpips=0.175]\n",
      "Steps:   4%|▍         | 415/10000 [2:00:53<54:31:49, 20.48s/it, loss_gram=0, loss_l2=0.0338, loss_lpips=0.23]  \n",
      "Steps:   4%|▍         | 416/10000 [2:01:14<54:41:22, 20.54s/it, loss_gram=0, loss_l2=0.00765, loss_lpips=0.0801]\n",
      "Steps:   4%|▍         | 417/10000 [2:01:34<54:01:53, 20.30s/it, loss_gram=0, loss_l2=0.0103, loss_lpips=0.131]  \n",
      "Steps:   4%|▍         | 418/10000 [2:01:53<53:35:17, 20.13s/it, loss_gram=0, loss_l2=0.00932, loss_lpips=0.106]\n",
      "Steps:   4%|▍         | 419/10000 [2:02:14<53:31:46, 20.11s/it, loss_gram=0, loss_l2=0.00779, loss_lpips=0.105]\n",
      "Steps:   4%|▍         | 420/10000 [2:02:33<53:06:36, 19.96s/it, loss_gram=0, loss_l2=0.0204, loss_lpips=0.158] \n",
      "Steps:   4%|▍         | 421/10000 [2:02:52<52:47:33, 19.84s/it, loss_gram=0, loss_l2=0.0076, loss_lpips=0.123]\n",
      "Steps:   4%|▍         | 422/10000 [2:03:12<52:07:59, 19.59s/it, loss_gram=0, loss_l2=0.00576, loss_lpips=0.0847]\n",
      "Steps:   4%|▍         | 423/10000 [2:03:32<53:13:36, 20.01s/it, loss_gram=0, loss_l2=0.0224, loss_lpips=0.0991] \n",
      "Steps:   4%|▍         | 424/10000 [2:03:53<54:03:53, 20.33s/it, loss_gram=0, loss_l2=0.0398, loss_lpips=0.163] \n",
      "Steps:   4%|▍         | 425/10000 [2:04:14<53:51:09, 20.25s/it, loss_gram=0, loss_l2=0.00546, loss_lpips=0.0867]\n",
      "Steps:   4%|▍         | 426/10000 [2:04:34<53:50:27, 20.25s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.185]  \n",
      "Steps:   4%|▍         | 427/10000 [2:04:55<54:40:09, 20.56s/it, loss_gram=0, loss_l2=0.0141, loss_lpips=0.237]\n",
      "Steps:   4%|▍         | 428/10000 [2:05:14<53:15:12, 20.03s/it, loss_gram=0, loss_l2=0.0149, loss_lpips=0.138]\n",
      "Steps:   4%|▍         | 429/10000 [2:05:35<53:44:32, 20.21s/it, loss_gram=0, loss_l2=0.00613, loss_lpips=0.154]\n",
      "Steps:   4%|▍         | 430/10000 [2:05:55<53:51:29, 20.26s/it, loss_gram=0, loss_l2=0.0333, loss_lpips=0.173] \n",
      "Steps:   4%|▍         | 431/10000 [2:06:15<53:48:17, 20.24s/it, loss_gram=0, loss_l2=0.0105, loss_lpips=0.166]\n",
      "Steps:   4%|▍         | 432/10000 [2:06:36<54:27:52, 20.49s/it, loss_gram=0, loss_l2=0.0435, loss_lpips=0.189]\n",
      "Steps:   4%|▍         | 433/10000 [2:06:58<55:48:16, 21.00s/it, loss_gram=0, loss_l2=0.00189, loss_lpips=0.145]\n",
      "Steps:   4%|▍         | 434/10000 [2:07:18<54:30:08, 20.51s/it, loss_gram=0, loss_l2=0.00885, loss_lpips=0.0956]\n",
      "Steps:   4%|▍         | 435/10000 [2:07:38<54:16:49, 20.43s/it, loss_gram=0, loss_l2=0.085, loss_lpips=0.321]   \n",
      "Steps:   4%|▍         | 436/10000 [2:07:58<53:55:59, 20.30s/it, loss_gram=0, loss_l2=0.00705, loss_lpips=0.12]\n",
      "Steps:   4%|▍         | 437/10000 [2:08:18<53:58:02, 20.32s/it, loss_gram=0, loss_l2=0.00701, loss_lpips=0.0723]\n",
      "Steps:   4%|▍         | 438/10000 [2:08:39<54:26:29, 20.50s/it, loss_gram=0, loss_l2=0.00422, loss_lpips=0.129] \n",
      "Steps:   4%|▍         | 439/10000 [2:09:01<55:37:39, 20.95s/it, loss_gram=0, loss_l2=0.00653, loss_lpips=0.066]\n",
      "Steps:   4%|▍         | 440/10000 [2:09:21<54:57:57, 20.70s/it, loss_gram=0, loss_l2=0.00755, loss_lpips=0.122]\n",
      "Steps:   4%|▍         | 441/10000 [2:09:43<55:20:09, 20.84s/it, loss_gram=0, loss_l2=0.00728, loss_lpips=0.144]\n",
      "Steps:   4%|▍         | 442/10000 [2:10:02<54:34:54, 20.56s/it, loss_gram=0, loss_l2=0.00243, loss_lpips=0.0754]\n",
      "Steps:   4%|▍         | 443/10000 [2:10:23<54:38:47, 20.58s/it, loss_gram=0, loss_l2=0.0282, loss_lpips=0.146]  \n",
      "Steps:   4%|▍         | 444/10000 [2:10:43<54:08:23, 20.40s/it, loss_gram=0, loss_l2=0.00201, loss_lpips=0.118]\n",
      "Steps:   4%|▍         | 445/10000 [2:11:04<54:17:24, 20.45s/it, loss_gram=0, loss_l2=0.0238, loss_lpips=0.153] \n",
      "Steps:   4%|▍         | 446/10000 [2:11:23<53:25:54, 20.13s/it, loss_gram=0, loss_l2=0.00762, loss_lpips=0.106]\n",
      "Steps:   4%|▍         | 447/10000 [2:11:44<54:26:19, 20.51s/it, loss_gram=0, loss_l2=0.00651, loss_lpips=0.159]\n",
      "Steps:   4%|▍         | 448/10000 [2:12:06<55:19:14, 20.85s/it, loss_gram=0, loss_l2=0.0132, loss_lpips=0.118] \n",
      "Steps:   4%|▍         | 449/10000 [2:12:26<54:47:06, 20.65s/it, loss_gram=0, loss_l2=0.00273, loss_lpips=0.0708]\n",
      "Steps:   4%|▍         | 450/10000 [2:12:46<53:53:24, 20.31s/it, loss_gram=0, loss_l2=0.00276, loss_lpips=0.136] \n",
      "Steps:   5%|▍         | 451/10000 [2:13:07<54:29:16, 20.54s/it, loss_gram=0, loss_l2=0.00656, loss_lpips=0.181]\n",
      "Steps:   5%|▍         | 452/10000 [2:13:27<53:48:08, 20.29s/it, loss_gram=0, loss_l2=0.00322, loss_lpips=0.0886]\n",
      "Steps:   5%|▍         | 453/10000 [2:13:47<53:33:37, 20.20s/it, loss_gram=0, loss_l2=0.0152, loss_lpips=0.129]  \n",
      "Steps:   5%|▍         | 454/10000 [2:14:06<52:51:14, 19.93s/it, loss_gram=0, loss_l2=0.00577, loss_lpips=0.087]\n",
      "Steps:   5%|▍         | 455/10000 [2:14:27<53:52:16, 20.32s/it, loss_gram=0, loss_l2=0.00897, loss_lpips=0.101]\n",
      "Steps:   5%|▍         | 456/10000 [2:14:47<54:02:21, 20.38s/it, loss_gram=0, loss_l2=0.00191, loss_lpips=0.0701]\n",
      "Steps:   5%|▍         | 457/10000 [2:15:09<55:09:48, 20.81s/it, loss_gram=0, loss_l2=0.00714, loss_lpips=0.106] \n",
      "Steps:   5%|▍         | 458/10000 [2:15:28<53:11:16, 20.07s/it, loss_gram=0, loss_l2=0.016, loss_lpips=0.111]  \n",
      "Steps:   5%|▍         | 459/10000 [2:15:47<52:47:43, 19.92s/it, loss_gram=0, loss_l2=0.00526, loss_lpips=0.101]\n",
      "Steps:   5%|▍         | 460/10000 [2:16:07<53:05:01, 20.03s/it, loss_gram=0, loss_l2=0.0177, loss_lpips=0.133] \n",
      "Steps:   5%|▍         | 461/10000 [2:16:27<52:57:09, 19.98s/it, loss_gram=0, loss_l2=0.0182, loss_lpips=0.143]\n",
      "Steps:   5%|▍         | 462/10000 [2:16:47<52:40:31, 19.88s/it, loss_gram=0, loss_l2=0.0122, loss_lpips=0.145]\n",
      "Steps:   5%|▍         | 463/10000 [2:17:08<53:26:28, 20.17s/it, loss_gram=0, loss_l2=0.00528, loss_lpips=0.176]\n",
      "Steps:   5%|▍         | 464/10000 [2:17:26<51:50:50, 19.57s/it, loss_gram=0, loss_l2=0.00463, loss_lpips=0.123]\n",
      "Steps:   5%|▍         | 465/10000 [2:17:45<51:20:39, 19.39s/it, loss_gram=0, loss_l2=0.00243, loss_lpips=0.129]\n",
      "Steps:   5%|▍         | 466/10000 [2:18:05<51:26:22, 19.42s/it, loss_gram=0, loss_l2=0.0235, loss_lpips=0.199] \n",
      "Steps:   5%|▍         | 467/10000 [2:18:24<51:34:00, 19.47s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.0972]\n",
      "Steps:   5%|▍         | 468/10000 [2:18:45<52:15:52, 19.74s/it, loss_gram=0, loss_l2=0.0561, loss_lpips=0.126] \n",
      "Steps:   5%|▍         | 469/10000 [2:19:05<53:00:54, 20.02s/it, loss_gram=0, loss_l2=0.0108, loss_lpips=0.265]\n",
      "Steps:   5%|▍         | 470/10000 [2:19:25<52:21:41, 19.78s/it, loss_gram=0, loss_l2=0.00365, loss_lpips=0.0798]\n",
      "Steps:   5%|▍         | 471/10000 [2:19:45<53:12:33, 20.10s/it, loss_gram=0, loss_l2=0.00383, loss_lpips=0.0806]\n",
      "Steps:   5%|▍         | 472/10000 [2:20:06<53:39:19, 20.27s/it, loss_gram=0, loss_l2=0.0273, loss_lpips=0.167]  \n",
      "Steps:   5%|▍         | 473/10000 [2:20:27<53:56:06, 20.38s/it, loss_gram=0, loss_l2=0.016, loss_lpips=0.0928]\n",
      "Steps:   5%|▍         | 474/10000 [2:20:47<54:02:32, 20.42s/it, loss_gram=0, loss_l2=0.0349, loss_lpips=0.176]\n",
      "Steps:   5%|▍         | 475/10000 [2:21:07<53:36:41, 20.26s/it, loss_gram=0, loss_l2=0.0201, loss_lpips=0.108]\n",
      "Steps:   5%|▍         | 476/10000 [2:21:27<53:33:46, 20.25s/it, loss_gram=0, loss_l2=0.0156, loss_lpips=0.162]\n",
      "Steps:   5%|▍         | 477/10000 [2:21:48<54:03:23, 20.44s/it, loss_gram=0, loss_l2=0.00469, loss_lpips=0.0844]\n",
      "Steps:   5%|▍         | 478/10000 [2:22:09<54:02:12, 20.43s/it, loss_gram=0, loss_l2=0.00966, loss_lpips=0.124] \n",
      "Steps:   5%|▍         | 479/10000 [2:22:31<55:16:49, 20.90s/it, loss_gram=0, loss_l2=0.0214, loss_lpips=0.159] \n",
      "Steps:   5%|▍         | 480/10000 [2:22:50<54:19:19, 20.54s/it, loss_gram=0, loss_l2=0.0141, loss_lpips=0.0864]\n",
      "Steps:   5%|▍         | 481/10000 [2:23:11<54:32:53, 20.63s/it, loss_gram=0, loss_l2=0.0137, loss_lpips=0.16]  \n",
      "Steps:   5%|▍         | 482/10000 [2:23:31<54:00:51, 20.43s/it, loss_gram=0, loss_l2=0.00223, loss_lpips=0.113]\n",
      "Steps:   5%|▍         | 483/10000 [2:23:50<53:01:46, 20.06s/it, loss_gram=0, loss_l2=0.0288, loss_lpips=0.188] \n",
      "Steps:   5%|▍         | 484/10000 [2:24:11<53:22:56, 20.20s/it, loss_gram=0, loss_l2=0.00531, loss_lpips=0.109]\n",
      "Steps:   5%|▍         | 485/10000 [2:24:31<53:33:13, 20.26s/it, loss_gram=0, loss_l2=0.00474, loss_lpips=0.126]\n",
      "Steps:   5%|▍         | 486/10000 [2:24:51<53:19:11, 20.18s/it, loss_gram=0, loss_l2=0.0287, loss_lpips=0.175] \n",
      "Steps:   5%|▍         | 487/10000 [2:25:12<53:58:55, 20.43s/it, loss_gram=0, loss_l2=0.00821, loss_lpips=0.0747]\n",
      "Steps:   5%|▍         | 488/10000 [2:25:31<52:53:06, 20.02s/it, loss_gram=0, loss_l2=0.0234, loss_lpips=0.14]   \n",
      "Steps:   5%|▍         | 489/10000 [2:25:54<55:01:44, 20.83s/it, loss_gram=0, loss_l2=0.00131, loss_lpips=0.244]\n",
      "Steps:   5%|▍         | 490/10000 [2:26:15<54:54:56, 20.79s/it, loss_gram=0, loss_l2=0.00458, loss_lpips=0.0962]\n",
      "Steps:   5%|▍         | 491/10000 [2:26:34<53:47:57, 20.37s/it, loss_gram=0, loss_l2=0.00736, loss_lpips=0.148] \n",
      "Steps:   5%|▍         | 492/10000 [2:26:54<53:46:54, 20.36s/it, loss_gram=0, loss_l2=0.00163, loss_lpips=0.14] \n",
      "Steps:   5%|▍         | 493/10000 [2:27:16<54:40:02, 20.70s/it, loss_gram=0, loss_l2=0.0101, loss_lpips=0.114]\n",
      "Steps:   5%|▍         | 494/10000 [2:27:34<53:01:44, 20.08s/it, loss_gram=0, loss_l2=0.0228, loss_lpips=0.114]\n",
      "Steps:   5%|▍         | 495/10000 [2:27:53<51:46:35, 19.61s/it, loss_gram=0, loss_l2=0.00558, loss_lpips=0.1] \n",
      "Steps:   5%|▍         | 496/10000 [2:28:13<52:00:02, 19.70s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.145]\n",
      "Steps:   5%|▍         | 497/10000 [2:28:33<52:02:13, 19.71s/it, loss_gram=0, loss_l2=0.00817, loss_lpips=0.112]\n",
      "Steps:   5%|▍         | 498/10000 [2:28:54<53:03:29, 20.10s/it, loss_gram=0, loss_l2=0.0111, loss_lpips=0.128] \n",
      "Steps:   5%|▍         | 499/10000 [2:29:15<53:58:00, 20.45s/it, loss_gram=0, loss_l2=0.00852, loss_lpips=0.151]\n",
      "Steps:   5%|▌         | 500/10000 [2:29:34<53:06:57, 20.13s/it, loss_gram=0, loss_l2=0.0149, loss_lpips=0.144] \n",
      "Steps:   5%|▌         | 501/10000 [2:29:56<54:16:28, 20.57s/it, loss_gram=0, loss_l2=0.019, loss_lpips=0.228] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   5%|▌         | 502/10000 [2:30:18<55:19:19, 20.97s/it, loss_gram=0, loss_l2=0.0101, loss_lpips=0.0968]\n",
      "Steps:   5%|▌         | 503/10000 [2:30:39<55:36:26, 21.08s/it, loss_gram=0, loss_l2=0.00536, loss_lpips=0.102]\n",
      "Steps:   5%|▌         | 504/10000 [2:31:01<55:55:49, 21.20s/it, loss_gram=0, loss_l2=0.0105, loss_lpips=0.108] \n",
      "Steps:   5%|▌         | 505/10000 [2:31:21<55:18:42, 20.97s/it, loss_gram=0, loss_l2=0.0135, loss_lpips=0.134]\n",
      "Steps:   5%|▌         | 506/10000 [2:31:42<55:03:01, 20.87s/it, loss_gram=0, loss_l2=0.00497, loss_lpips=0.116]\n",
      "Steps:   5%|▌         | 507/10000 [2:32:03<55:29:21, 21.04s/it, loss_gram=0, loss_l2=0.029, loss_lpips=0.192]  \n",
      "Steps:   5%|▌         | 508/10000 [2:32:24<55:01:38, 20.87s/it, loss_gram=0, loss_l2=0.0169, loss_lpips=0.164]\n",
      "Steps:   5%|▌         | 509/10000 [2:32:45<55:10:39, 20.93s/it, loss_gram=0, loss_l2=0.0024, loss_lpips=0.0906]\n",
      "Steps:   5%|▌         | 510/10000 [2:33:04<53:31:58, 20.31s/it, loss_gram=0, loss_l2=0.00974, loss_lpips=0.126]\n",
      "Steps:   5%|▌         | 511/10000 [2:33:25<54:20:07, 20.61s/it, loss_gram=0, loss_l2=0.0197, loss_lpips=0.118] \n",
      "Steps:   5%|▌         | 512/10000 [2:33:46<54:30:56, 20.68s/it, loss_gram=0, loss_l2=0.0122, loss_lpips=0.0964]\n",
      "Steps:   5%|▌         | 513/10000 [2:34:06<53:57:20, 20.47s/it, loss_gram=0, loss_l2=0.00144, loss_lpips=0.0803]\n",
      "Steps:   5%|▌         | 514/10000 [2:34:26<54:00:06, 20.49s/it, loss_gram=0, loss_l2=0.0363, loss_lpips=0.28]   \n",
      "Steps:   5%|▌         | 515/10000 [2:34:46<53:27:32, 20.29s/it, loss_gram=0, loss_l2=0.018, loss_lpips=0.19] \n",
      "Steps:   5%|▌         | 516/10000 [2:35:07<54:03:17, 20.52s/it, loss_gram=0, loss_l2=0.00703, loss_lpips=0.101]\n",
      "Steps:   5%|▌         | 517/10000 [2:35:27<53:28:02, 20.30s/it, loss_gram=0, loss_l2=0.0462, loss_lpips=0.154] \n",
      "Steps:   5%|▌         | 518/10000 [2:35:50<55:15:48, 20.98s/it, loss_gram=0, loss_l2=0.00328, loss_lpips=0.129]\n",
      "Steps:   5%|▌         | 519/10000 [2:36:10<54:51:26, 20.83s/it, loss_gram=0, loss_l2=0.0314, loss_lpips=0.175] \n",
      "Steps:   5%|▌         | 520/10000 [2:36:30<54:27:59, 20.68s/it, loss_gram=0, loss_l2=0.00275, loss_lpips=0.115]\n",
      "Steps:   5%|▌         | 521/10000 [2:36:51<54:36:31, 20.74s/it, loss_gram=0, loss_l2=0.0179, loss_lpips=0.143] \n",
      "Steps:   5%|▌         | 522/10000 [2:37:12<54:25:32, 20.67s/it, loss_gram=0, loss_l2=0.00551, loss_lpips=0.108]\n",
      "Steps:   5%|▌         | 523/10000 [2:37:32<53:54:57, 20.48s/it, loss_gram=0, loss_l2=0.00592, loss_lpips=0.0793]\n",
      "Steps:   5%|▌         | 524/10000 [2:37:51<53:09:55, 20.20s/it, loss_gram=0, loss_l2=0.0123, loss_lpips=0.143]  \n",
      "Steps:   5%|▌         | 525/10000 [2:38:12<53:22:17, 20.28s/it, loss_gram=0, loss_l2=0.0159, loss_lpips=0.116]\n",
      "Steps:   5%|▌         | 526/10000 [2:38:32<53:31:08, 20.34s/it, loss_gram=0, loss_l2=0.00159, loss_lpips=0.0762]\n",
      "Steps:   5%|▌         | 527/10000 [2:38:52<53:20:57, 20.27s/it, loss_gram=0, loss_l2=0.0169, loss_lpips=0.175]  \n",
      "Steps:   5%|▌         | 528/10000 [2:39:13<53:18:52, 20.26s/it, loss_gram=0, loss_l2=0.00616, loss_lpips=0.135]\n",
      "Steps:   5%|▌         | 529/10000 [2:39:33<53:30:18, 20.34s/it, loss_gram=0, loss_l2=0.0071, loss_lpips=0.125] \n",
      "Steps:   5%|▌         | 530/10000 [2:39:54<53:55:33, 20.50s/it, loss_gram=0, loss_l2=0.002, loss_lpips=0.193] \n",
      "Steps:   5%|▌         | 531/10000 [2:40:14<53:17:50, 20.26s/it, loss_gram=0, loss_l2=0.0238, loss_lpips=0.129]\n",
      "Steps:   5%|▌         | 532/10000 [2:40:33<52:49:07, 20.08s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.118]\n",
      "Steps:   5%|▌         | 533/10000 [2:40:52<51:49:57, 19.71s/it, loss_gram=0, loss_l2=0.000972, loss_lpips=0.0737]\n",
      "Steps:   5%|▌         | 534/10000 [2:41:12<51:59:28, 19.77s/it, loss_gram=0, loss_l2=0.00282, loss_lpips=0.149]  \n",
      "Steps:   5%|▌         | 535/10000 [2:41:32<52:13:05, 19.86s/it, loss_gram=0, loss_l2=0.00225, loss_lpips=0.139]\n",
      "Steps:   5%|▌         | 536/10000 [2:41:52<52:34:28, 20.00s/it, loss_gram=0, loss_l2=0.00996, loss_lpips=0.136]\n",
      "Steps:   5%|▌         | 537/10000 [2:42:12<52:18:57, 19.90s/it, loss_gram=0, loss_l2=0.0371, loss_lpips=0.271] \n",
      "Steps:   5%|▌         | 538/10000 [2:42:32<52:23:31, 19.93s/it, loss_gram=0, loss_l2=0.00335, loss_lpips=0.0978]\n",
      "Steps:   5%|▌         | 539/10000 [2:42:52<52:04:24, 19.81s/it, loss_gram=0, loss_l2=0.00703, loss_lpips=0.106] \n",
      "Steps:   5%|▌         | 540/10000 [2:43:12<52:44:05, 20.07s/it, loss_gram=0, loss_l2=0.00366, loss_lpips=0.114]\n",
      "Steps:   5%|▌         | 541/10000 [2:43:33<53:00:57, 20.18s/it, loss_gram=0, loss_l2=0.00333, loss_lpips=0.0947]\n",
      "Steps:   5%|▌         | 542/10000 [2:43:52<52:18:38, 19.91s/it, loss_gram=0, loss_l2=0.000987, loss_lpips=0.101]\n",
      "Steps:   5%|▌         | 543/10000 [2:44:11<51:15:33, 19.51s/it, loss_gram=0, loss_l2=0.0213, loss_lpips=0.148]  \n",
      "Steps:   5%|▌         | 544/10000 [2:44:33<53:29:11, 20.36s/it, loss_gram=0, loss_l2=0.0052, loss_lpips=0.0663]\n",
      "Steps:   5%|▌         | 545/10000 [2:44:52<52:23:58, 19.95s/it, loss_gram=0, loss_l2=0.00153, loss_lpips=0.158]\n",
      "Steps:   5%|▌         | 546/10000 [2:45:12<52:13:50, 19.89s/it, loss_gram=0, loss_l2=0.00398, loss_lpips=0.112]\n",
      "Steps:   5%|▌         | 547/10000 [2:45:33<53:03:26, 20.21s/it, loss_gram=0, loss_l2=0.00696, loss_lpips=0.0819]\n",
      "Steps:   5%|▌         | 548/10000 [2:45:53<53:34:09, 20.40s/it, loss_gram=0, loss_l2=0.00133, loss_lpips=0.122] \n",
      "Steps:   5%|▌         | 549/10000 [2:46:13<52:47:12, 20.11s/it, loss_gram=0, loss_l2=0.00907, loss_lpips=0.183]\n",
      "Steps:   6%|▌         | 550/10000 [2:46:33<52:25:07, 19.97s/it, loss_gram=0, loss_l2=0.106, loss_lpips=0.302]  \n",
      "Steps:   6%|▌         | 551/10000 [2:46:51<51:11:22, 19.50s/it, loss_gram=0, loss_l2=0.0219, loss_lpips=0.126]\n",
      "Steps:   6%|▌         | 552/10000 [2:47:12<52:45:52, 20.11s/it, loss_gram=0, loss_l2=0.0173, loss_lpips=0.138]\n",
      "Steps:   6%|▌         | 553/10000 [2:47:31<51:33:28, 19.65s/it, loss_gram=0, loss_l2=0.0175, loss_lpips=0.182]\n",
      "Steps:   6%|▌         | 554/10000 [2:47:51<51:52:56, 19.77s/it, loss_gram=0, loss_l2=0.0214, loss_lpips=0.206]\n",
      "Steps:   6%|▌         | 555/10000 [2:48:11<52:06:02, 19.86s/it, loss_gram=0, loss_l2=0.0076, loss_lpips=0.15] \n",
      "Steps:   6%|▌         | 556/10000 [2:48:32<52:34:39, 20.04s/it, loss_gram=0, loss_l2=0.0398, loss_lpips=0.227]\n",
      "Steps:   6%|▌         | 557/10000 [2:48:51<51:59:44, 19.82s/it, loss_gram=0, loss_l2=0.0212, loss_lpips=0.145]\n",
      "Steps:   6%|▌         | 558/10000 [2:49:13<53:22:13, 20.35s/it, loss_gram=0, loss_l2=0.00638, loss_lpips=0.0989]\n",
      "Steps:   6%|▌         | 559/10000 [2:49:32<52:44:19, 20.11s/it, loss_gram=0, loss_l2=0.0117, loss_lpips=0.178]  \n",
      "Steps:   6%|▌         | 560/10000 [2:49:52<52:32:28, 20.04s/it, loss_gram=0, loss_l2=0.0119, loss_lpips=0.113]\n",
      "Steps:   6%|▌         | 561/10000 [2:50:12<52:46:53, 20.13s/it, loss_gram=0, loss_l2=0.00745, loss_lpips=0.0972]\n",
      "Steps:   6%|▌         | 562/10000 [2:50:33<53:05:10, 20.25s/it, loss_gram=0, loss_l2=0.00299, loss_lpips=0.154] \n",
      "Steps:   6%|▌         | 563/10000 [2:50:52<52:29:00, 20.02s/it, loss_gram=0, loss_l2=0.00361, loss_lpips=0.126]\n",
      "Steps:   6%|▌         | 564/10000 [2:51:13<52:59:15, 20.22s/it, loss_gram=0, loss_l2=0.0114, loss_lpips=0.131] \n",
      "Steps:   6%|▌         | 565/10000 [2:51:33<53:05:57, 20.26s/it, loss_gram=0, loss_l2=0.0132, loss_lpips=0.133]\n",
      "Steps:   6%|▌         | 566/10000 [2:51:55<53:20:39, 20.36s/it, loss_gram=0, loss_l2=0.00752, loss_lpips=0.104]\n",
      "Steps:   6%|▌         | 567/10000 [2:52:15<53:56:15, 20.58s/it, loss_gram=0, loss_l2=0.0145, loss_lpips=0.123] \n",
      "Steps:   6%|▌         | 568/10000 [2:52:37<54:51:02, 20.94s/it, loss_gram=0, loss_l2=0.0133, loss_lpips=0.149]\n",
      "Steps:   6%|▌         | 569/10000 [2:52:56<53:10:40, 20.30s/it, loss_gram=0, loss_l2=0.00423, loss_lpips=0.0947]\n",
      "Steps:   6%|▌         | 570/10000 [2:53:17<54:25:44, 20.78s/it, loss_gram=0, loss_l2=0.0147, loss_lpips=0.114]  \n",
      "Steps:   6%|▌         | 571/10000 [2:53:36<52:58:48, 20.23s/it, loss_gram=0, loss_l2=0.0163, loss_lpips=0.152]\n",
      "Steps:   6%|▌         | 572/10000 [2:53:56<52:25:33, 20.02s/it, loss_gram=0, loss_l2=0.0159, loss_lpips=0.088]\n",
      "Steps:   6%|▌         | 573/10000 [2:54:18<53:44:39, 20.52s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.109]\n",
      "Steps:   6%|▌         | 574/10000 [2:54:38<53:16:14, 20.35s/it, loss_gram=0, loss_l2=0.00911, loss_lpips=0.0906]\n",
      "Steps:   6%|▌         | 575/10000 [2:54:57<52:30:24, 20.06s/it, loss_gram=0, loss_l2=0.0132, loss_lpips=0.137]  \n",
      "Steps:   6%|▌         | 576/10000 [2:55:17<52:30:15, 20.06s/it, loss_gram=0, loss_l2=0.00509, loss_lpips=0.0801]\n",
      "Steps:   6%|▌         | 577/10000 [2:55:39<54:15:28, 20.73s/it, loss_gram=0, loss_l2=0.0106, loss_lpips=0.105]  \n",
      "Steps:   6%|▌         | 578/10000 [2:56:00<54:20:48, 20.77s/it, loss_gram=0, loss_l2=0.00245, loss_lpips=0.0701]\n",
      "Steps:   6%|▌         | 579/10000 [2:56:22<54:44:48, 20.92s/it, loss_gram=0, loss_l2=0.0068, loss_lpips=0.0994] \n",
      "Steps:   6%|▌         | 580/10000 [2:56:42<54:27:54, 20.81s/it, loss_gram=0, loss_l2=0.00254, loss_lpips=0.127]\n",
      "Steps:   6%|▌         | 581/10000 [2:57:02<53:34:38, 20.48s/it, loss_gram=0, loss_l2=0.00464, loss_lpips=0.131]\n",
      "Steps:   6%|▌         | 582/10000 [2:57:21<52:44:35, 20.16s/it, loss_gram=0, loss_l2=0.0122, loss_lpips=0.104] \n",
      "Steps:   6%|▌         | 583/10000 [2:57:42<52:38:04, 20.12s/it, loss_gram=0, loss_l2=0.0129, loss_lpips=0.131]\n",
      "Steps:   6%|▌         | 584/10000 [2:58:01<52:02:19, 19.90s/it, loss_gram=0, loss_l2=0.0127, loss_lpips=0.266]\n",
      "Steps:   6%|▌         | 585/10000 [2:58:21<52:43:41, 20.16s/it, loss_gram=0, loss_l2=0.00935, loss_lpips=0.0884]\n",
      "Steps:   6%|▌         | 586/10000 [2:58:43<53:44:49, 20.55s/it, loss_gram=0, loss_l2=0.0144, loss_lpips=0.151]  \n",
      "Steps:   6%|▌         | 587/10000 [2:59:03<53:21:09, 20.40s/it, loss_gram=0, loss_l2=0.00737, loss_lpips=0.135]\n",
      "Steps:   6%|▌         | 588/10000 [2:59:23<52:51:04, 20.22s/it, loss_gram=0, loss_l2=0.0042, loss_lpips=0.138] \n",
      "Steps:   6%|▌         | 589/10000 [2:59:44<53:04:44, 20.30s/it, loss_gram=0, loss_l2=0.00498, loss_lpips=0.107]\n",
      "Steps:   6%|▌         | 590/10000 [3:00:02<51:47:58, 19.82s/it, loss_gram=0, loss_l2=0.00741, loss_lpips=0.128]\n",
      "Steps:   6%|▌         | 591/10000 [3:00:23<52:34:06, 20.11s/it, loss_gram=0, loss_l2=0.00157, loss_lpips=0.0803]\n",
      "Steps:   6%|▌         | 592/10000 [3:00:44<53:41:09, 20.54s/it, loss_gram=0, loss_l2=0.0109, loss_lpips=0.0908] \n",
      "Steps:   6%|▌         | 593/10000 [3:01:04<53:08:36, 20.34s/it, loss_gram=0, loss_l2=0.00696, loss_lpips=0.105]\n",
      "Steps:   6%|▌         | 594/10000 [3:01:24<52:44:07, 20.18s/it, loss_gram=0, loss_l2=0.00722, loss_lpips=0.118]\n",
      "Steps:   6%|▌         | 595/10000 [3:01:44<52:49:54, 20.22s/it, loss_gram=0, loss_l2=0.0121, loss_lpips=0.0873]\n",
      "Steps:   6%|▌         | 596/10000 [3:02:06<53:48:09, 20.60s/it, loss_gram=0, loss_l2=0.00785, loss_lpips=0.12] \n",
      "Steps:   6%|▌         | 597/10000 [3:02:27<54:16:15, 20.78s/it, loss_gram=0, loss_l2=0.013, loss_lpips=0.12]  \n",
      "Steps:   6%|▌         | 598/10000 [3:02:48<54:35:48, 20.90s/it, loss_gram=0, loss_l2=0.0186, loss_lpips=0.106]\n",
      "Steps:   6%|▌         | 599/10000 [3:03:09<54:47:14, 20.98s/it, loss_gram=0, loss_l2=0.00134, loss_lpips=0.0955]\n",
      "Steps:   6%|▌         | 600/10000 [3:03:29<53:30:25, 20.49s/it, loss_gram=0, loss_l2=0.0109, loss_lpips=0.0706] \n",
      "Steps:   6%|▌         | 601/10000 [3:03:50<54:14:46, 20.78s/it, loss_gram=0, loss_l2=0.0215, loss_lpips=0.142] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   6%|▌         | 602/10000 [3:04:11<54:32:58, 20.90s/it, loss_gram=0, loss_l2=0.000744, loss_lpips=0.133]\n",
      "Steps:   6%|▌         | 603/10000 [3:04:32<54:34:26, 20.91s/it, loss_gram=0, loss_l2=0.0051, loss_lpips=0.122]  \n",
      "Steps:   6%|▌         | 604/10000 [3:04:53<54:33:44, 20.91s/it, loss_gram=0, loss_l2=0.00702, loss_lpips=0.0881]\n",
      "Steps:   6%|▌         | 605/10000 [3:05:13<54:07:30, 20.74s/it, loss_gram=0, loss_l2=0.0158, loss_lpips=0.219]  \n",
      "Steps:   6%|▌         | 606/10000 [3:05:33<53:08:36, 20.37s/it, loss_gram=0, loss_l2=0.00644, loss_lpips=0.0994]\n",
      "Steps:   6%|▌         | 607/10000 [3:05:52<52:10:47, 20.00s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.147]  \n",
      "Steps:   6%|▌         | 608/10000 [3:06:13<52:43:38, 20.21s/it, loss_gram=0, loss_l2=0.00821, loss_lpips=0.176]\n",
      "Steps:   6%|▌         | 609/10000 [3:06:32<52:21:01, 20.07s/it, loss_gram=0, loss_l2=0.00315, loss_lpips=0.137]\n",
      "Steps:   6%|▌         | 610/10000 [3:06:53<52:22:46, 20.08s/it, loss_gram=0, loss_l2=0.0584, loss_lpips=0.187] \n",
      "Steps:   6%|▌         | 611/10000 [3:07:15<53:59:03, 20.70s/it, loss_gram=0, loss_l2=0.00654, loss_lpips=0.134]\n",
      "Steps:   6%|▌         | 612/10000 [3:07:34<53:16:31, 20.43s/it, loss_gram=0, loss_l2=0.0118, loss_lpips=0.0917]\n",
      "Steps:   6%|▌         | 613/10000 [3:07:55<53:32:15, 20.53s/it, loss_gram=0, loss_l2=0.00398, loss_lpips=0.128]\n",
      "Steps:   6%|▌         | 614/10000 [3:08:15<53:12:46, 20.41s/it, loss_gram=0, loss_l2=0.00973, loss_lpips=0.144]\n",
      "Steps:   6%|▌         | 615/10000 [3:08:37<53:56:50, 20.69s/it, loss_gram=0, loss_l2=0.00977, loss_lpips=0.116]\n",
      "Steps:   6%|▌         | 616/10000 [3:08:58<54:23:20, 20.87s/it, loss_gram=0, loss_l2=0.00704, loss_lpips=0.136]\n",
      "Steps:   6%|▌         | 617/10000 [3:09:18<53:33:58, 20.55s/it, loss_gram=0, loss_l2=0.00527, loss_lpips=0.094]\n",
      "Steps:   6%|▌         | 618/10000 [3:09:39<54:01:58, 20.73s/it, loss_gram=0, loss_l2=0.00578, loss_lpips=0.138]\n",
      "Steps:   6%|▌         | 619/10000 [3:09:59<53:34:16, 20.56s/it, loss_gram=0, loss_l2=0.0157, loss_lpips=0.121] \n",
      "Steps:   6%|▌         | 620/10000 [3:10:20<53:32:19, 20.55s/it, loss_gram=0, loss_l2=0.0049, loss_lpips=0.0696]\n",
      "Steps:   6%|▌         | 621/10000 [3:10:40<53:31:00, 20.54s/it, loss_gram=0, loss_l2=0.0156, loss_lpips=0.132] \n",
      "Steps:   6%|▌         | 622/10000 [3:10:59<52:13:06, 20.05s/it, loss_gram=0, loss_l2=0.00184, loss_lpips=0.0779]\n",
      "Steps:   6%|▌         | 623/10000 [3:11:19<52:12:07, 20.04s/it, loss_gram=0, loss_l2=0.0183, loss_lpips=0.296]  \n",
      "Steps:   6%|▌         | 624/10000 [3:11:40<53:03:12, 20.37s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.0947]\n",
      "Steps:   6%|▋         | 625/10000 [3:12:02<53:54:18, 20.70s/it, loss_gram=0, loss_l2=0.0036, loss_lpips=0.126] \n",
      "Steps:   6%|▋         | 626/10000 [3:12:22<53:16:00, 20.46s/it, loss_gram=0, loss_l2=0.00293, loss_lpips=0.122]\n",
      "Steps:   6%|▋         | 627/10000 [3:12:42<52:50:00, 20.29s/it, loss_gram=0, loss_l2=0.00787, loss_lpips=0.163]\n",
      "Steps:   6%|▋         | 628/10000 [3:13:02<52:34:15, 20.19s/it, loss_gram=0, loss_l2=0.00848, loss_lpips=0.114]\n",
      "Steps:   6%|▋         | 629/10000 [3:13:21<52:10:32, 20.04s/it, loss_gram=0, loss_l2=0.00662, loss_lpips=0.128]\n",
      "Steps:   6%|▋         | 630/10000 [3:13:40<51:29:16, 19.78s/it, loss_gram=0, loss_l2=0.00592, loss_lpips=0.156]\n",
      "Steps:   6%|▋         | 631/10000 [3:14:00<51:09:41, 19.66s/it, loss_gram=0, loss_l2=0.00725, loss_lpips=0.129]\n",
      "Steps:   6%|▋         | 632/10000 [3:14:21<52:16:38, 20.09s/it, loss_gram=0, loss_l2=0.00444, loss_lpips=0.0947]\n",
      "Steps:   6%|▋         | 633/10000 [3:14:41<52:11:49, 20.06s/it, loss_gram=0, loss_l2=0.00391, loss_lpips=0.0822]\n",
      "Steps:   6%|▋         | 634/10000 [3:15:01<52:15:22, 20.09s/it, loss_gram=0, loss_l2=0.00813, loss_lpips=0.284] \n",
      "Steps:   6%|▋         | 635/10000 [3:15:20<51:22:19, 19.75s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.132] \n",
      "Steps:   6%|▋         | 636/10000 [3:15:40<51:17:25, 19.72s/it, loss_gram=0, loss_l2=0.00659, loss_lpips=0.191]\n",
      "Steps:   6%|▋         | 637/10000 [3:16:00<51:41:00, 19.87s/it, loss_gram=0, loss_l2=0.0257, loss_lpips=0.118] \n",
      "Steps:   6%|▋         | 638/10000 [3:16:20<51:43:29, 19.89s/it, loss_gram=0, loss_l2=0.00287, loss_lpips=0.136]\n",
      "Steps:   6%|▋         | 639/10000 [3:16:40<52:10:51, 20.07s/it, loss_gram=0, loss_l2=0.0275, loss_lpips=0.173] \n",
      "Steps:   6%|▋         | 640/10000 [3:17:00<52:08:46, 20.06s/it, loss_gram=0, loss_l2=0.00368, loss_lpips=0.107]\n",
      "Steps:   6%|▋         | 641/10000 [3:17:20<52:06:33, 20.04s/it, loss_gram=0, loss_l2=0.00527, loss_lpips=0.0897]\n",
      "Steps:   6%|▋         | 642/10000 [3:17:40<51:50:27, 19.94s/it, loss_gram=0, loss_l2=0.0156, loss_lpips=0.192]  \n",
      "Steps:   6%|▋         | 643/10000 [3:18:01<53:03:14, 20.41s/it, loss_gram=0, loss_l2=0.0138, loss_lpips=0.209]\n",
      "Steps:   6%|▋         | 644/10000 [3:18:23<53:53:01, 20.73s/it, loss_gram=0, loss_l2=0.00659, loss_lpips=0.12]\n",
      "Steps:   6%|▋         | 645/10000 [3:18:44<53:56:50, 20.76s/it, loss_gram=0, loss_l2=0.000546, loss_lpips=0.0847]\n",
      "Steps:   6%|▋         | 646/10000 [3:19:05<53:48:16, 20.71s/it, loss_gram=0, loss_l2=0.0154, loss_lpips=0.153]   \n",
      "Steps:   6%|▋         | 647/10000 [3:19:25<54:03:33, 20.81s/it, loss_gram=0, loss_l2=0.00331, loss_lpips=0.153]\n",
      "Steps:   6%|▋         | 648/10000 [3:19:45<52:48:31, 20.33s/it, loss_gram=0, loss_l2=0.00692, loss_lpips=0.119]\n",
      "Steps:   6%|▋         | 649/10000 [3:20:06<53:13:29, 20.49s/it, loss_gram=0, loss_l2=0.00192, loss_lpips=0.104]\n",
      "Steps:   6%|▋         | 650/10000 [3:20:26<53:04:26, 20.43s/it, loss_gram=0, loss_l2=0.00308, loss_lpips=0.094]\n",
      "Steps:   7%|▋         | 651/10000 [3:20:46<52:57:54, 20.40s/it, loss_gram=0, loss_l2=0.00482, loss_lpips=0.109]\n",
      "Steps:   7%|▋         | 652/10000 [3:21:07<53:03:26, 20.43s/it, loss_gram=0, loss_l2=0.0195, loss_lpips=0.228] \n",
      "Steps:   7%|▋         | 653/10000 [3:21:26<52:18:26, 20.15s/it, loss_gram=0, loss_l2=0.00865, loss_lpips=0.117]\n",
      "Steps:   7%|▋         | 654/10000 [3:21:47<52:42:30, 20.30s/it, loss_gram=0, loss_l2=0.00667, loss_lpips=0.107]\n",
      "Steps:   7%|▋         | 655/10000 [3:22:07<52:38:17, 20.28s/it, loss_gram=0, loss_l2=0.0205, loss_lpips=0.144] \n",
      "Steps:   7%|▋         | 656/10000 [3:22:28<52:50:31, 20.36s/it, loss_gram=0, loss_l2=0.00488, loss_lpips=0.124]\n",
      "Steps:   7%|▋         | 657/10000 [3:22:49<53:13:42, 20.51s/it, loss_gram=0, loss_l2=0.0134, loss_lpips=0.124] \n",
      "Steps:   7%|▋         | 658/10000 [3:23:10<53:55:51, 20.78s/it, loss_gram=0, loss_l2=0.0187, loss_lpips=0.156]\n",
      "Steps:   7%|▋         | 659/10000 [3:23:30<53:12:48, 20.51s/it, loss_gram=0, loss_l2=0.00427, loss_lpips=0.0814]\n",
      "Steps:   7%|▋         | 660/10000 [3:23:51<53:29:33, 20.62s/it, loss_gram=0, loss_l2=0.00145, loss_lpips=0.149] \n",
      "Steps:   7%|▋         | 661/10000 [3:24:09<52:06:55, 20.09s/it, loss_gram=0, loss_l2=0.011, loss_lpips=0.126]  \n",
      "Steps:   7%|▋         | 662/10000 [3:24:28<50:57:10, 19.64s/it, loss_gram=0, loss_l2=0.00856, loss_lpips=0.0837]\n",
      "Steps:   7%|▋         | 663/10000 [3:24:47<50:27:58, 19.46s/it, loss_gram=0, loss_l2=0.00487, loss_lpips=0.0759]\n",
      "Steps:   7%|▋         | 664/10000 [3:25:08<52:04:43, 20.08s/it, loss_gram=0, loss_l2=0.00887, loss_lpips=0.138] \n",
      "Steps:   7%|▋         | 665/10000 [3:25:28<51:35:42, 19.90s/it, loss_gram=0, loss_l2=0.0247, loss_lpips=0.142] \n",
      "Steps:   7%|▋         | 666/10000 [3:25:48<51:21:19, 19.81s/it, loss_gram=0, loss_l2=0.000656, loss_lpips=0.0891]\n",
      "Steps:   7%|▋         | 667/10000 [3:26:08<52:01:14, 20.07s/it, loss_gram=0, loss_l2=0.00633, loss_lpips=0.0946] \n",
      "Steps:   7%|▋         | 668/10000 [3:26:27<51:07:08, 19.72s/it, loss_gram=0, loss_l2=0.00966, loss_lpips=0.12]  \n",
      "Steps:   7%|▋         | 669/10000 [3:26:48<52:04:43, 20.09s/it, loss_gram=0, loss_l2=0.00133, loss_lpips=0.106]\n",
      "Steps:   7%|▋         | 670/10000 [3:27:09<52:23:46, 20.22s/it, loss_gram=0, loss_l2=0.00647, loss_lpips=0.109]\n",
      "Steps:   7%|▋         | 671/10000 [3:27:30<52:38:32, 20.31s/it, loss_gram=0, loss_l2=0.00724, loss_lpips=0.0793]\n",
      "Steps:   7%|▋         | 672/10000 [3:27:49<52:33:51, 20.29s/it, loss_gram=0, loss_l2=0.0161, loss_lpips=0.129]  \n",
      "Steps:   7%|▋         | 673/10000 [3:28:10<52:27:23, 20.25s/it, loss_gram=0, loss_l2=0.00719, loss_lpips=0.088]\n",
      "Steps:   7%|▋         | 674/10000 [3:28:29<51:27:47, 19.87s/it, loss_gram=0, loss_l2=0.0183, loss_lpips=0.155] \n",
      "Steps:   7%|▋         | 675/10000 [3:28:50<52:47:17, 20.38s/it, loss_gram=0, loss_l2=0.0036, loss_lpips=0.103]\n",
      "Steps:   7%|▋         | 676/10000 [3:29:10<52:38:29, 20.32s/it, loss_gram=0, loss_l2=0.00555, loss_lpips=0.128]\n",
      "Steps:   7%|▋         | 677/10000 [3:29:31<53:01:20, 20.47s/it, loss_gram=0, loss_l2=0.0201, loss_lpips=0.203] \n",
      "Steps:   7%|▋         | 678/10000 [3:29:49<51:07:46, 19.75s/it, loss_gram=0, loss_l2=0.00371, loss_lpips=0.104]\n",
      "Steps:   7%|▋         | 679/10000 [3:30:09<51:20:01, 19.83s/it, loss_gram=0, loss_l2=0.0137, loss_lpips=0.259] \n",
      "Steps:   7%|▋         | 680/10000 [3:30:28<50:20:55, 19.45s/it, loss_gram=0, loss_l2=0.00852, loss_lpips=0.0898]\n",
      "Steps:   7%|▋         | 681/10000 [3:30:47<49:42:21, 19.20s/it, loss_gram=0, loss_l2=0.00303, loss_lpips=0.0784]\n",
      "Steps:   7%|▋         | 682/10000 [3:31:07<50:43:29, 19.60s/it, loss_gram=0, loss_l2=0.00509, loss_lpips=0.1]   \n",
      "Steps:   7%|▋         | 683/10000 [3:31:26<50:09:11, 19.38s/it, loss_gram=0, loss_l2=0.0253, loss_lpips=0.152]\n",
      "Steps:   7%|▋         | 684/10000 [3:31:46<50:30:33, 19.52s/it, loss_gram=0, loss_l2=0.0215, loss_lpips=0.166]\n",
      "Steps:   7%|▋         | 685/10000 [3:32:07<51:55:01, 20.06s/it, loss_gram=0, loss_l2=0.00996, loss_lpips=0.125]\n",
      "Steps:   7%|▋         | 686/10000 [3:32:26<51:25:15, 19.87s/it, loss_gram=0, loss_l2=0.00416, loss_lpips=0.0833]\n",
      "Steps:   7%|▋         | 687/10000 [3:32:46<51:24:24, 19.87s/it, loss_gram=0, loss_l2=0.00464, loss_lpips=0.104] \n",
      "Steps:   7%|▋         | 688/10000 [3:33:07<51:54:09, 20.07s/it, loss_gram=0, loss_l2=0.0112, loss_lpips=0.125] \n",
      "Steps:   7%|▋         | 689/10000 [3:33:27<52:01:06, 20.11s/it, loss_gram=0, loss_l2=0.015, loss_lpips=0.12]  \n",
      "Steps:   7%|▋         | 690/10000 [3:33:46<50:47:13, 19.64s/it, loss_gram=0, loss_l2=0.0478, loss_lpips=0.171]\n",
      "Steps:   7%|▋         | 691/10000 [3:34:08<52:36:55, 20.35s/it, loss_gram=0, loss_l2=0.000752, loss_lpips=0.128]\n",
      "Steps:   7%|▋         | 692/10000 [3:34:28<52:27:16, 20.29s/it, loss_gram=0, loss_l2=0.0348, loss_lpips=0.117]  \n",
      "Steps:   7%|▋         | 693/10000 [3:34:48<52:45:01, 20.40s/it, loss_gram=0, loss_l2=0.00684, loss_lpips=0.0991]\n",
      "Steps:   7%|▋         | 694/10000 [3:35:10<53:32:40, 20.71s/it, loss_gram=0, loss_l2=0.00694, loss_lpips=0.151] \n",
      "Steps:   7%|▋         | 695/10000 [3:35:31<53:40:47, 20.77s/it, loss_gram=0, loss_l2=0.00846, loss_lpips=0.197]\n",
      "Steps:   7%|▋         | 696/10000 [3:35:52<53:47:48, 20.82s/it, loss_gram=0, loss_l2=0.00487, loss_lpips=0.105]\n",
      "Steps:   7%|▋         | 697/10000 [3:36:12<52:46:46, 20.42s/it, loss_gram=0, loss_l2=0.0194, loss_lpips=0.148] \n",
      "Steps:   7%|▋         | 698/10000 [3:36:34<54:22:00, 21.04s/it, loss_gram=0, loss_l2=0.0046, loss_lpips=0.127]\n",
      "Steps:   7%|▋         | 699/10000 [3:36:55<54:13:52, 20.99s/it, loss_gram=0, loss_l2=0.00623, loss_lpips=0.111]\n",
      "Steps:   7%|▋         | 700/10000 [3:37:15<54:09:14, 20.96s/it, loss_gram=0, loss_l2=0.00743, loss_lpips=0.114]\n",
      "Steps:   7%|▋         | 701/10000 [3:37:36<53:30:32, 20.72s/it, loss_gram=0, loss_l2=0.0303, loss_lpips=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   7%|▋         | 702/10000 [3:37:58<54:39:03, 21.16s/it, loss_gram=0, loss_l2=0.00621, loss_lpips=0.12]\n",
      "Steps:   7%|▋         | 703/10000 [3:38:18<53:45:51, 20.82s/it, loss_gram=0, loss_l2=0.0508, loss_lpips=0.139]\n",
      "Steps:   7%|▋         | 704/10000 [3:38:39<54:13:56, 21.00s/it, loss_gram=0, loss_l2=0.00759, loss_lpips=0.0854]\n",
      "Steps:   7%|▋         | 705/10000 [3:38:59<53:08:28, 20.58s/it, loss_gram=0, loss_l2=0.00674, loss_lpips=0.0718]\n",
      "Steps:   7%|▋         | 706/10000 [3:39:18<51:56:00, 20.12s/it, loss_gram=0, loss_l2=0.0116, loss_lpips=0.115]  \n",
      "Steps:   7%|▋         | 707/10000 [3:39:39<52:40:38, 20.41s/it, loss_gram=0, loss_l2=0.00586, loss_lpips=0.0691]\n",
      "Steps:   7%|▋         | 708/10000 [3:40:00<52:43:59, 20.43s/it, loss_gram=0, loss_l2=0.0088, loss_lpips=0.104]  \n",
      "Steps:   7%|▋         | 709/10000 [3:40:20<52:27:23, 20.33s/it, loss_gram=0, loss_l2=0.00809, loss_lpips=0.111]\n",
      "Steps:   7%|▋         | 710/10000 [3:40:41<53:45:27, 20.83s/it, loss_gram=0, loss_l2=0.00248, loss_lpips=0.164]\n",
      "Steps:   7%|▋         | 711/10000 [3:41:01<52:42:04, 20.42s/it, loss_gram=0, loss_l2=0.00988, loss_lpips=0.119]\n",
      "Steps:   7%|▋         | 712/10000 [3:41:22<53:06:30, 20.58s/it, loss_gram=0, loss_l2=0.00472, loss_lpips=0.171]\n",
      "Steps:   7%|▋         | 713/10000 [3:41:41<52:09:13, 20.22s/it, loss_gram=0, loss_l2=0.00665, loss_lpips=0.089]\n",
      "Steps:   7%|▋         | 714/10000 [3:42:03<53:10:50, 20.62s/it, loss_gram=0, loss_l2=0.0174, loss_lpips=0.124] \n",
      "Steps:   7%|▋         | 715/10000 [3:42:23<53:10:24, 20.62s/it, loss_gram=0, loss_l2=0.00106, loss_lpips=0.14]\n",
      "Steps:   7%|▋         | 716/10000 [3:42:44<52:51:50, 20.50s/it, loss_gram=0, loss_l2=0.00495, loss_lpips=0.0951]\n",
      "Steps:   7%|▋         | 717/10000 [3:43:04<52:50:52, 20.49s/it, loss_gram=0, loss_l2=0.000864, loss_lpips=0.106]\n",
      "Steps:   7%|▋         | 718/10000 [3:43:24<52:00:23, 20.17s/it, loss_gram=0, loss_l2=0.0472, loss_lpips=0.127]  \n",
      "Steps:   7%|▋         | 719/10000 [3:43:45<52:47:07, 20.47s/it, loss_gram=0, loss_l2=0.00578, loss_lpips=0.0728]\n",
      "Steps:   7%|▋         | 720/10000 [3:44:04<51:59:46, 20.17s/it, loss_gram=0, loss_l2=0.0183, loss_lpips=0.108]  \n",
      "Steps:   7%|▋         | 721/10000 [3:44:25<52:03:59, 20.20s/it, loss_gram=0, loss_l2=0.00636, loss_lpips=0.0786]\n",
      "Steps:   7%|▋         | 722/10000 [3:44:44<51:51:06, 20.12s/it, loss_gram=0, loss_l2=0.00247, loss_lpips=0.131] \n",
      "Steps:   7%|▋         | 723/10000 [3:45:05<52:19:05, 20.30s/it, loss_gram=0, loss_l2=0.00493, loss_lpips=0.201]\n",
      "Steps:   7%|▋         | 724/10000 [3:45:25<52:19:41, 20.31s/it, loss_gram=0, loss_l2=0.00804, loss_lpips=0.109]\n",
      "Steps:   7%|▋         | 725/10000 [3:45:45<51:51:14, 20.13s/it, loss_gram=0, loss_l2=0.00282, loss_lpips=0.074]\n",
      "Steps:   7%|▋         | 726/10000 [3:46:06<52:28:11, 20.37s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.166] \n",
      "Steps:   7%|▋         | 727/10000 [3:46:26<52:11:51, 20.26s/it, loss_gram=0, loss_l2=0.0285, loss_lpips=0.118]\n",
      "Steps:   7%|▋         | 728/10000 [3:46:46<51:58:59, 20.18s/it, loss_gram=0, loss_l2=0.00379, loss_lpips=0.126]\n",
      "Steps:   7%|▋         | 729/10000 [3:47:06<51:53:29, 20.15s/it, loss_gram=0, loss_l2=0.0167, loss_lpips=0.0773]\n",
      "Steps:   7%|▋         | 730/10000 [3:47:26<51:00:20, 19.81s/it, loss_gram=0, loss_l2=0.00756, loss_lpips=0.112]\n",
      "Steps:   7%|▋         | 731/10000 [3:47:45<50:59:11, 19.80s/it, loss_gram=0, loss_l2=0.00288, loss_lpips=0.111]\n",
      "Steps:   7%|▋         | 732/10000 [3:48:06<51:32:18, 20.02s/it, loss_gram=0, loss_l2=0.0131, loss_lpips=0.134] \n",
      "Steps:   7%|▋         | 733/10000 [3:48:26<51:47:24, 20.12s/it, loss_gram=0, loss_l2=0.0226, loss_lpips=0.214]\n",
      "Steps:   7%|▋         | 734/10000 [3:48:45<51:06:56, 19.86s/it, loss_gram=0, loss_l2=0.0173, loss_lpips=0.177]\n",
      "Steps:   7%|▋         | 735/10000 [3:49:06<51:28:48, 20.00s/it, loss_gram=0, loss_l2=0.0314, loss_lpips=0.12] \n",
      "Steps:   7%|▋         | 736/10000 [3:49:26<51:43:52, 20.10s/it, loss_gram=0, loss_l2=0.0109, loss_lpips=0.102]\n",
      "Steps:   7%|▋         | 737/10000 [3:49:45<51:18:21, 19.94s/it, loss_gram=0, loss_l2=0.00967, loss_lpips=0.113]\n",
      "Steps:   7%|▋         | 738/10000 [3:50:07<52:12:31, 20.29s/it, loss_gram=0, loss_l2=0.00418, loss_lpips=0.0859]\n",
      "Steps:   7%|▋         | 739/10000 [3:50:27<52:17:47, 20.33s/it, loss_gram=0, loss_l2=0.0246, loss_lpips=0.161]  \n",
      "Steps:   7%|▋         | 740/10000 [3:50:46<51:25:55, 20.00s/it, loss_gram=0, loss_l2=0.00332, loss_lpips=0.0942]\n",
      "Steps:   7%|▋         | 741/10000 [3:51:05<50:46:20, 19.74s/it, loss_gram=0, loss_l2=0.00232, loss_lpips=0.118] \n",
      "Steps:   7%|▋         | 742/10000 [3:51:25<50:53:07, 19.79s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.153] \n",
      "Steps:   7%|▋         | 743/10000 [3:51:46<51:40:01, 20.09s/it, loss_gram=0, loss_l2=0.0124, loss_lpips=0.133]\n",
      "Steps:   7%|▋         | 744/10000 [3:52:06<51:52:15, 20.17s/it, loss_gram=0, loss_l2=0.00916, loss_lpips=0.126]\n",
      "Steps:   7%|▋         | 745/10000 [3:52:27<51:39:22, 20.09s/it, loss_gram=0, loss_l2=0.00951, loss_lpips=0.103]\n",
      "Steps:   7%|▋         | 746/10000 [3:52:48<52:53:18, 20.57s/it, loss_gram=0, loss_l2=0.00217, loss_lpips=0.151]\n",
      "Steps:   7%|▋         | 747/10000 [3:53:08<52:11:15, 20.30s/it, loss_gram=0, loss_l2=0.00232, loss_lpips=0.0957]\n",
      "Steps:   7%|▋         | 748/10000 [3:53:29<52:59:49, 20.62s/it, loss_gram=0, loss_l2=0.00228, loss_lpips=0.087] \n",
      "Steps:   7%|▋         | 749/10000 [3:53:48<51:53:52, 20.20s/it, loss_gram=0, loss_l2=0.133, loss_lpips=0.183]  \n",
      "Steps:   8%|▊         | 750/10000 [3:54:08<51:44:01, 20.13s/it, loss_gram=0, loss_l2=0.00844, loss_lpips=0.128]\n",
      "Steps:   8%|▊         | 751/10000 [3:54:29<52:03:54, 20.27s/it, loss_gram=0, loss_l2=0.015, loss_lpips=0.155]  \n",
      "Steps:   8%|▊         | 752/10000 [3:54:49<52:03:16, 20.26s/it, loss_gram=0, loss_l2=0.00553, loss_lpips=0.0851]\n",
      "Steps:   8%|▊         | 753/10000 [3:55:10<52:58:00, 20.62s/it, loss_gram=0, loss_l2=0.0103, loss_lpips=0.133]  \n",
      "Steps:   8%|▊         | 754/10000 [3:55:29<51:01:01, 19.86s/it, loss_gram=0, loss_l2=0.00955, loss_lpips=0.149]\n",
      "Steps:   8%|▊         | 755/10000 [3:55:49<51:07:04, 19.91s/it, loss_gram=0, loss_l2=0.00525, loss_lpips=0.181]\n",
      "Steps:   8%|▊         | 756/10000 [3:56:08<50:51:26, 19.81s/it, loss_gram=0, loss_l2=0.0116, loss_lpips=0.167] \n",
      "Steps:   8%|▊         | 757/10000 [3:56:27<50:32:21, 19.68s/it, loss_gram=0, loss_l2=0.0125, loss_lpips=0.146]\n",
      "Steps:   8%|▊         | 758/10000 [3:56:49<51:42:58, 20.14s/it, loss_gram=0, loss_l2=0.00422, loss_lpips=0.132]\n",
      "Steps:   8%|▊         | 759/10000 [3:57:08<51:02:59, 19.89s/it, loss_gram=0, loss_l2=0.00487, loss_lpips=0.123]\n",
      "Steps:   8%|▊         | 760/10000 [3:57:27<50:32:32, 19.69s/it, loss_gram=0, loss_l2=0.00336, loss_lpips=0.142]\n",
      "Steps:   8%|▊         | 761/10000 [3:57:47<50:35:19, 19.71s/it, loss_gram=0, loss_l2=0.00606, loss_lpips=0.152]\n",
      "Steps:   8%|▊         | 762/10000 [3:58:07<50:34:28, 19.71s/it, loss_gram=0, loss_l2=0.0058, loss_lpips=0.093] \n",
      "Steps:   8%|▊         | 763/10000 [3:58:27<50:40:18, 19.75s/it, loss_gram=0, loss_l2=0.00857, loss_lpips=0.211]\n",
      "Steps:   8%|▊         | 764/10000 [3:58:48<52:11:20, 20.34s/it, loss_gram=0, loss_l2=0.00608, loss_lpips=0.124]\n",
      "Steps:   8%|▊         | 765/10000 [3:59:09<52:39:02, 20.52s/it, loss_gram=0, loss_l2=0.00294, loss_lpips=0.147]\n",
      "Steps:   8%|▊         | 766/10000 [3:59:28<51:19:29, 20.01s/it, loss_gram=0, loss_l2=0.00739, loss_lpips=0.114]\n",
      "Steps:   8%|▊         | 767/10000 [3:59:49<52:10:25, 20.34s/it, loss_gram=0, loss_l2=0.0154, loss_lpips=0.141] \n",
      "Steps:   8%|▊         | 768/10000 [4:00:11<53:20:04, 20.80s/it, loss_gram=0, loss_l2=0.00157, loss_lpips=0.0768]\n",
      "Steps:   8%|▊         | 769/10000 [4:00:31<52:41:40, 20.55s/it, loss_gram=0, loss_l2=0.00623, loss_lpips=0.143] \n",
      "Steps:   8%|▊         | 770/10000 [4:00:51<52:05:08, 20.32s/it, loss_gram=0, loss_l2=0.0114, loss_lpips=0.103] \n",
      "Steps:   8%|▊         | 771/10000 [4:01:10<51:24:50, 20.06s/it, loss_gram=0, loss_l2=0.0139, loss_lpips=0.183]\n",
      "Steps:   8%|▊         | 772/10000 [4:01:31<51:40:08, 20.16s/it, loss_gram=0, loss_l2=0.0155, loss_lpips=0.139]\n",
      "Steps:   8%|▊         | 773/10000 [4:01:53<53:14:12, 20.77s/it, loss_gram=0, loss_l2=0.0198, loss_lpips=0.117]\n",
      "Steps:   8%|▊         | 774/10000 [4:02:13<52:34:16, 20.51s/it, loss_gram=0, loss_l2=0.0199, loss_lpips=0.13] \n",
      "Steps:   8%|▊         | 775/10000 [4:02:36<54:22:38, 21.22s/it, loss_gram=0, loss_l2=0.00356, loss_lpips=0.0909]\n",
      "Steps:   8%|▊         | 776/10000 [4:02:56<53:43:47, 20.97s/it, loss_gram=0, loss_l2=0.00888, loss_lpips=0.114] \n",
      "Steps:   8%|▊         | 777/10000 [4:03:17<53:53:24, 21.03s/it, loss_gram=0, loss_l2=0.00673, loss_lpips=0.0906]\n",
      "Steps:   8%|▊         | 778/10000 [4:03:39<54:12:45, 21.16s/it, loss_gram=0, loss_l2=0.00837, loss_lpips=0.126] \n",
      "Steps:   8%|▊         | 779/10000 [4:04:00<54:15:19, 21.18s/it, loss_gram=0, loss_l2=0.00305, loss_lpips=0.0927]\n",
      "Steps:   8%|▊         | 780/10000 [4:04:20<53:27:55, 20.88s/it, loss_gram=0, loss_l2=0.00884, loss_lpips=0.13]  \n",
      "Steps:   8%|▊         | 781/10000 [4:04:40<52:38:31, 20.56s/it, loss_gram=0, loss_l2=0.0231, loss_lpips=0.0942]\n",
      "Steps:   8%|▊         | 782/10000 [4:04:59<51:39:15, 20.17s/it, loss_gram=0, loss_l2=0.00458, loss_lpips=0.102]\n",
      "Steps:   8%|▊         | 783/10000 [4:05:19<51:37:10, 20.16s/it, loss_gram=0, loss_l2=0.0155, loss_lpips=0.122] \n",
      "Steps:   8%|▊         | 784/10000 [4:05:39<51:34:31, 20.15s/it, loss_gram=0, loss_l2=0.00788, loss_lpips=0.133]\n",
      "Steps:   8%|▊         | 785/10000 [4:06:00<51:39:05, 20.18s/it, loss_gram=0, loss_l2=0.00544, loss_lpips=0.0945]\n",
      "Steps:   8%|▊         | 786/10000 [4:06:20<52:12:26, 20.40s/it, loss_gram=0, loss_l2=0.0047, loss_lpips=0.0833] \n",
      "Steps:   8%|▊         | 787/10000 [4:06:40<51:21:45, 20.07s/it, loss_gram=0, loss_l2=0.00881, loss_lpips=0.0984]\n",
      "Steps:   8%|▊         | 788/10000 [4:07:01<52:22:54, 20.47s/it, loss_gram=0, loss_l2=0.011, loss_lpips=0.189]   \n",
      "Steps:   8%|▊         | 789/10000 [4:07:22<53:03:08, 20.73s/it, loss_gram=0, loss_l2=0.00307, loss_lpips=0.125]\n",
      "Steps:   8%|▊         | 790/10000 [4:07:43<52:43:31, 20.61s/it, loss_gram=0, loss_l2=0.0149, loss_lpips=0.0918]\n",
      "Steps:   8%|▊         | 791/10000 [4:08:04<52:46:25, 20.63s/it, loss_gram=0, loss_l2=0.0154, loss_lpips=0.138] \n",
      "Steps:   8%|▊         | 792/10000 [4:08:23<51:42:40, 20.22s/it, loss_gram=0, loss_l2=0.00427, loss_lpips=0.11]\n",
      "Steps:   8%|▊         | 793/10000 [4:08:41<50:09:37, 19.61s/it, loss_gram=0, loss_l2=0.00369, loss_lpips=0.0943]\n",
      "Steps:   8%|▊         | 794/10000 [4:09:02<50:35:05, 19.78s/it, loss_gram=0, loss_l2=0.0154, loss_lpips=0.123]  \n",
      "Steps:   8%|▊         | 795/10000 [4:09:22<51:10:01, 20.01s/it, loss_gram=0, loss_l2=0.011, loss_lpips=0.14]  \n",
      "Steps:   8%|▊         | 796/10000 [4:09:42<51:10:44, 20.02s/it, loss_gram=0, loss_l2=0.0175, loss_lpips=0.105]\n",
      "Steps:   8%|▊         | 797/10000 [4:10:02<51:32:46, 20.16s/it, loss_gram=0, loss_l2=0.00305, loss_lpips=0.0903]\n",
      "Steps:   8%|▊         | 798/10000 [4:10:24<52:38:37, 20.60s/it, loss_gram=0, loss_l2=0.00703, loss_lpips=0.135] \n",
      "Steps:   8%|▊         | 799/10000 [4:10:45<52:53:11, 20.69s/it, loss_gram=0, loss_l2=0.00526, loss_lpips=0.162]\n",
      "Steps:   8%|▊         | 800/10000 [4:11:05<52:40:52, 20.61s/it, loss_gram=0, loss_l2=0.0395, loss_lpips=0.136] \n",
      "Steps:   8%|▊         | 801/10000 [4:11:26<52:51:45, 20.69s/it, loss_gram=0, loss_l2=0.00475, loss_lpips=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   8%|▊         | 802/10000 [4:11:47<52:45:50, 20.65s/it, loss_gram=0, loss_l2=0.00252, loss_lpips=0.0762]\n",
      "Steps:   8%|▊         | 803/10000 [4:12:07<52:22:17, 20.50s/it, loss_gram=0, loss_l2=0.0451, loss_lpips=0.139]  \n",
      "Steps:   8%|▊         | 804/10000 [4:12:26<51:23:17, 20.12s/it, loss_gram=0, loss_l2=0.000784, loss_lpips=0.0836]\n",
      "Steps:   8%|▊         | 805/10000 [4:12:45<50:31:04, 19.78s/it, loss_gram=0, loss_l2=0.00126, loss_lpips=0.0699] \n",
      "Steps:   8%|▊         | 806/10000 [4:13:04<49:49:57, 19.51s/it, loss_gram=0, loss_l2=0.00695, loss_lpips=0.104] \n",
      "Steps:   8%|▊         | 807/10000 [4:13:24<49:54:56, 19.55s/it, loss_gram=0, loss_l2=0.000927, loss_lpips=0.182]\n",
      "Steps:   8%|▊         | 808/10000 [4:13:45<51:02:31, 19.99s/it, loss_gram=0, loss_l2=0.00367, loss_lpips=0.146] \n",
      "Steps:   8%|▊         | 809/10000 [4:14:06<51:54:33, 20.33s/it, loss_gram=0, loss_l2=0.00711, loss_lpips=0.128]\n",
      "Steps:   8%|▊         | 810/10000 [4:14:25<51:13:46, 20.07s/it, loss_gram=0, loss_l2=0.0133, loss_lpips=0.123] \n",
      "Steps:   8%|▊         | 811/10000 [4:14:46<51:20:39, 20.12s/it, loss_gram=0, loss_l2=0.00469, loss_lpips=0.118]\n",
      "Steps:   8%|▊         | 812/10000 [4:15:06<51:32:45, 20.20s/it, loss_gram=0, loss_l2=0.0264, loss_lpips=0.173] \n",
      "Steps:   8%|▊         | 813/10000 [4:15:26<51:40:31, 20.25s/it, loss_gram=0, loss_l2=0.0332, loss_lpips=0.175]\n",
      "Steps:   8%|▊         | 814/10000 [4:15:46<51:18:34, 20.11s/it, loss_gram=0, loss_l2=0.00844, loss_lpips=0.145]\n",
      "Steps:   8%|▊         | 815/10000 [4:16:06<51:26:36, 20.16s/it, loss_gram=0, loss_l2=0.00382, loss_lpips=0.0944]\n",
      "Steps:   8%|▊         | 816/10000 [4:16:25<50:28:17, 19.78s/it, loss_gram=0, loss_l2=0.00603, loss_lpips=0.104] \n",
      "Steps:   8%|▊         | 817/10000 [4:16:46<51:10:55, 20.06s/it, loss_gram=0, loss_l2=0.0072, loss_lpips=0.0663]\n",
      "Steps:   8%|▊         | 818/10000 [4:17:06<51:20:07, 20.13s/it, loss_gram=0, loss_l2=0.014, loss_lpips=0.155]  \n",
      "Steps:   8%|▊         | 819/10000 [4:17:25<50:09:46, 19.67s/it, loss_gram=0, loss_l2=0.003, loss_lpips=0.129]\n",
      "Steps:   8%|▊         | 820/10000 [4:17:44<50:08:54, 19.67s/it, loss_gram=0, loss_l2=0.0209, loss_lpips=0.111]\n",
      "Steps:   8%|▊         | 821/10000 [4:18:05<51:08:19, 20.06s/it, loss_gram=0, loss_l2=0.00609, loss_lpips=0.165]\n",
      "Steps:   8%|▊         | 822/10000 [4:18:24<50:20:07, 19.74s/it, loss_gram=0, loss_l2=0.00823, loss_lpips=0.119]\n",
      "Steps:   8%|▊         | 823/10000 [4:18:42<48:45:50, 19.13s/it, loss_gram=0, loss_l2=0.00731, loss_lpips=0.21] \n",
      "Steps:   8%|▊         | 824/10000 [4:19:03<50:09:41, 19.68s/it, loss_gram=0, loss_l2=0.0136, loss_lpips=0.12] \n",
      "Steps:   8%|▊         | 825/10000 [4:19:23<50:12:58, 19.70s/it, loss_gram=0, loss_l2=0.0155, loss_lpips=0.141]\n",
      "Steps:   8%|▊         | 826/10000 [4:19:46<52:31:14, 20.61s/it, loss_gram=0, loss_l2=0.00894, loss_lpips=0.0732]\n",
      "Steps:   8%|▊         | 827/10000 [4:20:06<52:22:23, 20.55s/it, loss_gram=0, loss_l2=0.00796, loss_lpips=0.128] \n",
      "Steps:   8%|▊         | 828/10000 [4:20:25<50:57:31, 20.00s/it, loss_gram=0, loss_l2=0.00446, loss_lpips=0.0955]\n",
      "Steps:   8%|▊         | 829/10000 [4:20:44<50:17:26, 19.74s/it, loss_gram=0, loss_l2=0.022, loss_lpips=0.183]   \n",
      "Steps:   8%|▊         | 830/10000 [4:21:04<50:45:01, 19.92s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.146]\n",
      "Steps:   8%|▊         | 831/10000 [4:21:24<50:29:31, 19.82s/it, loss_gram=0, loss_l2=0.0118, loss_lpips=0.0737]\n",
      "Steps:   8%|▊         | 832/10000 [4:21:44<51:05:52, 20.06s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.122] \n",
      "Steps:   8%|▊         | 833/10000 [4:22:05<51:06:40, 20.07s/it, loss_gram=0, loss_l2=0.00876, loss_lpips=0.111]\n",
      "Steps:   8%|▊         | 834/10000 [4:22:26<51:59:27, 20.42s/it, loss_gram=0, loss_l2=0.0163, loss_lpips=0.13]  \n",
      "Steps:   8%|▊         | 835/10000 [4:22:47<52:28:47, 20.61s/it, loss_gram=0, loss_l2=0.0055, loss_lpips=0.0911]\n",
      "Steps:   8%|▊         | 836/10000 [4:23:07<52:21:03, 20.57s/it, loss_gram=0, loss_l2=0.0123, loss_lpips=0.0798]\n",
      "Steps:   8%|▊         | 837/10000 [4:23:28<52:14:18, 20.52s/it, loss_gram=0, loss_l2=0.00582, loss_lpips=0.145]\n",
      "Steps:   8%|▊         | 838/10000 [4:23:49<52:32:01, 20.64s/it, loss_gram=0, loss_l2=0.00226, loss_lpips=0.0618]\n",
      "Steps:   8%|▊         | 839/10000 [4:24:08<51:16:53, 20.15s/it, loss_gram=0, loss_l2=0.000801, loss_lpips=0.133]\n",
      "Steps:   8%|▊         | 840/10000 [4:24:28<51:06:35, 20.09s/it, loss_gram=0, loss_l2=0.0101, loss_lpips=0.0867] \n",
      "Steps:   8%|▊         | 841/10000 [4:24:47<50:21:44, 19.80s/it, loss_gram=0, loss_l2=0.0071, loss_lpips=0.123] \n",
      "Steps:   8%|▊         | 842/10000 [4:25:06<50:05:14, 19.69s/it, loss_gram=0, loss_l2=0.00583, loss_lpips=0.115]\n",
      "Steps:   8%|▊         | 843/10000 [4:25:27<50:48:17, 19.97s/it, loss_gram=0, loss_l2=0.00842, loss_lpips=0.112]\n",
      "Steps:   8%|▊         | 844/10000 [4:25:48<51:54:17, 20.41s/it, loss_gram=0, loss_l2=0.00447, loss_lpips=0.0974]\n",
      "Steps:   8%|▊         | 845/10000 [4:26:07<51:06:27, 20.10s/it, loss_gram=0, loss_l2=0.0101, loss_lpips=0.0857] \n",
      "Steps:   8%|▊         | 846/10000 [4:26:27<50:35:46, 19.90s/it, loss_gram=0, loss_l2=0.00652, loss_lpips=0.125]\n",
      "Steps:   8%|▊         | 847/10000 [4:26:47<50:38:09, 19.92s/it, loss_gram=0, loss_l2=0.00523, loss_lpips=0.114]\n",
      "Steps:   8%|▊         | 848/10000 [4:27:08<51:32:44, 20.28s/it, loss_gram=0, loss_l2=0.0139, loss_lpips=0.117] \n",
      "Steps:   8%|▊         | 849/10000 [4:27:27<50:53:49, 20.02s/it, loss_gram=0, loss_l2=0.00507, loss_lpips=0.128]\n",
      "Steps:   8%|▊         | 850/10000 [4:27:49<52:12:27, 20.54s/it, loss_gram=0, loss_l2=0.233, loss_lpips=0.407]  \n",
      "Steps:   9%|▊         | 851/10000 [4:28:10<52:09:50, 20.53s/it, loss_gram=0, loss_l2=0.0247, loss_lpips=0.121]\n",
      "Steps:   9%|▊         | 852/10000 [4:28:32<53:48:44, 21.18s/it, loss_gram=0, loss_l2=0.0162, loss_lpips=0.147]\n",
      "Steps:   9%|▊         | 853/10000 [4:28:54<53:52:38, 21.20s/it, loss_gram=0, loss_l2=0.00525, loss_lpips=0.0856]\n",
      "Steps:   9%|▊         | 854/10000 [4:29:12<51:54:42, 20.43s/it, loss_gram=0, loss_l2=0.0139, loss_lpips=0.122]  \n",
      "Steps:   9%|▊         | 855/10000 [4:29:33<52:03:37, 20.49s/it, loss_gram=0, loss_l2=0.00723, loss_lpips=0.141]\n",
      "Steps:   9%|▊         | 856/10000 [4:29:54<52:22:59, 20.62s/it, loss_gram=0, loss_l2=0.00274, loss_lpips=0.108]\n",
      "Steps:   9%|▊         | 857/10000 [4:30:16<53:14:25, 20.96s/it, loss_gram=0, loss_l2=0.00239, loss_lpips=0.102]\n",
      "Steps:   9%|▊         | 858/10000 [4:30:36<52:41:11, 20.75s/it, loss_gram=0, loss_l2=0.00855, loss_lpips=0.128]\n",
      "Steps:   9%|▊         | 859/10000 [4:30:56<52:15:12, 20.58s/it, loss_gram=0, loss_l2=0.00428, loss_lpips=0.107]\n",
      "Steps:   9%|▊         | 860/10000 [4:31:17<52:19:54, 20.61s/it, loss_gram=0, loss_l2=0.0205, loss_lpips=0.126] \n",
      "Steps:   9%|▊         | 861/10000 [4:31:37<51:57:46, 20.47s/it, loss_gram=0, loss_l2=0.0402, loss_lpips=0.169]\n",
      "Steps:   9%|▊         | 862/10000 [4:31:57<51:27:57, 20.28s/it, loss_gram=0, loss_l2=0.0209, loss_lpips=0.129]\n",
      "Steps:   9%|▊         | 863/10000 [4:32:16<50:31:48, 19.91s/it, loss_gram=0, loss_l2=0.00352, loss_lpips=0.126]\n",
      "Steps:   9%|▊         | 864/10000 [4:32:37<51:40:19, 20.36s/it, loss_gram=0, loss_l2=0.012, loss_lpips=0.128]  \n",
      "Steps:   9%|▊         | 865/10000 [4:32:56<50:59:12, 20.09s/it, loss_gram=0, loss_l2=0.0358, loss_lpips=0.176]\n",
      "Steps:   9%|▊         | 866/10000 [4:33:16<50:37:11, 19.95s/it, loss_gram=0, loss_l2=0.00924, loss_lpips=0.162]\n",
      "Steps:   9%|▊         | 867/10000 [4:33:37<50:46:35, 20.01s/it, loss_gram=0, loss_l2=0.00639, loss_lpips=0.132]\n",
      "Steps:   9%|▊         | 868/10000 [4:33:56<50:45:29, 20.01s/it, loss_gram=0, loss_l2=0.0281, loss_lpips=0.136] \n",
      "Steps:   9%|▊         | 869/10000 [4:34:17<51:10:40, 20.18s/it, loss_gram=0, loss_l2=0.00424, loss_lpips=0.0978]\n",
      "Steps:   9%|▊         | 870/10000 [4:34:35<49:38:38, 19.57s/it, loss_gram=0, loss_l2=0.0098, loss_lpips=0.127]  \n",
      "Steps:   9%|▊         | 871/10000 [4:34:56<50:14:59, 19.82s/it, loss_gram=0, loss_l2=0.00878, loss_lpips=0.0876]\n",
      "Steps:   9%|▊         | 872/10000 [4:35:17<51:30:52, 20.32s/it, loss_gram=0, loss_l2=0.011, loss_lpips=0.125]   \n",
      "Steps:   9%|▊         | 873/10000 [4:35:38<51:46:14, 20.42s/it, loss_gram=0, loss_l2=0.00857, loss_lpips=0.144]\n",
      "Steps:   9%|▊         | 874/10000 [4:35:59<52:38:03, 20.76s/it, loss_gram=0, loss_l2=0.00894, loss_lpips=0.251]\n",
      "Steps:   9%|▉         | 875/10000 [4:36:20<52:45:00, 20.81s/it, loss_gram=0, loss_l2=0.00479, loss_lpips=0.0916]\n",
      "Steps:   9%|▉         | 876/10000 [4:36:39<51:37:02, 20.37s/it, loss_gram=0, loss_l2=0.0163, loss_lpips=0.132]  \n",
      "Steps:   9%|▉         | 877/10000 [4:36:59<51:01:09, 20.13s/it, loss_gram=0, loss_l2=0.00589, loss_lpips=0.0842]\n",
      "Steps:   9%|▉         | 878/10000 [4:37:19<51:05:03, 20.16s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.0796] \n",
      "Steps:   9%|▉         | 879/10000 [4:37:38<50:10:26, 19.80s/it, loss_gram=0, loss_l2=0.00314, loss_lpips=0.112]\n",
      "Steps:   9%|▉         | 880/10000 [4:37:59<50:46:56, 20.05s/it, loss_gram=0, loss_l2=0.00613, loss_lpips=0.101]\n",
      "Steps:   9%|▉         | 881/10000 [4:38:19<51:15:31, 20.24s/it, loss_gram=0, loss_l2=0.00508, loss_lpips=0.134]\n",
      "Steps:   9%|▉         | 882/10000 [4:38:41<52:07:42, 20.58s/it, loss_gram=0, loss_l2=0.00434, loss_lpips=0.0817]\n",
      "Steps:   9%|▉         | 883/10000 [4:39:01<52:15:41, 20.64s/it, loss_gram=0, loss_l2=0.00495, loss_lpips=0.16]  \n",
      "Steps:   9%|▉         | 884/10000 [4:39:21<50:41:51, 20.02s/it, loss_gram=0, loss_l2=0.00387, loss_lpips=0.133]\n",
      "Steps:   9%|▉         | 885/10000 [4:39:41<50:55:23, 20.11s/it, loss_gram=0, loss_l2=0.00602, loss_lpips=0.119]\n",
      "Steps:   9%|▉         | 886/10000 [4:40:00<50:27:58, 19.93s/it, loss_gram=0, loss_l2=0.00629, loss_lpips=0.0953]\n",
      "Steps:   9%|▉         | 887/10000 [4:40:21<50:46:43, 20.06s/it, loss_gram=0, loss_l2=0.00124, loss_lpips=0.103] \n",
      "Steps:   9%|▉         | 888/10000 [4:40:41<51:03:18, 20.17s/it, loss_gram=0, loss_l2=0.0159, loss_lpips=0.155] \n",
      "Steps:   9%|▉         | 889/10000 [4:41:00<50:40:35, 20.02s/it, loss_gram=0, loss_l2=0.0202, loss_lpips=0.0952]\n",
      "Steps:   9%|▉         | 890/10000 [4:41:22<51:41:56, 20.43s/it, loss_gram=0, loss_l2=0.00573, loss_lpips=0.108]\n",
      "Steps:   9%|▉         | 891/10000 [4:41:43<51:54:32, 20.52s/it, loss_gram=0, loss_l2=0.0271, loss_lpips=0.14]  \n",
      "Steps:   9%|▉         | 892/10000 [4:42:02<51:18:25, 20.28s/it, loss_gram=0, loss_l2=0.00758, loss_lpips=0.102]\n",
      "Steps:   9%|▉         | 893/10000 [4:42:23<51:17:15, 20.27s/it, loss_gram=0, loss_l2=0.0103, loss_lpips=0.12]  \n",
      "Steps:   9%|▉         | 894/10000 [4:42:43<51:36:55, 20.41s/it, loss_gram=0, loss_l2=0.00122, loss_lpips=0.178]\n",
      "Steps:   9%|▉         | 895/10000 [4:43:04<51:36:13, 20.40s/it, loss_gram=0, loss_l2=0.00534, loss_lpips=0.0708]\n",
      "Steps:   9%|▉         | 896/10000 [4:43:23<50:43:39, 20.06s/it, loss_gram=0, loss_l2=0.00235, loss_lpips=0.0864]\n",
      "Steps:   9%|▉         | 897/10000 [4:43:42<50:02:56, 19.79s/it, loss_gram=0, loss_l2=0.0128, loss_lpips=0.128]  \n",
      "Steps:   9%|▉         | 898/10000 [4:44:03<50:42:04, 20.05s/it, loss_gram=0, loss_l2=0.00529, loss_lpips=0.0873]\n",
      "Steps:   9%|▉         | 899/10000 [4:44:23<50:22:53, 19.93s/it, loss_gram=0, loss_l2=0.0214, loss_lpips=0.132]  \n",
      "Steps:   9%|▉         | 900/10000 [4:44:43<51:01:59, 20.19s/it, loss_gram=0, loss_l2=0.0185, loss_lpips=0.123]\n",
      "Steps:   9%|▉         | 901/10000 [4:45:03<50:48:15, 20.10s/it, loss_gram=0, loss_l2=0.000991, loss_lpips=0.0758]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:   9%|▉         | 902/10000 [4:45:23<50:42:46, 20.07s/it, loss_gram=0, loss_l2=0.00677, loss_lpips=0.118]  \n",
      "Steps:   9%|▉         | 903/10000 [4:45:43<50:42:11, 20.07s/it, loss_gram=0, loss_l2=0.00637, loss_lpips=0.161]\n",
      "Steps:   9%|▉         | 904/10000 [4:46:04<51:08:51, 20.24s/it, loss_gram=0, loss_l2=0.000977, loss_lpips=0.19]\n",
      "Steps:   9%|▉         | 905/10000 [4:46:23<50:31:52, 20.00s/it, loss_gram=0, loss_l2=0.00149, loss_lpips=0.109]\n",
      "Steps:   9%|▉         | 906/10000 [4:46:42<49:52:19, 19.74s/it, loss_gram=0, loss_l2=0.0165, loss_lpips=0.11]  \n",
      "Steps:   9%|▉         | 907/10000 [4:47:06<52:30:44, 20.79s/it, loss_gram=0, loss_l2=0.0123, loss_lpips=0.2] \n",
      "Steps:   9%|▉         | 908/10000 [4:47:26<51:52:02, 20.54s/it, loss_gram=0, loss_l2=0.00809, loss_lpips=0.114]\n",
      "Steps:   9%|▉         | 909/10000 [4:47:46<51:46:10, 20.50s/it, loss_gram=0, loss_l2=0.00203, loss_lpips=0.0841]\n",
      "Steps:   9%|▉         | 910/10000 [4:48:07<51:51:19, 20.54s/it, loss_gram=0, loss_l2=0.0116, loss_lpips=0.0977] \n",
      "Steps:   9%|▉         | 911/10000 [4:48:28<52:15:38, 20.70s/it, loss_gram=0, loss_l2=0.0065, loss_lpips=0.119] \n",
      "Steps:   9%|▉         | 912/10000 [4:48:48<52:02:59, 20.62s/it, loss_gram=0, loss_l2=0.00253, loss_lpips=0.102]\n",
      "Steps:   9%|▉         | 913/10000 [4:49:09<52:20:44, 20.74s/it, loss_gram=0, loss_l2=0.00751, loss_lpips=0.101]\n",
      "Steps:   9%|▉         | 914/10000 [4:49:28<51:10:59, 20.28s/it, loss_gram=0, loss_l2=0.0111, loss_lpips=0.142] \n",
      "Steps:   9%|▉         | 915/10000 [4:49:48<50:59:59, 20.21s/it, loss_gram=0, loss_l2=0.0421, loss_lpips=0.154]\n",
      "Steps:   9%|▉         | 916/10000 [4:50:09<51:11:01, 20.28s/it, loss_gram=0, loss_l2=0.0066, loss_lpips=0.126]\n",
      "Steps:   9%|▉         | 917/10000 [4:50:28<50:12:57, 19.90s/it, loss_gram=0, loss_l2=0.00788, loss_lpips=0.0919]\n",
      "Steps:   9%|▉         | 918/10000 [4:50:47<49:37:45, 19.67s/it, loss_gram=0, loss_l2=0.0029, loss_lpips=0.118]  \n",
      "Steps:   9%|▉         | 919/10000 [4:51:07<49:47:30, 19.74s/it, loss_gram=0, loss_l2=0.00994, loss_lpips=0.124]\n",
      "Steps:   9%|▉         | 920/10000 [4:51:28<50:48:50, 20.15s/it, loss_gram=0, loss_l2=0.000704, loss_lpips=0.171]\n",
      "Steps:   9%|▉         | 921/10000 [4:51:48<50:33:11, 20.05s/it, loss_gram=0, loss_l2=0.0157, loss_lpips=0.152]  \n",
      "Steps:   9%|▉         | 922/10000 [4:52:07<50:11:34, 19.90s/it, loss_gram=0, loss_l2=0.0035, loss_lpips=0.105]\n",
      "Steps:   9%|▉         | 923/10000 [4:52:27<50:03:26, 19.85s/it, loss_gram=0, loss_l2=0.00947, loss_lpips=0.0884]\n",
      "Steps:   9%|▉         | 924/10000 [4:52:48<50:30:13, 20.03s/it, loss_gram=0, loss_l2=0.0045, loss_lpips=0.0715] \n",
      "Steps:   9%|▉         | 925/10000 [4:53:08<51:01:31, 20.24s/it, loss_gram=0, loss_l2=0.0266, loss_lpips=0.158] \n",
      "Steps:   9%|▉         | 926/10000 [4:53:30<51:50:04, 20.56s/it, loss_gram=0, loss_l2=0.00848, loss_lpips=0.0822]\n",
      "Steps:   9%|▉         | 927/10000 [4:53:49<51:20:40, 20.37s/it, loss_gram=0, loss_l2=0.0162, loss_lpips=0.0967] \n",
      "Steps:   9%|▉         | 928/10000 [4:54:09<50:30:10, 20.04s/it, loss_gram=0, loss_l2=0.00574, loss_lpips=0.0962]\n",
      "Steps:   9%|▉         | 929/10000 [4:54:29<50:09:38, 19.91s/it, loss_gram=0, loss_l2=0.00729, loss_lpips=0.104] \n",
      "Steps:   9%|▉         | 930/10000 [4:54:49<50:35:28, 20.08s/it, loss_gram=0, loss_l2=0.00954, loss_lpips=0.134]\n",
      "Steps:   9%|▉         | 931/10000 [4:55:08<49:51:32, 19.79s/it, loss_gram=0, loss_l2=0.0099, loss_lpips=0.11]  \n",
      "Steps:   9%|▉         | 932/10000 [4:55:29<50:34:45, 20.08s/it, loss_gram=0, loss_l2=0.00952, loss_lpips=0.118]\n",
      "Steps:   9%|▉         | 933/10000 [4:55:49<50:38:46, 20.11s/it, loss_gram=0, loss_l2=0.00516, loss_lpips=0.163]\n",
      "Steps:   9%|▉         | 934/10000 [4:56:10<50:55:07, 20.22s/it, loss_gram=0, loss_l2=0.00874, loss_lpips=0.1]  \n",
      "Steps:   9%|▉         | 935/10000 [4:56:30<50:50:00, 20.19s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.084]\n",
      "Steps:   9%|▉         | 936/10000 [4:56:49<50:41:10, 20.13s/it, loss_gram=0, loss_l2=0.00351, loss_lpips=0.0745]\n",
      "Steps:   9%|▉         | 937/10000 [4:57:10<51:15:53, 20.36s/it, loss_gram=0, loss_l2=0.0095, loss_lpips=0.106]  \n",
      "Steps:   9%|▉         | 938/10000 [4:57:29<50:07:44, 19.91s/it, loss_gram=0, loss_l2=0.00143, loss_lpips=0.168]\n",
      "Steps:   9%|▉         | 939/10000 [4:57:50<50:29:38, 20.06s/it, loss_gram=0, loss_l2=0.000592, loss_lpips=0.064]\n",
      "Steps:   9%|▉         | 940/10000 [4:58:10<51:01:22, 20.27s/it, loss_gram=0, loss_l2=0.00493, loss_lpips=0.0706]\n",
      "Steps:   9%|▉         | 941/10000 [4:58:31<51:15:56, 20.37s/it, loss_gram=0, loss_l2=0.0184, loss_lpips=0.146]  \n",
      "Steps:   9%|▉         | 942/10000 [4:58:53<52:35:26, 20.90s/it, loss_gram=0, loss_l2=0.00353, loss_lpips=0.119]\n",
      "Steps:   9%|▉         | 943/10000 [4:59:13<51:48:36, 20.59s/it, loss_gram=0, loss_l2=0.00454, loss_lpips=0.0731]\n",
      "Steps:   9%|▉         | 944/10000 [4:59:34<51:46:25, 20.58s/it, loss_gram=0, loss_l2=0.0217, loss_lpips=0.134]  \n",
      "Steps:   9%|▉         | 945/10000 [4:59:55<52:04:19, 20.70s/it, loss_gram=0, loss_l2=0.00852, loss_lpips=0.0864]\n",
      "Steps:   9%|▉         | 946/10000 [5:00:13<50:29:31, 20.08s/it, loss_gram=0, loss_l2=0.00684, loss_lpips=0.173] \n",
      "Steps:   9%|▉         | 947/10000 [5:00:34<50:40:17, 20.15s/it, loss_gram=0, loss_l2=0.00693, loss_lpips=0.108]\n",
      "Steps:   9%|▉         | 948/10000 [5:00:54<50:43:03, 20.17s/it, loss_gram=0, loss_l2=0.00522, loss_lpips=0.0923]\n",
      "Steps:   9%|▉         | 949/10000 [5:01:14<50:34:49, 20.12s/it, loss_gram=0, loss_l2=0.0098, loss_lpips=0.136]  \n",
      "Steps:  10%|▉         | 950/10000 [5:01:35<51:10:48, 20.36s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.112]\n",
      "Steps:  10%|▉         | 951/10000 [5:01:55<51:07:42, 20.34s/it, loss_gram=0, loss_l2=0.00803, loss_lpips=0.0911]\n",
      "Steps:  10%|▉         | 952/10000 [5:02:15<51:04:42, 20.32s/it, loss_gram=0, loss_l2=0.0234, loss_lpips=0.191]  \n",
      "Steps:  10%|▉         | 953/10000 [5:02:35<50:20:50, 20.03s/it, loss_gram=0, loss_l2=0.00676, loss_lpips=0.105]\n",
      "Steps:  10%|▉         | 954/10000 [5:02:54<49:55:30, 19.87s/it, loss_gram=0, loss_l2=0.00555, loss_lpips=0.0863]\n",
      "Steps:  10%|▉         | 955/10000 [5:03:13<49:14:38, 19.60s/it, loss_gram=0, loss_l2=0.0022, loss_lpips=0.0945] \n",
      "Steps:  10%|▉         | 956/10000 [5:03:33<49:37:33, 19.75s/it, loss_gram=0, loss_l2=0.00156, loss_lpips=0.112]\n",
      "Steps:  10%|▉         | 957/10000 [5:03:53<49:40:39, 19.78s/it, loss_gram=0, loss_l2=0.0052, loss_lpips=0.103] \n",
      "Steps:  10%|▉         | 958/10000 [5:04:13<49:50:56, 19.85s/it, loss_gram=0, loss_l2=0.00156, loss_lpips=0.0645]\n",
      "Steps:  10%|▉         | 959/10000 [5:04:34<50:32:52, 20.13s/it, loss_gram=0, loss_l2=0.0261, loss_lpips=0.136]  \n",
      "Steps:  10%|▉         | 960/10000 [5:04:54<50:46:31, 20.22s/it, loss_gram=0, loss_l2=0.0163, loss_lpips=0.123]\n",
      "Steps:  10%|▉         | 961/10000 [5:05:14<50:26:11, 20.09s/it, loss_gram=0, loss_l2=0.00827, loss_lpips=0.0962]\n",
      "Steps:  10%|▉         | 962/10000 [5:05:35<51:17:24, 20.43s/it, loss_gram=0, loss_l2=0.00339, loss_lpips=0.0997]\n",
      "Steps:  10%|▉         | 963/10000 [5:05:55<51:11:12, 20.39s/it, loss_gram=0, loss_l2=0.0084, loss_lpips=0.118]  \n",
      "Steps:  10%|▉         | 964/10000 [5:06:17<51:30:09, 20.52s/it, loss_gram=0, loss_l2=0.0345, loss_lpips=0.117]\n",
      "Steps:  10%|▉         | 965/10000 [5:06:37<51:17:19, 20.44s/it, loss_gram=0, loss_l2=0.0119, loss_lpips=0.117]\n",
      "Steps:  10%|▉         | 966/10000 [5:06:58<51:37:12, 20.57s/it, loss_gram=0, loss_l2=0.00737, loss_lpips=0.0676]\n",
      "Steps:  10%|▉         | 967/10000 [5:07:18<51:04:44, 20.36s/it, loss_gram=0, loss_l2=0.00126, loss_lpips=0.0639]\n",
      "Steps:  10%|▉         | 968/10000 [5:07:38<51:04:33, 20.36s/it, loss_gram=0, loss_l2=0.00782, loss_lpips=0.14]  \n",
      "Steps:  10%|▉         | 969/10000 [5:07:59<51:34:58, 20.56s/it, loss_gram=0, loss_l2=0.00212, loss_lpips=0.0848]\n",
      "Steps:  10%|▉         | 970/10000 [5:08:20<51:48:02, 20.65s/it, loss_gram=0, loss_l2=0.00471, loss_lpips=0.157] \n",
      "Steps:  10%|▉         | 971/10000 [5:08:40<51:44:38, 20.63s/it, loss_gram=0, loss_l2=0.0127, loss_lpips=0.104] \n",
      "Steps:  10%|▉         | 972/10000 [5:09:00<51:21:10, 20.48s/it, loss_gram=0, loss_l2=0.00462, loss_lpips=0.115]\n",
      "Steps:  10%|▉         | 973/10000 [5:09:20<51:05:58, 20.38s/it, loss_gram=0, loss_l2=0.00146, loss_lpips=0.172]\n",
      "Steps:  10%|▉         | 974/10000 [5:09:40<50:13:27, 20.03s/it, loss_gram=0, loss_l2=0.009, loss_lpips=0.108]  \n",
      "Steps:  10%|▉         | 975/10000 [5:09:58<49:07:23, 19.59s/it, loss_gram=0, loss_l2=0.00467, loss_lpips=0.0695]\n",
      "Steps:  10%|▉         | 976/10000 [5:10:19<49:57:16, 19.93s/it, loss_gram=0, loss_l2=0.00647, loss_lpips=0.104] \n",
      "Steps:  10%|▉         | 977/10000 [5:10:39<49:51:44, 19.89s/it, loss_gram=0, loss_l2=0.00171, loss_lpips=0.0884]\n",
      "Steps:  10%|▉         | 978/10000 [5:10:59<50:01:06, 19.96s/it, loss_gram=0, loss_l2=0.00772, loss_lpips=0.0847]\n",
      "Steps:  10%|▉         | 979/10000 [5:11:19<50:27:26, 20.14s/it, loss_gram=0, loss_l2=0.00866, loss_lpips=0.0857]\n",
      "Steps:  10%|▉         | 980/10000 [5:11:40<50:55:57, 20.33s/it, loss_gram=0, loss_l2=0.0198, loss_lpips=0.2]    \n",
      "Steps:  10%|▉         | 981/10000 [5:12:00<50:16:34, 20.07s/it, loss_gram=0, loss_l2=0.00923, loss_lpips=0.161]\n",
      "Steps:  10%|▉         | 982/10000 [5:12:19<49:53:35, 19.92s/it, loss_gram=0, loss_l2=0.00323, loss_lpips=0.155]\n",
      "Steps:  10%|▉         | 983/10000 [5:12:42<51:47:11, 20.68s/it, loss_gram=0, loss_l2=0.00757, loss_lpips=0.115]\n",
      "Steps:  10%|▉         | 984/10000 [5:13:03<52:03:09, 20.78s/it, loss_gram=0, loss_l2=0.00435, loss_lpips=0.11] \n",
      "Steps:  10%|▉         | 985/10000 [5:13:25<53:10:39, 21.24s/it, loss_gram=0, loss_l2=0.00281, loss_lpips=0.139]\n",
      "Steps:  10%|▉         | 986/10000 [5:13:46<52:50:46, 21.11s/it, loss_gram=0, loss_l2=0.0156, loss_lpips=0.125] \n",
      "Steps:  10%|▉         | 987/10000 [5:14:07<52:35:20, 21.01s/it, loss_gram=0, loss_l2=0.00287, loss_lpips=0.126]\n",
      "Steps:  10%|▉         | 988/10000 [5:14:28<53:02:34, 21.19s/it, loss_gram=0, loss_l2=0.0353, loss_lpips=0.131] \n",
      "Steps:  10%|▉         | 989/10000 [5:14:48<51:46:18, 20.68s/it, loss_gram=0, loss_l2=0.00678, loss_lpips=0.109]\n",
      "Steps:  10%|▉         | 990/10000 [5:15:08<51:19:44, 20.51s/it, loss_gram=0, loss_l2=0.0128, loss_lpips=0.209] \n",
      "Steps:  10%|▉         | 991/10000 [5:15:28<50:49:16, 20.31s/it, loss_gram=0, loss_l2=0.00451, loss_lpips=0.0842]\n",
      "Steps:  10%|▉         | 992/10000 [5:15:48<50:37:22, 20.23s/it, loss_gram=0, loss_l2=0.0179, loss_lpips=0.113]  \n",
      "Steps:  10%|▉         | 993/10000 [5:16:09<51:14:48, 20.48s/it, loss_gram=0, loss_l2=0.0133, loss_lpips=0.163]\n",
      "Steps:  10%|▉         | 994/10000 [5:16:28<50:09:46, 20.05s/it, loss_gram=0, loss_l2=0.0108, loss_lpips=0.0703]\n",
      "Steps:  10%|▉         | 995/10000 [5:16:49<50:54:30, 20.35s/it, loss_gram=0, loss_l2=0.00632, loss_lpips=0.0773]\n",
      "Steps:  10%|▉         | 996/10000 [5:17:10<51:17:48, 20.51s/it, loss_gram=0, loss_l2=0.00584, loss_lpips=0.0997]\n",
      "Steps:  10%|▉         | 997/10000 [5:17:28<49:21:09, 19.73s/it, loss_gram=0, loss_l2=0.0051, loss_lpips=0.156]  \n",
      "Steps:  10%|▉         | 998/10000 [5:17:47<49:15:22, 19.70s/it, loss_gram=0, loss_l2=0.0321, loss_lpips=0.102]\n",
      "Steps:  10%|▉         | 999/10000 [5:18:07<48:57:13, 19.58s/it, loss_gram=0, loss_l2=0.00767, loss_lpips=0.141]\n",
      "Steps:  10%|█         | 1000/10000 [5:18:27<49:32:09, 19.81s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.114] \n",
      "Steps:  10%|█         | 1001/10000 [5:18:47<49:55:21, 19.97s/it, loss_gram=0, loss_l2=0.00594, loss_lpips=0.0904]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  10%|█         | 1002/10000 [5:23:11<232:34:48, 93.05s/it, loss_gram=0, loss_l2=0.00581, loss_lpips=0.113] \n",
      "Steps:  10%|█         | 1003/10000 [5:23:34<179:49:40, 71.96s/it, loss_gram=0, loss_l2=0.00333, loss_lpips=0.126]\n",
      "Steps:  10%|█         | 1004/10000 [5:23:55<141:43:06, 56.71s/it, loss_gram=0, loss_l2=0.0114, loss_lpips=0.149] \n",
      "Steps:  10%|█         | 1005/10000 [5:24:15<114:08:12, 45.68s/it, loss_gram=0, loss_l2=0.0202, loss_lpips=0.125]\n",
      "Steps:  10%|█         | 1006/10000 [5:24:39<97:38:21, 39.08s/it, loss_gram=0, loss_l2=0.00219, loss_lpips=0.072]\n",
      "Steps:  10%|█         | 1007/10000 [5:24:58<83:01:50, 33.24s/it, loss_gram=0, loss_l2=0.0045, loss_lpips=0.0736]\n",
      "Steps:  10%|█         | 1008/10000 [5:25:21<75:16:03, 30.13s/it, loss_gram=0, loss_l2=0.0139, loss_lpips=0.109] \n",
      "Steps:  10%|█         | 1009/10000 [5:25:40<67:07:59, 26.88s/it, loss_gram=0, loss_l2=0.00344, loss_lpips=0.134]\n",
      "Steps:  10%|█         | 1010/10000 [5:26:02<63:18:25, 25.35s/it, loss_gram=0, loss_l2=0.00456, loss_lpips=0.125]\n",
      "Steps:  10%|█         | 1011/10000 [5:26:23<59:53:19, 23.98s/it, loss_gram=0, loss_l2=0.0132, loss_lpips=0.142] \n",
      "Steps:  10%|█         | 1012/10000 [5:26:45<58:25:42, 23.40s/it, loss_gram=0, loss_l2=0.00381, loss_lpips=0.0878]\n",
      "Steps:  10%|█         | 1013/10000 [5:27:05<55:47:49, 22.35s/it, loss_gram=0, loss_l2=0.0187, loss_lpips=0.184]  \n",
      "Steps:  10%|█         | 1014/10000 [5:27:27<55:49:05, 22.36s/it, loss_gram=0, loss_l2=0.0096, loss_lpips=0.123]\n",
      "Steps:  10%|█         | 1015/10000 [5:27:47<53:53:30, 21.59s/it, loss_gram=0, loss_l2=0.00516, loss_lpips=0.102]\n",
      "Steps:  10%|█         | 1016/10000 [5:28:10<54:35:13, 21.87s/it, loss_gram=0, loss_l2=0.00766, loss_lpips=0.119]\n",
      "Steps:  10%|█         | 1017/10000 [5:28:28<51:59:36, 20.84s/it, loss_gram=0, loss_l2=0.0198, loss_lpips=0.27]  \n",
      "Steps:  10%|█         | 1018/10000 [5:28:50<52:53:20, 21.20s/it, loss_gram=0, loss_l2=0.00615, loss_lpips=0.126]\n",
      "Steps:  10%|█         | 1019/10000 [5:29:10<51:55:34, 20.81s/it, loss_gram=0, loss_l2=0.00507, loss_lpips=0.136]\n",
      "Steps:  10%|█         | 1020/10000 [5:29:33<53:46:55, 21.56s/it, loss_gram=0, loss_l2=0.00236, loss_lpips=0.092]\n",
      "Steps:  10%|█         | 1021/10000 [5:29:52<51:48:32, 20.77s/it, loss_gram=0, loss_l2=0.00601, loss_lpips=0.111]\n",
      "Steps:  10%|█         | 1022/10000 [5:30:16<53:51:12, 21.59s/it, loss_gram=0, loss_l2=0.0128, loss_lpips=0.134] \n",
      "Steps:  10%|█         | 1023/10000 [5:30:35<52:00:49, 20.86s/it, loss_gram=0, loss_l2=0.015, loss_lpips=0.155] \n",
      "Steps:  10%|█         | 1024/10000 [5:30:58<53:14:15, 21.35s/it, loss_gram=0, loss_l2=0.00234, loss_lpips=0.0786]\n",
      "Steps:  10%|█         | 1025/10000 [5:31:18<52:22:47, 21.01s/it, loss_gram=0, loss_l2=0.00534, loss_lpips=0.0674]\n",
      "Steps:  10%|█         | 1026/10000 [5:31:39<52:39:37, 21.13s/it, loss_gram=0, loss_l2=0.00497, loss_lpips=0.0624]\n",
      "Steps:  10%|█         | 1027/10000 [5:31:57<50:30:08, 20.26s/it, loss_gram=0, loss_l2=0.0052, loss_lpips=0.0786] \n",
      "Steps:  10%|█         | 1028/10000 [5:32:17<49:46:52, 19.97s/it, loss_gram=0, loss_l2=0.0189, loss_lpips=0.116] \n",
      "Steps:  10%|█         | 1029/10000 [5:32:35<48:20:45, 19.40s/it, loss_gram=0, loss_l2=0.0112, loss_lpips=0.121]\n",
      "Steps:  10%|█         | 1030/10000 [5:32:54<48:32:44, 19.48s/it, loss_gram=0, loss_l2=0.00614, loss_lpips=0.0674]\n",
      "Steps:  10%|█         | 1031/10000 [5:33:12<47:16:24, 18.97s/it, loss_gram=0, loss_l2=0.00408, loss_lpips=0.0901]\n",
      "Steps:  10%|█         | 1032/10000 [5:33:33<48:19:28, 19.40s/it, loss_gram=0, loss_l2=0.0156, loss_lpips=0.123]  \n",
      "Steps:  10%|█         | 1033/10000 [5:33:51<47:58:10, 19.26s/it, loss_gram=0, loss_l2=0.00551, loss_lpips=0.0955]\n",
      "Steps:  10%|█         | 1034/10000 [5:34:12<49:16:02, 19.78s/it, loss_gram=0, loss_l2=0.00457, loss_lpips=0.0842]\n",
      "Steps:  10%|█         | 1035/10000 [5:34:31<48:34:19, 19.50s/it, loss_gram=0, loss_l2=0.0308, loss_lpips=0.305]  \n",
      "Steps:  10%|█         | 1036/10000 [5:34:49<47:37:19, 19.13s/it, loss_gram=0, loss_l2=0.0109, loss_lpips=0.105]\n",
      "Steps:  10%|█         | 1037/10000 [5:35:08<47:09:32, 18.94s/it, loss_gram=0, loss_l2=0.00737, loss_lpips=0.0753]\n",
      "Steps:  10%|█         | 1038/10000 [5:35:31<50:05:58, 20.12s/it, loss_gram=0, loss_l2=0.00151, loss_lpips=0.141] \n",
      "Steps:  10%|█         | 1039/10000 [5:35:52<50:29:21, 20.28s/it, loss_gram=0, loss_l2=0.00124, loss_lpips=0.128]\n",
      "Steps:  10%|█         | 1040/10000 [5:36:14<52:03:36, 20.92s/it, loss_gram=0, loss_l2=0.0168, loss_lpips=0.175] \n",
      "Steps:  10%|█         | 1041/10000 [5:36:32<50:02:00, 20.11s/it, loss_gram=0, loss_l2=0.0213, loss_lpips=0.131]\n",
      "Steps:  10%|█         | 1042/10000 [5:36:53<50:32:12, 20.31s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.101]\n",
      "Steps:  10%|█         | 1043/10000 [5:37:10<48:37:54, 19.55s/it, loss_gram=0, loss_l2=0.00635, loss_lpips=0.125]\n",
      "Steps:  10%|█         | 1044/10000 [5:37:31<49:15:14, 19.80s/it, loss_gram=0, loss_l2=0.00991, loss_lpips=0.0918]\n",
      "Steps:  10%|█         | 1045/10000 [5:37:51<49:24:18, 19.86s/it, loss_gram=0, loss_l2=0.00804, loss_lpips=0.0765]\n",
      "Steps:  10%|█         | 1046/10000 [5:38:14<51:25:41, 20.68s/it, loss_gram=0, loss_l2=0.00644, loss_lpips=0.123] \n",
      "Steps:  10%|█         | 1047/10000 [5:38:31<49:22:59, 19.86s/it, loss_gram=0, loss_l2=0.00696, loss_lpips=0.116]\n",
      "Steps:  10%|█         | 1048/10000 [5:38:53<50:11:37, 20.19s/it, loss_gram=0, loss_l2=0.00485, loss_lpips=0.132]\n",
      "Steps:  10%|█         | 1049/10000 [5:39:11<48:31:03, 19.51s/it, loss_gram=0, loss_l2=0.00203, loss_lpips=0.0974]\n",
      "Steps:  10%|█         | 1050/10000 [5:39:32<49:53:09, 20.07s/it, loss_gram=0, loss_l2=0.0139, loss_lpips=0.118]  \n",
      "Steps:  11%|█         | 1051/10000 [5:39:49<47:47:47, 19.23s/it, loss_gram=0, loss_l2=0.00337, loss_lpips=0.075]\n",
      "Steps:  11%|█         | 1052/10000 [5:40:08<47:46:09, 19.22s/it, loss_gram=0, loss_l2=0.00713, loss_lpips=0.173]\n",
      "Steps:  11%|█         | 1053/10000 [5:40:29<48:38:07, 19.57s/it, loss_gram=0, loss_l2=0.0105, loss_lpips=0.103] \n",
      "Steps:  11%|█         | 1054/10000 [5:40:51<50:49:26, 20.45s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.106]\n",
      "Steps:  11%|█         | 1055/10000 [5:41:10<49:40:21, 19.99s/it, loss_gram=0, loss_l2=0.0643, loss_lpips=0.334]\n",
      "Steps:  11%|█         | 1056/10000 [5:41:32<50:37:54, 20.38s/it, loss_gram=0, loss_l2=0.00343, loss_lpips=0.268]\n",
      "Steps:  11%|█         | 1057/10000 [5:41:50<49:23:22, 19.88s/it, loss_gram=0, loss_l2=0.00658, loss_lpips=0.092]\n",
      "Steps:  11%|█         | 1058/10000 [5:42:10<49:42:05, 20.01s/it, loss_gram=0, loss_l2=0.00169, loss_lpips=0.123]\n",
      "Steps:  11%|█         | 1059/10000 [5:42:30<49:17:40, 19.85s/it, loss_gram=0, loss_l2=0.00258, loss_lpips=0.0798]\n",
      "Steps:  11%|█         | 1060/10000 [5:42:50<49:49:11, 20.06s/it, loss_gram=0, loss_l2=0.00226, loss_lpips=0.105] \n",
      "Steps:  11%|█         | 1061/10000 [5:43:09<48:40:55, 19.61s/it, loss_gram=0, loss_l2=0.00727, loss_lpips=0.089]\n",
      "Steps:  11%|█         | 1062/10000 [5:43:30<49:45:45, 20.04s/it, loss_gram=0, loss_l2=0.00242, loss_lpips=0.0833]\n",
      "Steps:  11%|█         | 1063/10000 [5:43:48<47:44:04, 19.23s/it, loss_gram=0, loss_l2=0.0198, loss_lpips=0.137]  \n",
      "Steps:  11%|█         | 1064/10000 [5:44:07<48:05:08, 19.37s/it, loss_gram=0, loss_l2=0.00494, loss_lpips=0.1] \n",
      "Steps:  11%|█         | 1065/10000 [5:44:28<48:53:32, 19.70s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.124]\n",
      "Steps:  11%|█         | 1066/10000 [5:44:47<48:46:54, 19.66s/it, loss_gram=0, loss_l2=0.00251, loss_lpips=0.123]\n",
      "Steps:  11%|█         | 1067/10000 [5:45:06<48:08:06, 19.40s/it, loss_gram=0, loss_l2=0.00565, loss_lpips=0.182]\n",
      "Steps:  11%|█         | 1068/10000 [5:45:30<51:10:02, 20.62s/it, loss_gram=0, loss_l2=0.0136, loss_lpips=0.132] \n",
      "Steps:  11%|█         | 1069/10000 [5:45:48<49:42:56, 20.04s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.169]\n",
      "Steps:  11%|█         | 1070/10000 [5:46:12<52:31:37, 21.18s/it, loss_gram=0, loss_l2=0.0009, loss_lpips=0.0742]\n",
      "Steps:  11%|█         | 1071/10000 [5:46:32<51:28:58, 20.76s/it, loss_gram=0, loss_l2=0.0048, loss_lpips=0.118] \n",
      "Steps:  11%|█         | 1072/10000 [5:46:53<51:32:49, 20.79s/it, loss_gram=0, loss_l2=0.00416, loss_lpips=0.0809]\n",
      "Steps:  11%|█         | 1073/10000 [5:47:10<49:15:43, 19.87s/it, loss_gram=0, loss_l2=0.015, loss_lpips=0.132]   \n",
      "Steps:  11%|█         | 1074/10000 [5:47:33<51:14:26, 20.67s/it, loss_gram=0, loss_l2=0.00592, loss_lpips=0.145]\n",
      "Steps:  11%|█         | 1075/10000 [5:47:51<49:13:42, 19.86s/it, loss_gram=0, loss_l2=0.0114, loss_lpips=0.114] \n",
      "Steps:  11%|█         | 1076/10000 [5:48:14<51:50:47, 20.92s/it, loss_gram=0, loss_l2=0.00323, loss_lpips=0.0798]\n",
      "Steps:  11%|█         | 1077/10000 [5:48:33<50:41:20, 20.45s/it, loss_gram=0, loss_l2=0.0069, loss_lpips=0.0833] \n",
      "Steps:  11%|█         | 1078/10000 [5:48:56<52:16:55, 21.10s/it, loss_gram=0, loss_l2=0.00437, loss_lpips=0.108]\n",
      "Steps:  11%|█         | 1079/10000 [5:49:15<50:41:10, 20.45s/it, loss_gram=0, loss_l2=0.00635, loss_lpips=0.0737]\n",
      "Steps:  11%|█         | 1080/10000 [5:49:36<50:56:19, 20.56s/it, loss_gram=0, loss_l2=0.00618, loss_lpips=0.119] \n",
      "Steps:  11%|█         | 1081/10000 [5:49:57<51:12:54, 20.67s/it, loss_gram=0, loss_l2=0.0033, loss_lpips=0.178] \n",
      "Steps:  11%|█         | 1082/10000 [5:50:19<52:39:05, 21.25s/it, loss_gram=0, loss_l2=0.00938, loss_lpips=0.111]\n",
      "Steps:  11%|█         | 1083/10000 [5:50:39<51:18:20, 20.71s/it, loss_gram=0, loss_l2=0.016, loss_lpips=0.116]  \n",
      "Steps:  11%|█         | 1084/10000 [5:51:05<55:26:22, 22.38s/it, loss_gram=0, loss_l2=0.0165, loss_lpips=0.117]\n",
      "Steps:  11%|█         | 1085/10000 [5:51:26<53:50:41, 21.74s/it, loss_gram=0, loss_l2=0.00774, loss_lpips=0.126]\n",
      "Steps:  11%|█         | 1086/10000 [5:51:46<53:04:10, 21.43s/it, loss_gram=0, loss_l2=0.00305, loss_lpips=0.16] \n",
      "Steps:  11%|█         | 1087/10000 [5:52:04<50:15:25, 20.30s/it, loss_gram=0, loss_l2=0.0103, loss_lpips=0.101]\n",
      "Steps:  11%|█         | 1088/10000 [5:52:23<49:59:08, 20.19s/it, loss_gram=0, loss_l2=0.0138, loss_lpips=0.114]\n",
      "Steps:  11%|█         | 1089/10000 [5:52:41<47:41:39, 19.27s/it, loss_gram=0, loss_l2=0.0317, loss_lpips=0.137]\n",
      "Steps:  11%|█         | 1090/10000 [5:53:00<47:44:18, 19.29s/it, loss_gram=0, loss_l2=0.0046, loss_lpips=0.106]\n",
      "Steps:  11%|█         | 1091/10000 [5:53:18<47:05:24, 19.03s/it, loss_gram=0, loss_l2=0.00609, loss_lpips=0.0869]\n",
      "Steps:  11%|█         | 1092/10000 [5:53:38<47:38:07, 19.25s/it, loss_gram=0, loss_l2=0.006, loss_lpips=0.151]   \n",
      "Steps:  11%|█         | 1093/10000 [5:53:59<48:41:55, 19.68s/it, loss_gram=0, loss_l2=0.00998, loss_lpips=0.201]\n",
      "Steps:  11%|█         | 1094/10000 [5:54:18<48:36:51, 19.65s/it, loss_gram=0, loss_l2=0.00531, loss_lpips=0.111]\n",
      "Steps:  11%|█         | 1095/10000 [5:54:38<48:12:34, 19.49s/it, loss_gram=0, loss_l2=0.0171, loss_lpips=0.139] \n",
      "Steps:  11%|█         | 1096/10000 [5:54:57<48:05:23, 19.44s/it, loss_gram=0, loss_l2=0.0112, loss_lpips=0.102]\n",
      "Steps:  11%|█         | 1097/10000 [5:55:17<48:14:51, 19.51s/it, loss_gram=0, loss_l2=0.00112, loss_lpips=0.101]\n",
      "Steps:  11%|█         | 1098/10000 [5:55:40<50:43:46, 20.52s/it, loss_gram=0, loss_l2=0.0237, loss_lpips=0.154] \n",
      "Steps:  11%|█         | 1099/10000 [5:56:00<50:51:51, 20.57s/it, loss_gram=0, loss_l2=0.0135, loss_lpips=0.135]\n",
      "Steps:  11%|█         | 1100/10000 [5:56:22<51:18:31, 20.75s/it, loss_gram=0, loss_l2=0.0143, loss_lpips=0.134]\n",
      "Steps:  11%|█         | 1101/10000 [5:56:42<50:58:23, 20.62s/it, loss_gram=0, loss_l2=0.00565, loss_lpips=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  11%|█         | 1102/10000 [5:57:02<50:38:11, 20.49s/it, loss_gram=0, loss_l2=0.0101, loss_lpips=0.215] \n",
      "Steps:  11%|█         | 1103/10000 [5:57:20<49:00:40, 19.83s/it, loss_gram=0, loss_l2=0.00368, loss_lpips=0.0731]\n",
      "Steps:  11%|█         | 1104/10000 [5:57:41<49:56:55, 20.21s/it, loss_gram=0, loss_l2=0.00714, loss_lpips=0.0823]\n",
      "Steps:  11%|█         | 1105/10000 [5:58:00<48:51:57, 19.78s/it, loss_gram=0, loss_l2=0.00874, loss_lpips=0.104] \n",
      "Steps:  11%|█         | 1106/10000 [5:58:20<49:05:32, 19.87s/it, loss_gram=0, loss_l2=0.00442, loss_lpips=0.128]\n",
      "Steps:  11%|█         | 1107/10000 [5:58:37<46:29:52, 18.82s/it, loss_gram=0, loss_l2=0.0105, loss_lpips=0.123] \n",
      "Steps:  11%|█         | 1108/10000 [5:58:58<48:19:51, 19.57s/it, loss_gram=0, loss_l2=0.00335, loss_lpips=0.111]\n",
      "Steps:  11%|█         | 1109/10000 [5:59:14<46:14:47, 18.73s/it, loss_gram=0, loss_l2=0.0038, loss_lpips=0.059] \n",
      "Steps:  11%|█         | 1110/10000 [5:59:36<47:47:27, 19.35s/it, loss_gram=0, loss_l2=0.00715, loss_lpips=0.0981]\n",
      "Steps:  11%|█         | 1111/10000 [5:59:53<46:30:03, 18.83s/it, loss_gram=0, loss_l2=0.00309, loss_lpips=0.111] \n",
      "Steps:  11%|█         | 1112/10000 [6:00:15<49:03:56, 19.87s/it, loss_gram=0, loss_l2=0.0149, loss_lpips=0.159] \n",
      "Steps:  11%|█         | 1113/10000 [6:00:34<47:58:29, 19.43s/it, loss_gram=0, loss_l2=0.00745, loss_lpips=0.0764]\n",
      "Steps:  11%|█         | 1114/10000 [6:00:55<49:30:10, 20.06s/it, loss_gram=0, loss_l2=0.00831, loss_lpips=0.154] \n",
      "Steps:  11%|█         | 1115/10000 [6:01:14<48:47:46, 19.77s/it, loss_gram=0, loss_l2=0.0125, loss_lpips=0.089] \n",
      "Steps:  11%|█         | 1116/10000 [6:01:34<48:24:56, 19.62s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.163]\n",
      "Steps:  11%|█         | 1117/10000 [6:01:53<47:59:53, 19.45s/it, loss_gram=0, loss_l2=0.00626, loss_lpips=0.149]\n",
      "Steps:  11%|█         | 1118/10000 [6:02:12<48:19:25, 19.59s/it, loss_gram=0, loss_l2=0.00477, loss_lpips=0.108]\n",
      "Steps:  11%|█         | 1119/10000 [6:02:31<47:18:16, 19.18s/it, loss_gram=0, loss_l2=0.00568, loss_lpips=0.0797]\n",
      "Steps:  11%|█         | 1120/10000 [6:02:52<48:46:01, 19.77s/it, loss_gram=0, loss_l2=0.00293, loss_lpips=0.112] \n",
      "Steps:  11%|█         | 1121/10000 [6:03:09<46:50:40, 18.99s/it, loss_gram=0, loss_l2=0.0511, loss_lpips=0.214] \n",
      "Steps:  11%|█         | 1122/10000 [6:03:30<48:09:13, 19.53s/it, loss_gram=0, loss_l2=0.00129, loss_lpips=0.128]\n",
      "Steps:  11%|█         | 1123/10000 [6:03:48<46:40:42, 18.93s/it, loss_gram=0, loss_l2=0.0101, loss_lpips=0.106] \n",
      "Steps:  11%|█         | 1124/10000 [6:04:08<47:41:06, 19.34s/it, loss_gram=0, loss_l2=0.024, loss_lpips=0.124] \n",
      "Steps:  11%|█▏        | 1125/10000 [6:04:26<47:05:20, 19.10s/it, loss_gram=0, loss_l2=0.0124, loss_lpips=0.0895]\n",
      "Steps:  11%|█▏        | 1126/10000 [6:04:47<48:25:07, 19.64s/it, loss_gram=0, loss_l2=0.00485, loss_lpips=0.0781]\n",
      "Steps:  11%|█▏        | 1127/10000 [6:05:05<46:39:35, 18.93s/it, loss_gram=0, loss_l2=0.00306, loss_lpips=0.0775]\n",
      "Steps:  11%|█▏        | 1128/10000 [6:05:25<47:59:46, 19.48s/it, loss_gram=0, loss_l2=0.00831, loss_lpips=0.0918]\n",
      "Steps:  11%|█▏        | 1129/10000 [6:05:42<46:16:00, 18.78s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.193]  \n",
      "Steps:  11%|█▏        | 1130/10000 [6:06:02<46:29:56, 18.87s/it, loss_gram=0, loss_l2=0.0396, loss_lpips=0.167]\n",
      "Steps:  11%|█▏        | 1131/10000 [6:06:19<45:49:20, 18.60s/it, loss_gram=0, loss_l2=0.0134, loss_lpips=0.123]\n",
      "Steps:  11%|█▏        | 1132/10000 [6:06:39<46:34:59, 18.91s/it, loss_gram=0, loss_l2=0.00738, loss_lpips=0.103]\n",
      "Steps:  11%|█▏        | 1133/10000 [6:06:56<45:19:49, 18.40s/it, loss_gram=0, loss_l2=0.00422, loss_lpips=0.141]\n",
      "Steps:  11%|█▏        | 1134/10000 [6:07:16<46:42:45, 18.97s/it, loss_gram=0, loss_l2=0.00466, loss_lpips=0.11] \n",
      "Steps:  11%|█▏        | 1135/10000 [6:07:34<45:26:34, 18.45s/it, loss_gram=0, loss_l2=0.00945, loss_lpips=0.149]\n",
      "Steps:  11%|█▏        | 1136/10000 [6:07:55<47:19:17, 19.22s/it, loss_gram=0, loss_l2=0.0133, loss_lpips=0.0935]\n",
      "Steps:  11%|█▏        | 1137/10000 [6:08:13<46:27:29, 18.87s/it, loss_gram=0, loss_l2=0.00453, loss_lpips=0.139]\n",
      "Steps:  11%|█▏        | 1138/10000 [6:08:31<46:19:35, 18.82s/it, loss_gram=0, loss_l2=0.011, loss_lpips=0.0989] \n",
      "Steps:  11%|█▏        | 1139/10000 [6:08:49<45:29:46, 18.48s/it, loss_gram=0, loss_l2=0.00359, loss_lpips=0.0684]\n",
      "Steps:  11%|█▏        | 1140/10000 [6:09:09<46:31:11, 18.90s/it, loss_gram=0, loss_l2=0.00467, loss_lpips=0.111] \n",
      "Steps:  11%|█▏        | 1141/10000 [6:09:26<45:05:20, 18.32s/it, loss_gram=0, loss_l2=0.00649, loss_lpips=0.0895]\n",
      "Steps:  11%|█▏        | 1142/10000 [6:09:46<45:56:28, 18.67s/it, loss_gram=0, loss_l2=0.00474, loss_lpips=0.0713]\n",
      "Steps:  11%|█▏        | 1143/10000 [6:10:03<44:31:47, 18.10s/it, loss_gram=0, loss_l2=0.00375, loss_lpips=0.089] \n",
      "Steps:  11%|█▏        | 1144/10000 [6:10:23<46:12:58, 18.79s/it, loss_gram=0, loss_l2=0.000905, loss_lpips=0.0791]\n",
      "Steps:  11%|█▏        | 1145/10000 [6:10:43<46:58:41, 19.10s/it, loss_gram=0, loss_l2=0.00925, loss_lpips=0.169]  \n",
      "Steps:  11%|█▏        | 1146/10000 [6:11:02<47:17:33, 19.23s/it, loss_gram=0, loss_l2=0.00355, loss_lpips=0.0778]\n",
      "Steps:  11%|█▏        | 1147/10000 [6:11:20<45:55:56, 18.68s/it, loss_gram=0, loss_l2=0.0116, loss_lpips=0.11]   \n",
      "Steps:  11%|█▏        | 1148/10000 [6:11:39<46:17:01, 18.82s/it, loss_gram=0, loss_l2=0.00506, loss_lpips=0.0868]\n",
      "Steps:  11%|█▏        | 1149/10000 [6:11:56<45:13:36, 18.40s/it, loss_gram=0, loss_l2=0.0129, loss_lpips=0.138]  \n",
      "Steps:  12%|█▏        | 1150/10000 [6:12:16<46:32:45, 18.93s/it, loss_gram=0, loss_l2=0.0119, loss_lpips=0.142]\n",
      "Steps:  12%|█▏        | 1151/10000 [6:12:35<46:23:42, 18.87s/it, loss_gram=0, loss_l2=0.00552, loss_lpips=0.0765]\n",
      "Steps:  12%|█▏        | 1152/10000 [6:12:54<46:16:25, 18.83s/it, loss_gram=0, loss_l2=0.00293, loss_lpips=0.105] \n",
      "Steps:  12%|█▏        | 1153/10000 [6:13:11<45:04:13, 18.34s/it, loss_gram=0, loss_l2=0.0028, loss_lpips=0.0713]\n",
      "Steps:  12%|█▏        | 1154/10000 [6:13:30<45:22:14, 18.46s/it, loss_gram=0, loss_l2=0.00424, loss_lpips=0.098]\n",
      "Steps:  12%|█▏        | 1155/10000 [6:13:48<45:01:14, 18.32s/it, loss_gram=0, loss_l2=0.0234, loss_lpips=0.258] \n",
      "Steps:  12%|█▏        | 1156/10000 [6:14:08<46:17:12, 18.84s/it, loss_gram=0, loss_l2=0.00739, loss_lpips=0.0833]\n",
      "Steps:  12%|█▏        | 1157/10000 [6:14:25<45:17:01, 18.44s/it, loss_gram=0, loss_l2=0.00202, loss_lpips=0.0776]\n",
      "Steps:  12%|█▏        | 1158/10000 [6:14:44<45:18:21, 18.45s/it, loss_gram=0, loss_l2=0.0127, loss_lpips=0.106]  \n",
      "Steps:  12%|█▏        | 1159/10000 [6:15:02<45:22:25, 18.48s/it, loss_gram=0, loss_l2=0.0522, loss_lpips=0.311]\n",
      "Steps:  12%|█▏        | 1160/10000 [6:15:23<46:44:15, 19.03s/it, loss_gram=0, loss_l2=0.00239, loss_lpips=0.133]\n",
      "Steps:  12%|█▏        | 1161/10000 [6:15:40<45:46:30, 18.64s/it, loss_gram=0, loss_l2=0.032, loss_lpips=0.18]   \n",
      "Steps:  12%|█▏        | 1162/10000 [6:16:01<47:06:42, 19.19s/it, loss_gram=0, loss_l2=0.0179, loss_lpips=0.128]\n",
      "Steps:  12%|█▏        | 1163/10000 [6:16:20<46:56:59, 19.13s/it, loss_gram=0, loss_l2=0.00714, loss_lpips=0.0894]\n",
      "Steps:  12%|█▏        | 1164/10000 [6:16:40<47:52:30, 19.51s/it, loss_gram=0, loss_l2=0.00292, loss_lpips=0.093] \n",
      "Steps:  12%|█▏        | 1165/10000 [6:16:58<46:53:50, 19.11s/it, loss_gram=0, loss_l2=0.00575, loss_lpips=0.111]\n",
      "Steps:  12%|█▏        | 1166/10000 [6:17:19<47:54:21, 19.52s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.243] \n",
      "Steps:  12%|█▏        | 1167/10000 [6:17:37<46:59:51, 19.15s/it, loss_gram=0, loss_l2=0.00922, loss_lpips=0.131]\n",
      "Steps:  12%|█▏        | 1168/10000 [6:17:58<48:11:49, 19.65s/it, loss_gram=0, loss_l2=0.0422, loss_lpips=0.159] \n",
      "Steps:  12%|█▏        | 1169/10000 [6:18:16<47:01:45, 19.17s/it, loss_gram=0, loss_l2=0.0144, loss_lpips=0.105]\n",
      "Steps:  12%|█▏        | 1170/10000 [6:18:37<48:13:10, 19.66s/it, loss_gram=0, loss_l2=0.0277, loss_lpips=0.171]\n",
      "Steps:  12%|█▏        | 1171/10000 [6:18:55<47:22:43, 19.32s/it, loss_gram=0, loss_l2=0.0182, loss_lpips=0.136]\n",
      "Steps:  12%|█▏        | 1172/10000 [6:19:15<47:47:21, 19.49s/it, loss_gram=0, loss_l2=0.0109, loss_lpips=0.112]\n",
      "Steps:  12%|█▏        | 1173/10000 [6:19:35<47:49:06, 19.50s/it, loss_gram=0, loss_l2=0.00783, loss_lpips=0.0806]\n",
      "Steps:  12%|█▏        | 1174/10000 [6:19:54<47:14:00, 19.27s/it, loss_gram=0, loss_l2=0.0133, loss_lpips=0.0996] \n",
      "Steps:  12%|█▏        | 1175/10000 [6:20:10<45:39:05, 18.62s/it, loss_gram=0, loss_l2=0.00772, loss_lpips=0.143]\n",
      "Steps:  12%|█▏        | 1176/10000 [6:20:32<47:35:03, 19.41s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.183] \n",
      "Steps:  12%|█▏        | 1177/10000 [6:20:49<46:02:29, 18.79s/it, loss_gram=0, loss_l2=0.0135, loss_lpips=0.146]\n",
      "Steps:  12%|█▏        | 1178/10000 [6:21:09<46:49:55, 19.11s/it, loss_gram=0, loss_l2=0.00657, loss_lpips=0.0999]\n",
      "Steps:  12%|█▏        | 1179/10000 [6:21:27<46:01:53, 18.79s/it, loss_gram=0, loss_l2=0.0192, loss_lpips=0.0903] \n",
      "Steps:  12%|█▏        | 1180/10000 [6:21:47<47:14:32, 19.28s/it, loss_gram=0, loss_l2=0.0084, loss_lpips=0.0713]\n",
      "Steps:  12%|█▏        | 1181/10000 [6:22:04<45:25:59, 18.55s/it, loss_gram=0, loss_l2=0.00529, loss_lpips=0.0717]\n",
      "Steps:  12%|█▏        | 1182/10000 [6:22:26<47:31:41, 19.40s/it, loss_gram=0, loss_l2=0.0191, loss_lpips=0.116]  \n",
      "Steps:  12%|█▏        | 1183/10000 [6:22:44<46:18:36, 18.91s/it, loss_gram=0, loss_l2=0.00717, loss_lpips=0.0812]\n",
      "Steps:  12%|█▏        | 1184/10000 [6:23:03<47:09:47, 19.26s/it, loss_gram=0, loss_l2=0.0101, loss_lpips=0.12]   \n",
      "Steps:  12%|█▏        | 1185/10000 [6:23:21<46:08:27, 18.84s/it, loss_gram=0, loss_l2=0.00172, loss_lpips=0.113]\n",
      "Steps:  12%|█▏        | 1186/10000 [6:23:43<48:21:24, 19.75s/it, loss_gram=0, loss_l2=0.00552, loss_lpips=0.172]\n",
      "Steps:  12%|█▏        | 1187/10000 [6:24:01<47:15:39, 19.31s/it, loss_gram=0, loss_l2=0.00606, loss_lpips=0.101]\n",
      "Steps:  12%|█▏        | 1188/10000 [6:24:21<47:43:06, 19.49s/it, loss_gram=0, loss_l2=0.00611, loss_lpips=0.114]\n",
      "Steps:  12%|█▏        | 1189/10000 [6:24:40<46:46:30, 19.11s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.121] \n",
      "Steps:  12%|█▏        | 1190/10000 [6:24:59<47:14:25, 19.30s/it, loss_gram=0, loss_l2=0.00202, loss_lpips=0.127]\n",
      "Steps:  12%|█▏        | 1191/10000 [6:25:16<45:22:00, 18.54s/it, loss_gram=0, loss_l2=0.0137, loss_lpips=0.158] \n",
      "Steps:  12%|█▏        | 1192/10000 [6:25:36<46:14:53, 18.90s/it, loss_gram=0, loss_l2=0.0149, loss_lpips=0.0922]\n",
      "Steps:  12%|█▏        | 1193/10000 [6:25:54<45:39:56, 18.67s/it, loss_gram=0, loss_l2=0.0281, loss_lpips=0.209] \n",
      "Steps:  12%|█▏        | 1194/10000 [6:26:16<47:42:46, 19.51s/it, loss_gram=0, loss_l2=0.0302, loss_lpips=0.158]\n",
      "Steps:  12%|█▏        | 1195/10000 [6:26:33<46:05:25, 18.84s/it, loss_gram=0, loss_l2=0.0547, loss_lpips=0.238]\n",
      "Steps:  12%|█▏        | 1196/10000 [6:26:52<46:36:29, 19.06s/it, loss_gram=0, loss_l2=0.00104, loss_lpips=0.0822]\n",
      "Steps:  12%|█▏        | 1197/10000 [6:27:10<45:48:49, 18.74s/it, loss_gram=0, loss_l2=0.000696, loss_lpips=0.112]\n",
      "Steps:  12%|█▏        | 1198/10000 [6:27:31<47:01:51, 19.24s/it, loss_gram=0, loss_l2=0.0242, loss_lpips=0.173]  \n",
      "Steps:  12%|█▏        | 1199/10000 [6:27:49<46:07:25, 18.87s/it, loss_gram=0, loss_l2=0.0227, loss_lpips=0.221]\n",
      "Steps:  12%|█▏        | 1200/10000 [6:28:08<46:22:06, 18.97s/it, loss_gram=0, loss_l2=0.0108, loss_lpips=0.0957]\n",
      "Steps:  12%|█▏        | 1201/10000 [6:28:27<46:11:14, 18.90s/it, loss_gram=0, loss_l2=0.00427, loss_lpips=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  12%|█▏        | 1202/10000 [6:28:47<47:05:54, 19.27s/it, loss_gram=0, loss_l2=0.00856, loss_lpips=0.127]\n",
      "Steps:  12%|█▏        | 1203/10000 [6:29:05<46:13:40, 18.92s/it, loss_gram=0, loss_l2=0.00131, loss_lpips=0.0953]\n",
      "Steps:  12%|█▏        | 1204/10000 [6:29:26<47:57:30, 19.63s/it, loss_gram=0, loss_l2=0.0087, loss_lpips=0.098]  \n",
      "Steps:  12%|█▏        | 1205/10000 [6:29:44<46:17:44, 18.95s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.0836]\n",
      "Steps:  12%|█▏        | 1206/10000 [6:30:04<46:59:56, 19.24s/it, loss_gram=0, loss_l2=0.0255, loss_lpips=0.226] \n",
      "Steps:  12%|█▏        | 1207/10000 [6:30:21<45:47:05, 18.75s/it, loss_gram=0, loss_l2=0.00478, loss_lpips=0.151]\n",
      "Steps:  12%|█▏        | 1208/10000 [6:30:40<46:01:11, 18.84s/it, loss_gram=0, loss_l2=0.00343, loss_lpips=0.108]\n",
      "Steps:  12%|█▏        | 1209/10000 [6:30:58<45:34:58, 18.67s/it, loss_gram=0, loss_l2=0.00424, loss_lpips=0.14] \n",
      "Steps:  12%|█▏        | 1210/10000 [6:31:19<46:39:41, 19.11s/it, loss_gram=0, loss_l2=0.00448, loss_lpips=0.0808]\n",
      "Steps:  12%|█▏        | 1211/10000 [6:31:36<45:43:41, 18.73s/it, loss_gram=0, loss_l2=0.00707, loss_lpips=0.116] \n",
      "Steps:  12%|█▏        | 1212/10000 [6:31:57<46:46:16, 19.16s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.108] \n",
      "Steps:  12%|█▏        | 1213/10000 [6:32:15<46:11:03, 18.92s/it, loss_gram=0, loss_l2=0.00836, loss_lpips=0.122]\n",
      "Steps:  12%|█▏        | 1214/10000 [6:32:36<47:31:50, 19.48s/it, loss_gram=0, loss_l2=0.00858, loss_lpips=0.11] \n",
      "Steps:  12%|█▏        | 1215/10000 [6:32:54<46:37:55, 19.11s/it, loss_gram=0, loss_l2=0.00813, loss_lpips=0.103]\n",
      "Steps:  12%|█▏        | 1216/10000 [6:33:15<48:17:02, 19.79s/it, loss_gram=0, loss_l2=0.0283, loss_lpips=0.218] \n",
      "Steps:  12%|█▏        | 1217/10000 [6:33:32<46:02:36, 18.87s/it, loss_gram=0, loss_l2=0.0195, loss_lpips=0.139]\n",
      "Steps:  12%|█▏        | 1218/10000 [6:33:53<47:41:33, 19.55s/it, loss_gram=0, loss_l2=0.012, loss_lpips=0.0875]\n",
      "Steps:  12%|█▏        | 1219/10000 [6:34:11<46:21:20, 19.00s/it, loss_gram=0, loss_l2=0.00123, loss_lpips=0.0739]\n",
      "Steps:  12%|█▏        | 1220/10000 [6:34:30<46:42:31, 19.15s/it, loss_gram=0, loss_l2=0.0089, loss_lpips=0.108]  \n",
      "Steps:  12%|█▏        | 1221/10000 [6:34:48<45:24:52, 18.62s/it, loss_gram=0, loss_l2=0.0239, loss_lpips=0.171]\n",
      "Steps:  12%|█▏        | 1222/10000 [6:35:08<46:33:54, 19.10s/it, loss_gram=0, loss_l2=0.00818, loss_lpips=0.109]\n",
      "Steps:  12%|█▏        | 1223/10000 [6:35:25<45:24:22, 18.62s/it, loss_gram=0, loss_l2=0.00302, loss_lpips=0.147]\n",
      "Steps:  12%|█▏        | 1224/10000 [6:35:46<46:40:27, 19.15s/it, loss_gram=0, loss_l2=0.00155, loss_lpips=0.0952]\n",
      "Steps:  12%|█▏        | 1225/10000 [6:36:04<45:51:57, 18.82s/it, loss_gram=0, loss_l2=0.00815, loss_lpips=0.0891]\n",
      "Steps:  12%|█▏        | 1226/10000 [6:36:25<46:57:05, 19.26s/it, loss_gram=0, loss_l2=0.0155, loss_lpips=0.13]   \n",
      "Steps:  12%|█▏        | 1227/10000 [6:36:44<46:45:22, 19.19s/it, loss_gram=0, loss_l2=0.00878, loss_lpips=0.125]\n",
      "Steps:  12%|█▏        | 1228/10000 [6:37:04<47:58:11, 19.69s/it, loss_gram=0, loss_l2=0.00112, loss_lpips=0.115]\n",
      "Steps:  12%|█▏        | 1229/10000 [6:37:23<47:11:37, 19.37s/it, loss_gram=0, loss_l2=0.0434, loss_lpips=0.243] \n",
      "Steps:  12%|█▏        | 1230/10000 [6:37:43<47:55:11, 19.67s/it, loss_gram=0, loss_l2=0.0197, loss_lpips=0.0918]\n",
      "Steps:  12%|█▏        | 1231/10000 [6:38:00<46:01:29, 18.89s/it, loss_gram=0, loss_l2=0.00963, loss_lpips=0.0945]\n",
      "Steps:  12%|█▏        | 1232/10000 [6:38:22<47:34:00, 19.53s/it, loss_gram=0, loss_l2=0.00469, loss_lpips=0.0996]\n",
      "Steps:  12%|█▏        | 1233/10000 [6:38:39<46:17:44, 19.01s/it, loss_gram=0, loss_l2=0.00903, loss_lpips=0.0933]\n",
      "Steps:  12%|█▏        | 1234/10000 [6:38:58<46:37:09, 19.15s/it, loss_gram=0, loss_l2=0.0121, loss_lpips=0.117]  \n",
      "Steps:  12%|█▏        | 1235/10000 [6:39:17<46:09:13, 18.96s/it, loss_gram=0, loss_l2=0.00585, loss_lpips=0.217]\n",
      "Steps:  12%|█▏        | 1236/10000 [6:39:37<46:42:43, 19.19s/it, loss_gram=0, loss_l2=0.000819, loss_lpips=0.107]\n",
      "Steps:  12%|█▏        | 1237/10000 [6:39:56<46:49:08, 19.23s/it, loss_gram=0, loss_l2=0.00565, loss_lpips=0.125] \n",
      "Steps:  12%|█▏        | 1238/10000 [6:40:16<47:09:08, 19.37s/it, loss_gram=0, loss_l2=0.0207, loss_lpips=0.116] \n",
      "Steps:  12%|█▏        | 1239/10000 [6:40:34<46:32:01, 19.12s/it, loss_gram=0, loss_l2=0.0103, loss_lpips=0.0854]\n",
      "Steps:  12%|█▏        | 1240/10000 [6:40:56<48:01:03, 19.73s/it, loss_gram=0, loss_l2=0.00512, loss_lpips=0.11] \n",
      "Steps:  12%|█▏        | 1241/10000 [6:41:13<46:24:07, 19.07s/it, loss_gram=0, loss_l2=0.00607, loss_lpips=0.132]\n",
      "Steps:  12%|█▏        | 1242/10000 [6:41:32<46:27:03, 19.09s/it, loss_gram=0, loss_l2=0.00365, loss_lpips=0.0698]\n",
      "Steps:  12%|█▏        | 1243/10000 [6:41:49<45:05:28, 18.54s/it, loss_gram=0, loss_l2=0.00376, loss_lpips=0.0851]\n",
      "Steps:  12%|█▏        | 1244/10000 [6:42:09<46:12:30, 19.00s/it, loss_gram=0, loss_l2=0.0226, loss_lpips=0.126]  \n",
      "Steps:  12%|█▏        | 1245/10000 [6:42:28<45:43:15, 18.80s/it, loss_gram=0, loss_l2=0.0053, loss_lpips=0.116]\n",
      "Steps:  12%|█▏        | 1246/10000 [6:42:48<47:10:26, 19.40s/it, loss_gram=0, loss_l2=0.0165, loss_lpips=0.12] \n",
      "Steps:  12%|█▏        | 1247/10000 [6:43:06<45:58:17, 18.91s/it, loss_gram=0, loss_l2=0.00576, loss_lpips=0.0803]\n",
      "Steps:  12%|█▏        | 1248/10000 [6:43:26<46:29:57, 19.13s/it, loss_gram=0, loss_l2=0.00566, loss_lpips=0.108] \n",
      "Steps:  12%|█▏        | 1249/10000 [6:43:43<45:06:23, 18.56s/it, loss_gram=0, loss_l2=0.00969, loss_lpips=0.133]\n",
      "Steps:  12%|█▎        | 1250/10000 [6:44:03<46:16:36, 19.04s/it, loss_gram=0, loss_l2=0.0417, loss_lpips=0.208] \n",
      "Steps:  13%|█▎        | 1251/10000 [6:44:22<45:48:48, 18.85s/it, loss_gram=0, loss_l2=0.00706, loss_lpips=0.0795]\n",
      "Steps:  13%|█▎        | 1252/10000 [6:44:42<46:36:35, 19.18s/it, loss_gram=0, loss_l2=0.00791, loss_lpips=0.0914]\n",
      "Steps:  13%|█▎        | 1253/10000 [6:44:58<44:43:36, 18.41s/it, loss_gram=0, loss_l2=0.00792, loss_lpips=0.1]   \n",
      "Steps:  13%|█▎        | 1254/10000 [6:45:19<46:11:48, 19.02s/it, loss_gram=0, loss_l2=0.0059, loss_lpips=0.119]\n",
      "Steps:  13%|█▎        | 1255/10000 [6:45:36<44:39:30, 18.38s/it, loss_gram=0, loss_l2=0.00297, loss_lpips=0.0719]\n",
      "Steps:  13%|█▎        | 1256/10000 [6:45:56<45:53:40, 18.90s/it, loss_gram=0, loss_l2=0.00934, loss_lpips=0.0735]\n",
      "Steps:  13%|█▎        | 1257/10000 [6:46:13<44:50:00, 18.46s/it, loss_gram=0, loss_l2=0.00555, loss_lpips=0.113] \n",
      "Steps:  13%|█▎        | 1258/10000 [6:46:35<47:06:18, 19.40s/it, loss_gram=0, loss_l2=0.0393, loss_lpips=0.219] \n",
      "Steps:  13%|█▎        | 1259/10000 [6:46:53<46:21:24, 19.09s/it, loss_gram=0, loss_l2=0.00491, loss_lpips=0.0994]\n",
      "Steps:  13%|█▎        | 1260/10000 [6:47:12<46:21:19, 19.09s/it, loss_gram=0, loss_l2=0.0172, loss_lpips=0.102]  \n",
      "Steps:  13%|█▎        | 1261/10000 [6:47:29<44:45:30, 18.44s/it, loss_gram=0, loss_l2=0.0164, loss_lpips=0.153]\n",
      "Steps:  13%|█▎        | 1262/10000 [6:47:49<45:34:57, 18.78s/it, loss_gram=0, loss_l2=0.0462, loss_lpips=0.215]\n",
      "Steps:  13%|█▎        | 1263/10000 [6:48:06<44:40:02, 18.40s/it, loss_gram=0, loss_l2=0.0201, loss_lpips=0.137]\n",
      "Steps:  13%|█▎        | 1264/10000 [6:48:25<44:53:56, 18.50s/it, loss_gram=0, loss_l2=0.0223, loss_lpips=0.121]\n",
      "Steps:  13%|█▎        | 1265/10000 [6:48:43<44:21:01, 18.28s/it, loss_gram=0, loss_l2=0.0069, loss_lpips=0.092]\n",
      "Steps:  13%|█▎        | 1266/10000 [6:49:02<45:15:23, 18.65s/it, loss_gram=0, loss_l2=0.00738, loss_lpips=0.156]\n",
      "Steps:  13%|█▎        | 1267/10000 [6:49:21<45:15:10, 18.65s/it, loss_gram=0, loss_l2=0.00512, loss_lpips=0.172]\n",
      "Steps:  13%|█▎        | 1268/10000 [6:49:41<46:05:16, 19.00s/it, loss_gram=0, loss_l2=0.0159, loss_lpips=0.187] \n",
      "Steps:  13%|█▎        | 1269/10000 [6:49:58<45:04:44, 18.59s/it, loss_gram=0, loss_l2=0.0193, loss_lpips=0.121]\n",
      "Steps:  13%|█▎        | 1270/10000 [6:50:18<45:38:15, 18.82s/it, loss_gram=0, loss_l2=0.00881, loss_lpips=0.104]\n",
      "Steps:  13%|█▎        | 1271/10000 [6:50:36<45:30:36, 18.77s/it, loss_gram=0, loss_l2=0.00533, loss_lpips=0.0986]\n",
      "Steps:  13%|█▎        | 1272/10000 [6:50:57<46:39:17, 19.24s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.107]  \n",
      "Steps:  13%|█▎        | 1273/10000 [6:51:14<45:18:21, 18.69s/it, loss_gram=0, loss_l2=0.00591, loss_lpips=0.158]\n",
      "Steps:  13%|█▎        | 1274/10000 [6:51:36<47:35:10, 19.63s/it, loss_gram=0, loss_l2=0.00282, loss_lpips=0.132]\n",
      "Steps:  13%|█▎        | 1275/10000 [6:51:53<45:44:50, 18.88s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.135] \n",
      "Steps:  13%|█▎        | 1276/10000 [6:52:14<46:58:27, 19.38s/it, loss_gram=0, loss_l2=0.0194, loss_lpips=0.144]\n",
      "Steps:  13%|█▎        | 1277/10000 [6:52:31<45:20:19, 18.71s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.119]\n",
      "Steps:  13%|█▎        | 1278/10000 [6:52:51<45:59:19, 18.98s/it, loss_gram=0, loss_l2=0.0153, loss_lpips=0.117]\n",
      "Steps:  13%|█▎        | 1279/10000 [6:53:11<46:51:40, 19.34s/it, loss_gram=0, loss_l2=0.00525, loss_lpips=0.098]\n",
      "Steps:  13%|█▎        | 1280/10000 [6:53:29<46:18:04, 19.12s/it, loss_gram=0, loss_l2=0.0244, loss_lpips=0.145] \n",
      "Steps:  13%|█▎        | 1281/10000 [6:53:46<44:57:37, 18.56s/it, loss_gram=0, loss_l2=0.0087, loss_lpips=0.071]\n",
      "Steps:  13%|█▎        | 1282/10000 [6:54:07<46:29:36, 19.20s/it, loss_gram=0, loss_l2=0.0124, loss_lpips=0.136]\n",
      "Steps:  13%|█▎        | 1283/10000 [6:54:24<44:47:28, 18.50s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.0911]\n",
      "Steps:  13%|█▎        | 1284/10000 [6:54:43<45:19:13, 18.72s/it, loss_gram=0, loss_l2=0.0018, loss_lpips=0.135] \n",
      "Steps:  13%|█▎        | 1285/10000 [6:55:02<45:26:11, 18.77s/it, loss_gram=0, loss_l2=0.0276, loss_lpips=0.142]\n",
      "Steps:  13%|█▎        | 1286/10000 [6:55:25<48:20:58, 19.97s/it, loss_gram=0, loss_l2=0.00666, loss_lpips=0.108]\n",
      "Steps:  13%|█▎        | 1287/10000 [6:55:43<47:06:39, 19.47s/it, loss_gram=0, loss_l2=0.000968, loss_lpips=0.171]\n",
      "Steps:  13%|█▎        | 1288/10000 [6:56:01<46:16:45, 19.12s/it, loss_gram=0, loss_l2=0.0237, loss_lpips=0.13]   \n",
      "Steps:  13%|█▎        | 1289/10000 [6:56:21<46:12:24, 19.10s/it, loss_gram=0, loss_l2=0.00979, loss_lpips=0.0935]\n",
      "Steps:  13%|█▎        | 1290/10000 [6:56:40<46:06:07, 19.05s/it, loss_gram=0, loss_l2=0.0156, loss_lpips=0.242]  \n",
      "Steps:  13%|█▎        | 1291/10000 [6:56:58<45:40:32, 18.88s/it, loss_gram=0, loss_l2=0.183, loss_lpips=0.328] \n",
      "Steps:  13%|█▎        | 1292/10000 [6:57:17<45:32:24, 18.83s/it, loss_gram=0, loss_l2=0.00743, loss_lpips=0.093]\n",
      "Steps:  13%|█▎        | 1293/10000 [6:57:35<45:09:23, 18.67s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.0677]\n",
      "Steps:  13%|█▎        | 1294/10000 [6:57:54<45:34:46, 18.85s/it, loss_gram=0, loss_l2=0.00252, loss_lpips=0.0875]\n",
      "Steps:  13%|█▎        | 1295/10000 [6:58:11<44:00:58, 18.20s/it, loss_gram=0, loss_l2=0.0195, loss_lpips=0.107]  \n",
      "Steps:  13%|█▎        | 1296/10000 [6:58:31<45:40:00, 18.89s/it, loss_gram=0, loss_l2=0.0128, loss_lpips=0.216]\n",
      "Steps:  13%|█▎        | 1297/10000 [6:58:50<45:26:23, 18.80s/it, loss_gram=0, loss_l2=0.00719, loss_lpips=0.104]\n",
      "Steps:  13%|█▎        | 1298/10000 [6:59:10<45:59:39, 19.03s/it, loss_gram=0, loss_l2=0.00371, loss_lpips=0.182]\n",
      "Steps:  13%|█▎        | 1299/10000 [6:59:28<45:13:19, 18.71s/it, loss_gram=0, loss_l2=0.00605, loss_lpips=0.157]\n",
      "Steps:  13%|█▎        | 1300/10000 [6:59:47<45:37:16, 18.88s/it, loss_gram=0, loss_l2=0.017, loss_lpips=0.12]   \n",
      "Steps:  13%|█▎        | 1301/10000 [7:00:06<45:34:17, 18.86s/it, loss_gram=0, loss_l2=0.00347, loss_lpips=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  13%|█▎        | 1302/10000 [7:00:27<47:13:34, 19.55s/it, loss_gram=0, loss_l2=0.00632, loss_lpips=0.15] \n",
      "Steps:  13%|█▎        | 1303/10000 [7:00:44<46:03:26, 19.06s/it, loss_gram=0, loss_l2=0.0181, loss_lpips=0.13] \n",
      "Steps:  13%|█▎        | 1304/10000 [7:01:05<46:59:42, 19.46s/it, loss_gram=0, loss_l2=0.0345, loss_lpips=0.255]\n",
      "Steps:  13%|█▎        | 1305/10000 [7:01:23<45:55:23, 19.01s/it, loss_gram=0, loss_l2=0.014, loss_lpips=0.146] \n",
      "Steps:  13%|█▎        | 1306/10000 [7:01:43<46:49:32, 19.39s/it, loss_gram=0, loss_l2=0.00258, loss_lpips=0.0883]\n",
      "Steps:  13%|█▎        | 1307/10000 [7:02:00<45:09:03, 18.70s/it, loss_gram=0, loss_l2=0.00914, loss_lpips=0.132] \n",
      "Steps:  13%|█▎        | 1308/10000 [7:02:20<45:48:45, 18.97s/it, loss_gram=0, loss_l2=0.00322, loss_lpips=0.0793]\n",
      "Steps:  13%|█▎        | 1309/10000 [7:02:38<45:09:34, 18.71s/it, loss_gram=0, loss_l2=0.0159, loss_lpips=0.129]  \n",
      "Steps:  13%|█▎        | 1310/10000 [7:02:59<46:53:12, 19.42s/it, loss_gram=0, loss_l2=0.0044, loss_lpips=0.0964]\n",
      "Steps:  13%|█▎        | 1311/10000 [7:03:19<47:30:02, 19.68s/it, loss_gram=0, loss_l2=0.0155, loss_lpips=0.136] \n",
      "Steps:  13%|█▎        | 1312/10000 [7:03:43<50:31:20, 20.93s/it, loss_gram=0, loss_l2=0.00902, loss_lpips=0.126]\n",
      "Steps:  13%|█▎        | 1313/10000 [7:04:04<49:55:25, 20.69s/it, loss_gram=0, loss_l2=0.00768, loss_lpips=0.114]\n",
      "Steps:  13%|█▎        | 1314/10000 [7:04:25<50:46:30, 21.04s/it, loss_gram=0, loss_l2=0.00151, loss_lpips=0.107]\n",
      "Steps:  13%|█▎        | 1315/10000 [7:04:45<50:06:03, 20.77s/it, loss_gram=0, loss_l2=0.00859, loss_lpips=0.0928]\n",
      "Steps:  13%|█▎        | 1316/10000 [7:05:09<51:42:51, 21.44s/it, loss_gram=0, loss_l2=0.0017, loss_lpips=0.0975] \n",
      "Steps:  13%|█▎        | 1317/10000 [7:05:28<50:18:35, 20.86s/it, loss_gram=0, loss_l2=0.00779, loss_lpips=0.128]\n",
      "Steps:  13%|█▎        | 1318/10000 [7:05:51<51:22:06, 21.30s/it, loss_gram=0, loss_l2=0.00511, loss_lpips=0.11] \n",
      "Steps:  13%|█▎        | 1319/10000 [7:06:12<51:12:26, 21.24s/it, loss_gram=0, loss_l2=0.00725, loss_lpips=0.109]\n",
      "Steps:  13%|█▎        | 1320/10000 [7:06:32<50:56:48, 21.13s/it, loss_gram=0, loss_l2=0.0271, loss_lpips=0.196] \n",
      "Steps:  13%|█▎        | 1321/10000 [7:06:52<49:32:31, 20.55s/it, loss_gram=0, loss_l2=0.00384, loss_lpips=0.0945]\n",
      "Steps:  13%|█▎        | 1322/10000 [7:07:12<49:39:51, 20.60s/it, loss_gram=0, loss_l2=0.00177, loss_lpips=0.0767]\n",
      "Steps:  13%|█▎        | 1323/10000 [7:07:31<48:39:39, 20.19s/it, loss_gram=0, loss_l2=0.0315, loss_lpips=0.139]  \n",
      "Steps:  13%|█▎        | 1324/10000 [7:07:53<49:43:07, 20.63s/it, loss_gram=0, loss_l2=0.00356, loss_lpips=0.0945]\n",
      "Steps:  13%|█▎        | 1325/10000 [7:08:12<48:39:07, 20.19s/it, loss_gram=0, loss_l2=0.00703, loss_lpips=0.0869]\n",
      "Steps:  13%|█▎        | 1326/10000 [7:08:35<50:45:40, 21.07s/it, loss_gram=0, loss_l2=0.0132, loss_lpips=0.141]  \n",
      "Steps:  13%|█▎        | 1327/10000 [7:08:55<49:37:35, 20.60s/it, loss_gram=0, loss_l2=0.0151, loss_lpips=0.0958]\n",
      "Steps:  13%|█▎        | 1328/10000 [7:09:19<52:10:13, 21.66s/it, loss_gram=0, loss_l2=0.0133, loss_lpips=0.102] \n",
      "Steps:  13%|█▎        | 1329/10000 [7:09:40<52:03:23, 21.61s/it, loss_gram=0, loss_l2=0.0013, loss_lpips=0.127]\n",
      "Steps:  13%|█▎        | 1330/10000 [7:10:03<52:48:02, 21.92s/it, loss_gram=0, loss_l2=0.00806, loss_lpips=0.159]\n",
      "Steps:  13%|█▎        | 1331/10000 [7:10:23<51:27:48, 21.37s/it, loss_gram=0, loss_l2=0.00593, loss_lpips=0.107]\n",
      "Steps:  13%|█▎        | 1332/10000 [7:10:46<52:35:13, 21.84s/it, loss_gram=0, loss_l2=0.00628, loss_lpips=0.1]  \n",
      "Steps:  13%|█▎        | 1333/10000 [7:11:06<51:23:13, 21.34s/it, loss_gram=0, loss_l2=0.00525, loss_lpips=0.0922]\n",
      "Steps:  13%|█▎        | 1334/10000 [7:11:30<52:50:59, 21.95s/it, loss_gram=0, loss_l2=0.0125, loss_lpips=0.103]  \n",
      "Steps:  13%|█▎        | 1335/10000 [7:11:50<51:36:00, 21.44s/it, loss_gram=0, loss_l2=0.0182, loss_lpips=0.0877]\n",
      "Steps:  13%|█▎        | 1336/10000 [7:12:12<51:55:39, 21.58s/it, loss_gram=0, loss_l2=0.0319, loss_lpips=0.137] \n",
      "Steps:  13%|█▎        | 1337/10000 [7:12:31<50:09:23, 20.84s/it, loss_gram=0, loss_l2=0.0111, loss_lpips=0.118]\n",
      "Steps:  13%|█▎        | 1338/10000 [7:12:52<50:19:10, 20.91s/it, loss_gram=0, loss_l2=0.0122, loss_lpips=0.137]\n",
      "Steps:  13%|█▎        | 1339/10000 [7:13:12<49:44:04, 20.67s/it, loss_gram=0, loss_l2=0.00966, loss_lpips=0.104]\n",
      "Steps:  13%|█▎        | 1340/10000 [7:13:36<52:12:08, 21.70s/it, loss_gram=0, loss_l2=0.0151, loss_lpips=0.124] \n",
      "Steps:  13%|█▎        | 1341/10000 [7:13:56<50:27:04, 20.98s/it, loss_gram=0, loss_l2=0.0292, loss_lpips=0.131]\n",
      "Steps:  13%|█▎        | 1342/10000 [7:14:18<51:48:30, 21.54s/it, loss_gram=0, loss_l2=0.00397, loss_lpips=0.085]\n",
      "Steps:  13%|█▎        | 1343/10000 [7:14:39<51:03:48, 21.23s/it, loss_gram=0, loss_l2=0.00925, loss_lpips=0.0808]\n",
      "Steps:  13%|█▎        | 1344/10000 [7:15:02<52:14:59, 21.73s/it, loss_gram=0, loss_l2=0.0103, loss_lpips=0.135]  \n",
      "Steps:  13%|█▎        | 1345/10000 [7:15:22<50:53:40, 21.17s/it, loss_gram=0, loss_l2=0.00708, loss_lpips=0.0957]\n",
      "Steps:  13%|█▎        | 1346/10000 [7:15:43<51:08:47, 21.28s/it, loss_gram=0, loss_l2=0.00145, loss_lpips=0.126] \n",
      "Steps:  13%|█▎        | 1347/10000 [7:16:04<50:32:14, 21.03s/it, loss_gram=0, loss_l2=0.00319, loss_lpips=0.0725]\n",
      "Steps:  13%|█▎        | 1348/10000 [7:16:27<52:04:17, 21.67s/it, loss_gram=0, loss_l2=0.00522, loss_lpips=0.102] \n",
      "Steps:  13%|█▎        | 1349/10000 [7:16:47<50:50:03, 21.15s/it, loss_gram=0, loss_l2=0.00305, loss_lpips=0.0754]\n",
      "Steps:  14%|█▎        | 1350/10000 [7:17:09<51:30:13, 21.44s/it, loss_gram=0, loss_l2=0.0162, loss_lpips=0.103]  \n",
      "Steps:  14%|█▎        | 1351/10000 [7:17:28<50:10:03, 20.88s/it, loss_gram=0, loss_l2=0.0254, loss_lpips=0.312]\n",
      "Steps:  14%|█▎        | 1352/10000 [7:17:51<51:28:19, 21.43s/it, loss_gram=0, loss_l2=0.0287, loss_lpips=0.209]\n",
      "Steps:  14%|█▎        | 1353/10000 [7:18:14<52:14:23, 21.75s/it, loss_gram=0, loss_l2=0.0014, loss_lpips=0.0825]\n",
      "Steps:  14%|█▎        | 1354/10000 [7:18:38<53:40:59, 22.35s/it, loss_gram=0, loss_l2=0.009, loss_lpips=0.0952] \n",
      "Steps:  14%|█▎        | 1355/10000 [7:18:58<51:58:36, 21.64s/it, loss_gram=0, loss_l2=0.00762, loss_lpips=0.105]\n",
      "Steps:  14%|█▎        | 1356/10000 [7:19:20<52:29:37, 21.86s/it, loss_gram=0, loss_l2=0.0105, loss_lpips=0.11]  \n",
      "Steps:  14%|█▎        | 1357/10000 [7:19:40<50:59:03, 21.24s/it, loss_gram=0, loss_l2=0.00878, loss_lpips=0.111]\n",
      "Steps:  14%|█▎        | 1358/10000 [7:20:03<52:23:42, 21.83s/it, loss_gram=0, loss_l2=0.0146, loss_lpips=0.109] \n",
      "Steps:  14%|█▎        | 1359/10000 [7:20:24<51:52:41, 21.61s/it, loss_gram=0, loss_l2=0.00335, loss_lpips=0.104]\n",
      "Steps:  14%|█▎        | 1360/10000 [7:20:45<51:45:09, 21.56s/it, loss_gram=0, loss_l2=0.00662, loss_lpips=0.0847]\n",
      "Steps:  14%|█▎        | 1361/10000 [7:21:05<50:12:43, 20.92s/it, loss_gram=0, loss_l2=0.000277, loss_lpips=0.104]\n",
      "Steps:  14%|█▎        | 1362/10000 [7:21:27<50:30:32, 21.05s/it, loss_gram=0, loss_l2=0.0247, loss_lpips=0.153]  \n",
      "Steps:  14%|█▎        | 1363/10000 [7:21:47<50:10:55, 20.92s/it, loss_gram=0, loss_l2=0.0067, loss_lpips=0.124]\n",
      "Steps:  14%|█▎        | 1364/10000 [7:22:09<51:23:47, 21.43s/it, loss_gram=0, loss_l2=0.0148, loss_lpips=0.0953]\n",
      "Steps:  14%|█▎        | 1365/10000 [7:22:30<50:33:03, 21.08s/it, loss_gram=0, loss_l2=0.0438, loss_lpips=0.134] \n",
      "Steps:  14%|█▎        | 1366/10000 [7:22:51<50:32:34, 21.07s/it, loss_gram=0, loss_l2=0.00786, loss_lpips=0.114]\n",
      "Steps:  14%|█▎        | 1367/10000 [7:23:09<48:39:27, 20.29s/it, loss_gram=0, loss_l2=0.00394, loss_lpips=0.0869]\n",
      "Steps:  14%|█▎        | 1368/10000 [7:23:31<49:47:53, 20.77s/it, loss_gram=0, loss_l2=0.00527, loss_lpips=0.0673]\n",
      "Steps:  14%|█▎        | 1369/10000 [7:23:49<47:54:05, 19.98s/it, loss_gram=0, loss_l2=0.00632, loss_lpips=0.0782]\n",
      "Steps:  14%|█▎        | 1370/10000 [7:24:10<48:44:24, 20.33s/it, loss_gram=0, loss_l2=0.00578, loss_lpips=0.112] \n",
      "Steps:  14%|█▎        | 1371/10000 [7:24:27<45:48:51, 19.11s/it, loss_gram=0, loss_l2=0.0305, loss_lpips=0.142] \n",
      "Steps:  14%|█▎        | 1372/10000 [7:24:47<46:25:03, 19.37s/it, loss_gram=0, loss_l2=0.0027, loss_lpips=0.113]\n",
      "Steps:  14%|█▎        | 1373/10000 [7:25:05<45:42:46, 19.08s/it, loss_gram=0, loss_l2=0.0136, loss_lpips=0.101]\n",
      "Steps:  14%|█▎        | 1374/10000 [7:25:27<47:48:14, 19.95s/it, loss_gram=0, loss_l2=0.0108, loss_lpips=0.0967]\n",
      "Steps:  14%|█▍        | 1375/10000 [7:25:46<47:14:22, 19.72s/it, loss_gram=0, loss_l2=0.00764, loss_lpips=0.0978]\n",
      "Steps:  14%|█▍        | 1376/10000 [7:26:06<46:57:56, 19.61s/it, loss_gram=0, loss_l2=0.0019, loss_lpips=0.128]  \n",
      "Steps:  14%|█▍        | 1377/10000 [7:26:25<46:43:28, 19.51s/it, loss_gram=0, loss_l2=0.00233, loss_lpips=0.105]\n",
      "Steps:  14%|█▍        | 1378/10000 [7:26:45<47:22:10, 19.78s/it, loss_gram=0, loss_l2=0.00319, loss_lpips=0.165]\n",
      "Steps:  14%|█▍        | 1379/10000 [7:27:03<46:09:37, 19.28s/it, loss_gram=0, loss_l2=0.00455, loss_lpips=0.0831]\n",
      "Steps:  14%|█▍        | 1380/10000 [7:27:22<46:13:01, 19.30s/it, loss_gram=0, loss_l2=0.00598, loss_lpips=0.0836]\n",
      "Steps:  14%|█▍        | 1381/10000 [7:27:40<44:39:41, 18.65s/it, loss_gram=0, loss_l2=0.0134, loss_lpips=0.139]  \n",
      "Steps:  14%|█▍        | 1382/10000 [7:28:01<46:25:03, 19.39s/it, loss_gram=0, loss_l2=0.00752, loss_lpips=0.0938]\n",
      "Steps:  14%|█▍        | 1383/10000 [7:28:20<45:51:00, 19.16s/it, loss_gram=0, loss_l2=0.013, loss_lpips=0.14]    \n",
      "Steps:  14%|█▍        | 1384/10000 [7:28:40<46:31:17, 19.44s/it, loss_gram=0, loss_l2=0.0245, loss_lpips=0.114]\n",
      "Steps:  14%|█▍        | 1385/10000 [7:28:57<45:26:46, 18.99s/it, loss_gram=0, loss_l2=0.00461, loss_lpips=0.0576]\n",
      "Steps:  14%|█▍        | 1386/10000 [7:29:18<46:34:58, 19.47s/it, loss_gram=0, loss_l2=0.00487, loss_lpips=0.109] \n",
      "Steps:  14%|█▍        | 1387/10000 [7:29:37<45:29:19, 19.01s/it, loss_gram=0, loss_l2=0.00459, loss_lpips=0.072]\n",
      "Steps:  14%|█▍        | 1388/10000 [7:29:57<46:54:12, 19.61s/it, loss_gram=0, loss_l2=0.00611, loss_lpips=0.118]\n",
      "Steps:  14%|█▍        | 1389/10000 [7:30:15<45:38:18, 19.08s/it, loss_gram=0, loss_l2=0.00337, loss_lpips=0.0984]\n",
      "Steps:  14%|█▍        | 1390/10000 [7:30:36<46:49:44, 19.58s/it, loss_gram=0, loss_l2=0.00515, loss_lpips=0.089] \n",
      "Steps:  14%|█▍        | 1391/10000 [7:30:55<46:21:22, 19.38s/it, loss_gram=0, loss_l2=0.022, loss_lpips=0.0975] \n",
      "Steps:  14%|█▍        | 1392/10000 [7:31:17<48:15:00, 20.18s/it, loss_gram=0, loss_l2=0.00454, loss_lpips=0.0728]\n",
      "Steps:  14%|█▍        | 1393/10000 [7:31:34<46:35:00, 19.48s/it, loss_gram=0, loss_l2=0.0065, loss_lpips=0.131]  \n",
      "Steps:  14%|█▍        | 1394/10000 [7:31:53<46:13:44, 19.34s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.216]\n",
      "Steps:  14%|█▍        | 1395/10000 [7:32:12<45:22:42, 18.98s/it, loss_gram=0, loss_l2=0.00689, loss_lpips=0.112]\n",
      "Steps:  14%|█▍        | 1396/10000 [7:32:29<44:28:24, 18.61s/it, loss_gram=0, loss_l2=0.00162, loss_lpips=0.059]\n",
      "Steps:  14%|█▍        | 1397/10000 [7:32:48<44:31:59, 18.64s/it, loss_gram=0, loss_l2=0.0108, loss_lpips=0.0986]\n",
      "Steps:  14%|█▍        | 1398/10000 [7:33:10<46:51:55, 19.61s/it, loss_gram=0, loss_l2=0.0124, loss_lpips=0.102] \n",
      "Steps:  14%|█▍        | 1399/10000 [7:33:30<47:03:30, 19.70s/it, loss_gram=0, loss_l2=0.00303, loss_lpips=0.153]\n",
      "Steps:  14%|█▍        | 1400/10000 [7:33:50<47:21:24, 19.82s/it, loss_gram=0, loss_l2=0.00867, loss_lpips=0.184]\n",
      "Steps:  14%|█▍        | 1401/10000 [7:34:09<46:49:35, 19.60s/it, loss_gram=0, loss_l2=0.00421, loss_lpips=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  14%|█▍        | 1402/10000 [7:34:33<49:57:20, 20.92s/it, loss_gram=0, loss_l2=0.00342, loss_lpips=0.0685]\n",
      "Steps:  14%|█▍        | 1403/10000 [7:34:54<50:04:41, 20.97s/it, loss_gram=0, loss_l2=0.00739, loss_lpips=0.0974]\n",
      "Steps:  14%|█▍        | 1404/10000 [7:35:15<50:07:07, 20.99s/it, loss_gram=0, loss_l2=0.0852, loss_lpips=0.313]  \n",
      "Steps:  14%|█▍        | 1405/10000 [7:35:34<48:51:09, 20.46s/it, loss_gram=0, loss_l2=0.00828, loss_lpips=0.0757]\n",
      "Steps:  14%|█▍        | 1406/10000 [7:35:57<50:33:48, 21.18s/it, loss_gram=0, loss_l2=0.00438, loss_lpips=0.128] \n",
      "Steps:  14%|█▍        | 1407/10000 [7:36:15<48:28:05, 20.31s/it, loss_gram=0, loss_l2=0.00717, loss_lpips=0.08] \n",
      "Steps:  14%|█▍        | 1408/10000 [7:36:38<50:00:23, 20.95s/it, loss_gram=0, loss_l2=0.00552, loss_lpips=0.113]\n",
      "Steps:  14%|█▍        | 1409/10000 [7:36:56<48:07:39, 20.17s/it, loss_gram=0, loss_l2=0.00375, loss_lpips=0.0929]\n",
      "Steps:  14%|█▍        | 1410/10000 [7:37:19<49:57:36, 20.94s/it, loss_gram=0, loss_l2=0.0135, loss_lpips=0.105]  \n",
      "Steps:  14%|█▍        | 1411/10000 [7:37:39<48:45:09, 20.43s/it, loss_gram=0, loss_l2=0.00905, loss_lpips=0.139]\n",
      "Steps:  14%|█▍        | 1412/10000 [7:38:00<49:28:42, 20.74s/it, loss_gram=0, loss_l2=0.00114, loss_lpips=0.101]\n",
      "Steps:  14%|█▍        | 1413/10000 [7:38:18<47:48:55, 20.05s/it, loss_gram=0, loss_l2=0.0298, loss_lpips=0.0958]\n",
      "Steps:  14%|█▍        | 1414/10000 [7:38:39<48:17:09, 20.25s/it, loss_gram=0, loss_l2=0.0183, loss_lpips=0.16]  \n",
      "Steps:  14%|█▍        | 1415/10000 [7:38:57<47:02:51, 19.73s/it, loss_gram=0, loss_l2=0.00704, loss_lpips=0.0864]\n",
      "Steps:  14%|█▍        | 1416/10000 [7:39:19<48:28:42, 20.33s/it, loss_gram=0, loss_l2=0.0117, loss_lpips=0.152]  \n",
      "Steps:  14%|█▍        | 1417/10000 [7:39:38<47:20:01, 19.85s/it, loss_gram=0, loss_l2=0.0135, loss_lpips=0.0964]\n",
      "Steps:  14%|█▍        | 1418/10000 [7:39:59<47:53:37, 20.09s/it, loss_gram=0, loss_l2=0.026, loss_lpips=0.107]  \n",
      "Steps:  14%|█▍        | 1419/10000 [7:40:19<47:53:57, 20.10s/it, loss_gram=0, loss_l2=0.00912, loss_lpips=0.0963]\n",
      "Steps:  14%|█▍        | 1420/10000 [7:40:40<48:45:08, 20.46s/it, loss_gram=0, loss_l2=0.00415, loss_lpips=0.0726]\n",
      "Steps:  14%|█▍        | 1421/10000 [7:40:59<47:28:38, 19.92s/it, loss_gram=0, loss_l2=0.00618, loss_lpips=0.115] \n",
      "Steps:  14%|█▍        | 1422/10000 [7:41:19<47:48:56, 20.07s/it, loss_gram=0, loss_l2=0.0177, loss_lpips=0.095] \n",
      "Steps:  14%|█▍        | 1423/10000 [7:41:37<45:49:12, 19.23s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.113]\n",
      "Steps:  14%|█▍        | 1424/10000 [7:41:56<46:26:13, 19.49s/it, loss_gram=0, loss_l2=0.0199, loss_lpips=0.166]\n",
      "Steps:  14%|█▍        | 1425/10000 [7:42:13<44:26:40, 18.66s/it, loss_gram=0, loss_l2=0.0064, loss_lpips=0.118]\n",
      "Steps:  14%|█▍        | 1426/10000 [7:42:35<46:44:43, 19.63s/it, loss_gram=0, loss_l2=0.015, loss_lpips=0.134] \n",
      "Steps:  14%|█▍        | 1427/10000 [7:42:52<44:59:21, 18.89s/it, loss_gram=0, loss_l2=0.00392, loss_lpips=0.0803]\n",
      "Steps:  14%|█▍        | 1428/10000 [7:43:12<45:27:03, 19.09s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.217]  \n",
      "Steps:  14%|█▍        | 1429/10000 [7:43:29<44:19:24, 18.62s/it, loss_gram=0, loss_l2=0.018, loss_lpips=0.12]  \n",
      "Steps:  14%|█▍        | 1430/10000 [7:43:49<44:57:48, 18.89s/it, loss_gram=0, loss_l2=0.00535, loss_lpips=0.063]\n",
      "Steps:  14%|█▍        | 1431/10000 [7:44:06<43:39:14, 18.34s/it, loss_gram=0, loss_l2=0.00739, loss_lpips=0.125]\n",
      "Steps:  14%|█▍        | 1432/10000 [7:44:26<44:43:39, 18.79s/it, loss_gram=0, loss_l2=0.00468, loss_lpips=0.101]\n",
      "Steps:  14%|█▍        | 1433/10000 [7:44:45<45:06:06, 18.95s/it, loss_gram=0, loss_l2=0.00814, loss_lpips=0.127]\n",
      "Steps:  14%|█▍        | 1434/10000 [7:45:05<45:38:16, 19.18s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.195] \n",
      "Steps:  14%|█▍        | 1435/10000 [7:45:25<46:38:17, 19.60s/it, loss_gram=0, loss_l2=0.0078, loss_lpips=0.0994]\n",
      "Steps:  14%|█▍        | 1436/10000 [7:45:49<49:30:55, 20.81s/it, loss_gram=0, loss_l2=0.00733, loss_lpips=0.0903]\n",
      "Steps:  14%|█▍        | 1437/10000 [7:46:08<48:11:11, 20.26s/it, loss_gram=0, loss_l2=0.00881, loss_lpips=0.115] \n",
      "Steps:  14%|█▍        | 1438/10000 [7:46:29<48:53:58, 20.56s/it, loss_gram=0, loss_l2=0.00918, loss_lpips=0.143]\n",
      "Steps:  14%|█▍        | 1439/10000 [7:46:48<47:37:17, 20.03s/it, loss_gram=0, loss_l2=0.0167, loss_lpips=0.079] \n",
      "Steps:  14%|█▍        | 1440/10000 [7:47:09<48:07:51, 20.24s/it, loss_gram=0, loss_l2=0.00157, loss_lpips=0.0732]\n",
      "Steps:  14%|█▍        | 1441/10000 [7:47:26<46:23:41, 19.51s/it, loss_gram=0, loss_l2=0.0583, loss_lpips=0.253]  \n",
      "Steps:  14%|█▍        | 1442/10000 [7:47:48<47:44:27, 20.08s/it, loss_gram=0, loss_l2=0.0329, loss_lpips=0.147]\n",
      "Steps:  14%|█▍        | 1443/10000 [7:48:06<46:31:15, 19.57s/it, loss_gram=0, loss_l2=0.0161, loss_lpips=0.113]\n",
      "Steps:  14%|█▍        | 1444/10000 [7:48:28<48:23:00, 20.36s/it, loss_gram=0, loss_l2=0.00558, loss_lpips=0.202]\n",
      "Steps:  14%|█▍        | 1445/10000 [7:48:46<46:48:51, 19.70s/it, loss_gram=0, loss_l2=0.00856, loss_lpips=0.11] \n",
      "Steps:  14%|█▍        | 1446/10000 [7:49:07<46:53:39, 19.74s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.14] \n",
      "Steps:  14%|█▍        | 1447/10000 [7:49:25<46:11:57, 19.45s/it, loss_gram=0, loss_l2=0.0213, loss_lpips=0.115]\n",
      "Steps:  14%|█▍        | 1448/10000 [7:49:45<46:13:40, 19.46s/it, loss_gram=0, loss_l2=0.00621, loss_lpips=0.0806]\n",
      "Steps:  14%|█▍        | 1449/10000 [7:50:03<45:05:48, 18.99s/it, loss_gram=0, loss_l2=0.0202, loss_lpips=0.106]  \n",
      "Steps:  14%|█▍        | 1450/10000 [7:50:24<47:08:41, 19.85s/it, loss_gram=0, loss_l2=0.00395, loss_lpips=0.0686]\n",
      "Steps:  15%|█▍        | 1451/10000 [7:50:44<47:06:39, 19.84s/it, loss_gram=0, loss_l2=0.0112, loss_lpips=0.105]  \n",
      "Steps:  15%|█▍        | 1452/10000 [7:51:06<48:30:58, 20.43s/it, loss_gram=0, loss_l2=0.00686, loss_lpips=0.0729]\n",
      "Steps:  15%|█▍        | 1453/10000 [7:51:25<47:31:44, 20.02s/it, loss_gram=0, loss_l2=0.00468, loss_lpips=0.104] \n",
      "Steps:  15%|█▍        | 1454/10000 [7:51:45<47:46:31, 20.13s/it, loss_gram=0, loss_l2=0.0106, loss_lpips=0.122] \n",
      "Steps:  15%|█▍        | 1455/10000 [7:52:03<46:09:53, 19.45s/it, loss_gram=0, loss_l2=0.00395, loss_lpips=0.09]\n",
      "Steps:  15%|█▍        | 1456/10000 [7:52:25<48:04:31, 20.26s/it, loss_gram=0, loss_l2=0.0581, loss_lpips=0.229]\n",
      "Steps:  15%|█▍        | 1457/10000 [7:52:47<49:09:33, 20.72s/it, loss_gram=0, loss_l2=0.00579, loss_lpips=0.0808]\n",
      "Steps:  15%|█▍        | 1458/10000 [7:53:08<48:54:13, 20.61s/it, loss_gram=0, loss_l2=0.0106, loss_lpips=0.135]  \n",
      "Steps:  15%|█▍        | 1459/10000 [7:53:24<46:06:07, 19.43s/it, loss_gram=0, loss_l2=0.00943, loss_lpips=0.0864]\n",
      "Steps:  15%|█▍        | 1460/10000 [7:53:46<47:20:55, 19.96s/it, loss_gram=0, loss_l2=0.00137, loss_lpips=0.142] \n",
      "Steps:  15%|█▍        | 1461/10000 [7:54:03<45:42:09, 19.27s/it, loss_gram=0, loss_l2=0.00578, loss_lpips=0.135]\n",
      "Steps:  15%|█▍        | 1462/10000 [7:54:25<47:27:07, 20.01s/it, loss_gram=0, loss_l2=0.00217, loss_lpips=0.121]\n",
      "Steps:  15%|█▍        | 1463/10000 [7:54:43<45:54:19, 19.36s/it, loss_gram=0, loss_l2=0.0138, loss_lpips=0.115] \n",
      "Steps:  15%|█▍        | 1464/10000 [7:55:02<46:04:47, 19.43s/it, loss_gram=0, loss_l2=0.0118, loss_lpips=0.104]\n",
      "Steps:  15%|█▍        | 1465/10000 [7:55:19<44:22:28, 18.72s/it, loss_gram=0, loss_l2=0.0159, loss_lpips=0.129]\n",
      "Steps:  15%|█▍        | 1466/10000 [7:55:39<45:14:00, 19.08s/it, loss_gram=0, loss_l2=0.0247, loss_lpips=0.131]\n",
      "Steps:  15%|█▍        | 1467/10000 [7:55:57<44:06:22, 18.61s/it, loss_gram=0, loss_l2=0.0347, loss_lpips=0.142]\n",
      "Steps:  15%|█▍        | 1468/10000 [7:56:17<44:51:58, 18.93s/it, loss_gram=0, loss_l2=0.0156, loss_lpips=0.104]\n",
      "Steps:  15%|█▍        | 1469/10000 [7:56:35<44:36:16, 18.82s/it, loss_gram=0, loss_l2=0.00341, loss_lpips=0.0814]\n",
      "Steps:  15%|█▍        | 1470/10000 [7:56:55<45:10:48, 19.07s/it, loss_gram=0, loss_l2=0.00788, loss_lpips=0.115] \n",
      "Steps:  15%|█▍        | 1471/10000 [7:57:13<44:12:34, 18.66s/it, loss_gram=0, loss_l2=0.00816, loss_lpips=0.103]\n",
      "Steps:  15%|█▍        | 1472/10000 [7:57:31<44:15:19, 18.68s/it, loss_gram=0, loss_l2=0.00582, loss_lpips=0.1]  \n",
      "Steps:  15%|█▍        | 1473/10000 [7:57:50<44:09:42, 18.64s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.121]\n",
      "Steps:  15%|█▍        | 1474/10000 [7:58:10<45:00:20, 19.00s/it, loss_gram=0, loss_l2=0.02, loss_lpips=0.137]  \n",
      "Steps:  15%|█▍        | 1475/10000 [7:58:28<44:14:33, 18.68s/it, loss_gram=0, loss_l2=0.0172, loss_lpips=0.117]\n",
      "Steps:  15%|█▍        | 1476/10000 [7:58:48<45:22:31, 19.16s/it, loss_gram=0, loss_l2=0.00741, loss_lpips=0.122]\n",
      "Steps:  15%|█▍        | 1477/10000 [7:59:06<44:34:29, 18.83s/it, loss_gram=0, loss_l2=0.00982, loss_lpips=0.104]\n",
      "Steps:  15%|█▍        | 1478/10000 [7:59:25<45:11:36, 19.09s/it, loss_gram=0, loss_l2=0.00501, loss_lpips=0.12] \n",
      "Steps:  15%|█▍        | 1479/10000 [7:59:44<44:31:09, 18.81s/it, loss_gram=0, loss_l2=0.0171, loss_lpips=0.0995]\n",
      "Steps:  15%|█▍        | 1480/10000 [8:00:06<46:44:12, 19.75s/it, loss_gram=0, loss_l2=0.0131, loss_lpips=0.139] \n",
      "Steps:  15%|█▍        | 1481/10000 [8:00:24<45:21:15, 19.17s/it, loss_gram=0, loss_l2=0.00344, loss_lpips=0.0673]\n",
      "Steps:  15%|█▍        | 1482/10000 [8:00:43<45:42:09, 19.32s/it, loss_gram=0, loss_l2=0.011, loss_lpips=0.121]   \n",
      "Steps:  15%|█▍        | 1483/10000 [8:01:01<44:49:00, 18.94s/it, loss_gram=0, loss_l2=0.00334, loss_lpips=0.0867]\n",
      "Steps:  15%|█▍        | 1484/10000 [8:01:20<44:26:07, 18.78s/it, loss_gram=0, loss_l2=0.0361, loss_lpips=0.131]  \n",
      "Steps:  15%|█▍        | 1485/10000 [8:01:38<43:48:39, 18.52s/it, loss_gram=0, loss_l2=0.00247, loss_lpips=0.0844]\n",
      "Steps:  15%|█▍        | 1486/10000 [8:01:59<45:20:05, 19.17s/it, loss_gram=0, loss_l2=0.0121, loss_lpips=0.113]  \n",
      "Steps:  15%|█▍        | 1487/10000 [8:02:18<45:26:45, 19.22s/it, loss_gram=0, loss_l2=0.0225, loss_lpips=0.134]\n",
      "Steps:  15%|█▍        | 1488/10000 [8:02:37<45:24:41, 19.21s/it, loss_gram=0, loss_l2=0.00244, loss_lpips=0.0841]\n",
      "Steps:  15%|█▍        | 1489/10000 [8:02:56<45:01:48, 19.05s/it, loss_gram=0, loss_l2=0.0131, loss_lpips=0.102]  \n",
      "Steps:  15%|█▍        | 1490/10000 [8:03:15<45:26:56, 19.23s/it, loss_gram=0, loss_l2=0.015, loss_lpips=0.236] \n",
      "Steps:  15%|█▍        | 1491/10000 [8:03:34<45:09:35, 19.11s/it, loss_gram=0, loss_l2=0.0122, loss_lpips=0.134]\n",
      "Steps:  15%|█▍        | 1492/10000 [8:03:53<45:22:06, 19.20s/it, loss_gram=0, loss_l2=0.0259, loss_lpips=0.125]\n",
      "Steps:  15%|█▍        | 1493/10000 [8:04:11<44:17:18, 18.74s/it, loss_gram=0, loss_l2=0.0148, loss_lpips=0.118]\n",
      "Steps:  15%|█▍        | 1494/10000 [8:04:33<46:24:59, 19.64s/it, loss_gram=0, loss_l2=0.0096, loss_lpips=0.18] \n",
      "Steps:  15%|█▍        | 1495/10000 [8:04:51<45:07:21, 19.10s/it, loss_gram=0, loss_l2=0.00584, loss_lpips=0.0792]\n",
      "Steps:  15%|█▍        | 1496/10000 [8:05:10<45:16:55, 19.17s/it, loss_gram=0, loss_l2=0.00532, loss_lpips=0.102] \n",
      "Steps:  15%|█▍        | 1497/10000 [8:05:28<44:28:45, 18.83s/it, loss_gram=0, loss_l2=0.045, loss_lpips=0.272]  \n",
      "Steps:  15%|█▍        | 1498/10000 [8:05:48<45:10:28, 19.13s/it, loss_gram=0, loss_l2=0.00779, loss_lpips=0.189]\n",
      "Steps:  15%|█▍        | 1499/10000 [8:06:07<44:50:54, 18.99s/it, loss_gram=0, loss_l2=0.00257, loss_lpips=0.112]\n",
      "Steps:  15%|█▌        | 1500/10000 [8:06:26<44:49:38, 18.99s/it, loss_gram=0, loss_l2=0.0193, loss_lpips=0.179] \n",
      "Steps:  15%|█▌        | 1501/10000 [8:06:45<45:30:56, 19.28s/it, loss_gram=0, loss_l2=0.00383, loss_lpips=0.0798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  15%|█▌        | 1502/10000 [8:07:04<45:02:46, 19.08s/it, loss_gram=0, loss_l2=0.00108, loss_lpips=0.135] \n",
      "Steps:  15%|█▌        | 1503/10000 [8:07:22<44:01:33, 18.65s/it, loss_gram=0, loss_l2=0.00357, loss_lpips=0.0981]\n",
      "Steps:  15%|█▌        | 1504/10000 [8:07:43<45:42:26, 19.37s/it, loss_gram=0, loss_l2=0.00954, loss_lpips=0.0823]\n",
      "Steps:  15%|█▌        | 1505/10000 [8:08:01<44:44:11, 18.96s/it, loss_gram=0, loss_l2=0.00442, loss_lpips=0.0828]\n",
      "Steps:  15%|█▌        | 1506/10000 [8:08:22<46:05:58, 19.54s/it, loss_gram=0, loss_l2=0.0108, loss_lpips=0.105]  \n",
      "Steps:  15%|█▌        | 1507/10000 [8:08:40<45:09:04, 19.14s/it, loss_gram=0, loss_l2=0.00376, loss_lpips=0.0852]\n",
      "Steps:  15%|█▌        | 1508/10000 [8:08:58<44:26:44, 18.84s/it, loss_gram=0, loss_l2=0.0114, loss_lpips=0.0814] \n",
      "Steps:  15%|█▌        | 1509/10000 [8:09:16<44:02:31, 18.67s/it, loss_gram=0, loss_l2=0.00578, loss_lpips=0.11] \n",
      "Steps:  15%|█▌        | 1510/10000 [8:09:36<44:53:59, 19.04s/it, loss_gram=0, loss_l2=0.0096, loss_lpips=0.0753]\n",
      "Steps:  15%|█▌        | 1511/10000 [8:09:54<44:16:09, 18.77s/it, loss_gram=0, loss_l2=0.00902, loss_lpips=0.0808]\n",
      "Steps:  15%|█▌        | 1512/10000 [8:10:14<44:42:15, 18.96s/it, loss_gram=0, loss_l2=0.00691, loss_lpips=0.0751]\n",
      "Steps:  15%|█▌        | 1513/10000 [8:10:31<43:33:12, 18.47s/it, loss_gram=0, loss_l2=0.0127, loss_lpips=0.108]  \n",
      "Steps:  15%|█▌        | 1514/10000 [8:10:51<44:43:26, 18.97s/it, loss_gram=0, loss_l2=0.00661, loss_lpips=0.22]\n",
      "Steps:  15%|█▌        | 1515/10000 [8:11:10<44:50:30, 19.03s/it, loss_gram=0, loss_l2=0.0151, loss_lpips=0.136]\n",
      "Steps:  15%|█▌        | 1516/10000 [8:11:31<45:53:43, 19.47s/it, loss_gram=0, loss_l2=0.00144, loss_lpips=0.0985]\n",
      "Steps:  15%|█▌        | 1517/10000 [8:11:49<44:57:54, 19.08s/it, loss_gram=0, loss_l2=0.0322, loss_lpips=0.161]  \n",
      "Steps:  15%|█▌        | 1518/10000 [8:12:10<45:57:54, 19.51s/it, loss_gram=0, loss_l2=0.000522, loss_lpips=0.172]\n",
      "Steps:  15%|█▌        | 1519/10000 [8:12:29<45:46:40, 19.43s/it, loss_gram=0, loss_l2=0.0015, loss_lpips=0.1]    \n",
      "Steps:  15%|█▌        | 1520/10000 [8:12:50<47:15:47, 20.06s/it, loss_gram=0, loss_l2=0.00561, loss_lpips=0.142]\n",
      "Steps:  15%|█▌        | 1521/10000 [8:13:09<46:13:48, 19.63s/it, loss_gram=0, loss_l2=0.00363, loss_lpips=0.112]\n",
      "Steps:  15%|█▌        | 1522/10000 [8:13:29<46:29:33, 19.74s/it, loss_gram=0, loss_l2=0.0141, loss_lpips=0.122] \n",
      "Steps:  15%|█▌        | 1523/10000 [8:13:46<44:30:40, 18.90s/it, loss_gram=0, loss_l2=0.00525, loss_lpips=0.0833]\n",
      "Steps:  15%|█▌        | 1524/10000 [8:14:06<45:30:39, 19.33s/it, loss_gram=0, loss_l2=0.00251, loss_lpips=0.113] \n",
      "Steps:  15%|█▌        | 1525/10000 [8:14:25<45:10:10, 19.19s/it, loss_gram=0, loss_l2=0.00503, loss_lpips=0.0981]\n",
      "Steps:  15%|█▌        | 1526/10000 [8:14:44<45:25:36, 19.30s/it, loss_gram=0, loss_l2=0.0635, loss_lpips=0.25]   \n",
      "Steps:  15%|█▌        | 1527/10000 [8:15:04<45:17:19, 19.24s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.139]\n",
      "Steps:  15%|█▌        | 1528/10000 [8:15:24<45:32:23, 19.35s/it, loss_gram=0, loss_l2=0.0358, loss_lpips=0.0997]\n",
      "Steps:  15%|█▌        | 1529/10000 [8:15:42<45:08:59, 19.19s/it, loss_gram=0, loss_l2=0.0133, loss_lpips=0.124] \n",
      "Steps:  15%|█▌        | 1530/10000 [8:16:02<45:58:29, 19.54s/it, loss_gram=0, loss_l2=0.00686, loss_lpips=0.132]\n",
      "Steps:  15%|█▌        | 1531/10000 [8:16:20<44:22:58, 18.87s/it, loss_gram=0, loss_l2=0.00177, loss_lpips=0.0959]\n",
      "Steps:  15%|█▌        | 1532/10000 [8:16:40<45:39:19, 19.41s/it, loss_gram=0, loss_l2=0.000915, loss_lpips=0.111]\n",
      "Steps:  15%|█▌        | 1533/10000 [8:16:59<44:51:51, 19.08s/it, loss_gram=0, loss_l2=0.00386, loss_lpips=0.102] \n",
      "Steps:  15%|█▌        | 1534/10000 [8:17:18<45:03:45, 19.16s/it, loss_gram=0, loss_l2=0.013, loss_lpips=0.101]  \n",
      "Steps:  15%|█▌        | 1535/10000 [8:17:37<44:49:15, 19.06s/it, loss_gram=0, loss_l2=0.0174, loss_lpips=0.117]\n",
      "Steps:  15%|█▌        | 1536/10000 [8:17:56<44:50:09, 19.07s/it, loss_gram=0, loss_l2=0.0165, loss_lpips=0.126]\n",
      "Steps:  15%|█▌        | 1537/10000 [8:18:14<44:21:29, 18.87s/it, loss_gram=0, loss_l2=0.00533, loss_lpips=0.102]\n",
      "Steps:  15%|█▌        | 1538/10000 [8:18:33<44:11:34, 18.80s/it, loss_gram=0, loss_l2=0.00607, loss_lpips=0.104]\n",
      "Steps:  15%|█▌        | 1539/10000 [8:18:51<43:41:24, 18.59s/it, loss_gram=0, loss_l2=0.00901, loss_lpips=0.079]\n",
      "Steps:  15%|█▌        | 1540/10000 [8:19:10<43:44:06, 18.61s/it, loss_gram=0, loss_l2=0.00595, loss_lpips=0.122]\n",
      "Steps:  15%|█▌        | 1541/10000 [8:19:28<43:23:11, 18.46s/it, loss_gram=0, loss_l2=0.00599, loss_lpips=0.0874]\n",
      "Steps:  15%|█▌        | 1542/10000 [8:19:48<44:32:54, 18.96s/it, loss_gram=0, loss_l2=0.00259, loss_lpips=0.13]  \n",
      "Steps:  15%|█▌        | 1543/10000 [8:20:06<43:53:39, 18.69s/it, loss_gram=0, loss_l2=0.018, loss_lpips=0.0813]\n",
      "Steps:  15%|█▌        | 1544/10000 [8:20:26<44:38:01, 19.00s/it, loss_gram=0, loss_l2=0.0419, loss_lpips=0.218]\n",
      "Steps:  15%|█▌        | 1545/10000 [8:20:44<43:58:54, 18.73s/it, loss_gram=0, loss_l2=0.00781, loss_lpips=0.11]\n",
      "Steps:  15%|█▌        | 1546/10000 [8:21:05<45:22:54, 19.33s/it, loss_gram=0, loss_l2=0.00377, loss_lpips=0.0889]\n",
      "Steps:  15%|█▌        | 1547/10000 [8:21:22<43:42:07, 18.61s/it, loss_gram=0, loss_l2=0.00574, loss_lpips=0.0867]\n",
      "Steps:  15%|█▌        | 1548/10000 [8:21:41<43:55:11, 18.71s/it, loss_gram=0, loss_l2=0.0381, loss_lpips=0.153]  \n",
      "Steps:  15%|█▌        | 1549/10000 [8:22:00<44:06:08, 18.79s/it, loss_gram=0, loss_l2=0.0276, loss_lpips=0.131]\n",
      "Steps:  16%|█▌        | 1550/10000 [8:22:19<44:47:59, 19.09s/it, loss_gram=0, loss_l2=0.00524, loss_lpips=0.0928]\n",
      "Steps:  16%|█▌        | 1551/10000 [8:22:37<43:52:16, 18.69s/it, loss_gram=0, loss_l2=0.00705, loss_lpips=0.0813]\n",
      "Steps:  16%|█▌        | 1552/10000 [8:22:55<43:37:29, 18.59s/it, loss_gram=0, loss_l2=0.0119, loss_lpips=0.107]  \n",
      "Steps:  16%|█▌        | 1553/10000 [8:23:13<43:10:18, 18.40s/it, loss_gram=0, loss_l2=0.00672, loss_lpips=0.0775]\n",
      "Steps:  16%|█▌        | 1554/10000 [8:23:34<44:39:44, 19.04s/it, loss_gram=0, loss_l2=0.00843, loss_lpips=0.0854]\n",
      "Steps:  16%|█▌        | 1555/10000 [8:23:52<44:17:42, 18.88s/it, loss_gram=0, loss_l2=0.00337, loss_lpips=0.105] \n",
      "Steps:  16%|█▌        | 1556/10000 [8:24:14<45:46:25, 19.52s/it, loss_gram=0, loss_l2=0.00278, loss_lpips=0.0859]\n",
      "Steps:  16%|█▌        | 1557/10000 [8:24:31<44:32:11, 18.99s/it, loss_gram=0, loss_l2=0.00573, loss_lpips=0.128] \n",
      "Steps:  16%|█▌        | 1558/10000 [8:24:51<45:26:38, 19.38s/it, loss_gram=0, loss_l2=0.016, loss_lpips=0.146]  \n",
      "Steps:  16%|█▌        | 1559/10000 [8:25:10<44:37:37, 19.03s/it, loss_gram=0, loss_l2=0.00201, loss_lpips=0.115]\n",
      "Steps:  16%|█▌        | 1560/10000 [8:25:31<45:47:30, 19.53s/it, loss_gram=0, loss_l2=0.0267, loss_lpips=0.167] \n",
      "Steps:  16%|█▌        | 1561/10000 [8:25:48<44:36:49, 19.03s/it, loss_gram=0, loss_l2=0.0098, loss_lpips=0.166]\n",
      "Steps:  16%|█▌        | 1562/10000 [8:26:09<45:28:47, 19.40s/it, loss_gram=0, loss_l2=0.00158, loss_lpips=0.0732]\n",
      "Steps:  16%|█▌        | 1563/10000 [8:26:27<44:34:18, 19.02s/it, loss_gram=0, loss_l2=0.0101, loss_lpips=0.114]  \n",
      "Steps:  16%|█▌        | 1564/10000 [8:26:46<44:59:58, 19.20s/it, loss_gram=0, loss_l2=0.00804, loss_lpips=0.133]\n",
      "Steps:  16%|█▌        | 1565/10000 [8:27:05<44:58:57, 19.20s/it, loss_gram=0, loss_l2=0.00758, loss_lpips=0.0813]\n",
      "Steps:  16%|█▌        | 1566/10000 [8:27:24<44:47:33, 19.12s/it, loss_gram=0, loss_l2=0.0063, loss_lpips=0.102]  \n",
      "Steps:  16%|█▌        | 1567/10000 [8:27:41<43:15:16, 18.47s/it, loss_gram=0, loss_l2=0.0138, loss_lpips=0.0846]\n",
      "Steps:  16%|█▌        | 1568/10000 [8:28:01<43:58:11, 18.77s/it, loss_gram=0, loss_l2=0.0168, loss_lpips=0.172] \n",
      "Steps:  16%|█▌        | 1569/10000 [8:28:20<43:43:46, 18.67s/it, loss_gram=0, loss_l2=0.0011, loss_lpips=0.092]\n",
      "Steps:  16%|█▌        | 1570/10000 [8:28:41<45:59:39, 19.64s/it, loss_gram=0, loss_l2=0.0014, loss_lpips=0.103]\n",
      "Steps:  16%|█▌        | 1571/10000 [8:29:00<44:59:04, 19.21s/it, loss_gram=0, loss_l2=0.00501, loss_lpips=0.106]\n",
      "Steps:  16%|█▌        | 1572/10000 [8:29:20<45:53:24, 19.60s/it, loss_gram=0, loss_l2=0.027, loss_lpips=0.291]  \n",
      "Steps:  16%|█▌        | 1573/10000 [8:29:38<44:47:43, 19.14s/it, loss_gram=0, loss_l2=0.00669, loss_lpips=0.104]\n",
      "Steps:  16%|█▌        | 1574/10000 [8:29:58<44:59:26, 19.22s/it, loss_gram=0, loss_l2=0.00463, loss_lpips=0.1]  \n",
      "Steps:  16%|█▌        | 1575/10000 [8:30:16<44:32:53, 19.04s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.11]\n",
      "Steps:  16%|█▌        | 1576/10000 [8:30:37<45:47:46, 19.57s/it, loss_gram=0, loss_l2=0.00565, loss_lpips=0.0874]\n",
      "Steps:  16%|█▌        | 1577/10000 [8:30:54<44:02:41, 18.82s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.181]  \n",
      "Steps:  16%|█▌        | 1578/10000 [8:31:15<45:19:49, 19.38s/it, loss_gram=0, loss_l2=0.00611, loss_lpips=0.0754]\n",
      "Steps:  16%|█▌        | 1579/10000 [8:31:32<44:18:19, 18.94s/it, loss_gram=0, loss_l2=0.00309, loss_lpips=0.0916]\n",
      "Steps:  16%|█▌        | 1580/10000 [8:31:53<44:58:31, 19.23s/it, loss_gram=0, loss_l2=0.00731, loss_lpips=0.111] \n",
      "Steps:  16%|█▌        | 1581/10000 [8:32:09<43:08:24, 18.45s/it, loss_gram=0, loss_l2=0.0074, loss_lpips=0.125] \n",
      "Steps:  16%|█▌        | 1582/10000 [8:32:29<44:02:17, 18.83s/it, loss_gram=0, loss_l2=0.0295, loss_lpips=0.229]\n",
      "Steps:  16%|█▌        | 1583/10000 [8:32:47<43:19:07, 18.53s/it, loss_gram=0, loss_l2=0.00644, loss_lpips=0.112]\n",
      "Steps:  16%|█▌        | 1584/10000 [8:33:07<45:08:23, 19.31s/it, loss_gram=0, loss_l2=0.00684, loss_lpips=0.0867]\n",
      "Steps:  16%|█▌        | 1585/10000 [8:33:25<43:35:48, 18.65s/it, loss_gram=0, loss_l2=0.00861, loss_lpips=0.0854]\n",
      "Steps:  16%|█▌        | 1586/10000 [8:33:44<43:59:47, 18.82s/it, loss_gram=0, loss_l2=0.00688, loss_lpips=0.0903]\n",
      "Steps:  16%|█▌        | 1587/10000 [8:34:03<44:12:40, 18.92s/it, loss_gram=0, loss_l2=0.00355, loss_lpips=0.186] \n",
      "Steps:  16%|█▌        | 1588/10000 [8:34:22<44:05:33, 18.87s/it, loss_gram=0, loss_l2=0.00768, loss_lpips=0.11] \n",
      "Steps:  16%|█▌        | 1589/10000 [8:34:40<43:17:14, 18.53s/it, loss_gram=0, loss_l2=0.000648, loss_lpips=0.0903]\n",
      "Steps:  16%|█▌        | 1590/10000 [8:34:59<44:10:21, 18.91s/it, loss_gram=0, loss_l2=0.00597, loss_lpips=0.0852] \n",
      "Steps:  16%|█▌        | 1591/10000 [8:35:17<42:58:40, 18.40s/it, loss_gram=0, loss_l2=0.016, loss_lpips=0.127]   \n",
      "Steps:  16%|█▌        | 1592/10000 [8:35:35<43:17:01, 18.53s/it, loss_gram=0, loss_l2=0.0213, loss_lpips=0.13]\n",
      "Steps:  16%|█▌        | 1593/10000 [8:35:53<42:47:04, 18.32s/it, loss_gram=0, loss_l2=0.00492, loss_lpips=0.0664]\n",
      "Steps:  16%|█▌        | 1594/10000 [8:36:12<42:58:22, 18.40s/it, loss_gram=0, loss_l2=0.00948, loss_lpips=0.104] \n",
      "Steps:  16%|█▌        | 1595/10000 [8:36:30<42:42:18, 18.29s/it, loss_gram=0, loss_l2=0.0112, loss_lpips=0.142] \n",
      "Steps:  16%|█▌        | 1596/10000 [8:36:48<42:23:06, 18.16s/it, loss_gram=0, loss_l2=0.0061, loss_lpips=0.101]\n",
      "Steps:  16%|█▌        | 1597/10000 [8:37:06<42:40:22, 18.28s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.118]\n",
      "Steps:  16%|█▌        | 1598/10000 [8:37:27<44:12:10, 18.94s/it, loss_gram=0, loss_l2=0.0122, loss_lpips=0.107]\n",
      "Steps:  16%|█▌        | 1599/10000 [8:37:45<43:59:35, 18.85s/it, loss_gram=0, loss_l2=0.00498, loss_lpips=0.0862]\n",
      "Steps:  16%|█▌        | 1600/10000 [8:38:05<44:14:41, 18.96s/it, loss_gram=0, loss_l2=0.025, loss_lpips=0.247]   \n",
      "Steps:  16%|█▌        | 1601/10000 [8:38:22<43:18:51, 18.57s/it, loss_gram=0, loss_l2=0.0143, loss_lpips=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  16%|█▌        | 1602/10000 [8:38:42<44:07:15, 18.91s/it, loss_gram=0, loss_l2=0.0153, loss_lpips=0.128]\n",
      "Steps:  16%|█▌        | 1603/10000 [8:39:00<43:21:03, 18.59s/it, loss_gram=0, loss_l2=0.00172, loss_lpips=0.153]\n",
      "Steps:  16%|█▌        | 1604/10000 [8:39:19<43:33:25, 18.68s/it, loss_gram=0, loss_l2=0.00107, loss_lpips=0.169]\n",
      "Steps:  16%|█▌        | 1605/10000 [8:39:36<42:25:55, 18.20s/it, loss_gram=0, loss_l2=0.0056, loss_lpips=0.0565]\n",
      "Steps:  16%|█▌        | 1606/10000 [8:39:56<43:46:03, 18.77s/it, loss_gram=0, loss_l2=0.00278, loss_lpips=0.0632]\n",
      "Steps:  16%|█▌        | 1607/10000 [8:40:13<42:41:48, 18.31s/it, loss_gram=0, loss_l2=0.00914, loss_lpips=0.113] \n",
      "Steps:  16%|█▌        | 1608/10000 [8:40:32<42:55:38, 18.41s/it, loss_gram=0, loss_l2=0.016, loss_lpips=0.203]  \n",
      "Steps:  16%|█▌        | 1609/10000 [8:40:49<42:17:48, 18.15s/it, loss_gram=0, loss_l2=0.0021, loss_lpips=0.106]\n",
      "Steps:  16%|█▌        | 1610/10000 [8:41:10<43:22:40, 18.61s/it, loss_gram=0, loss_l2=0.0247, loss_lpips=0.199]\n",
      "Steps:  16%|█▌        | 1611/10000 [8:41:28<43:25:55, 18.64s/it, loss_gram=0, loss_l2=0.00808, loss_lpips=0.0962]\n",
      "Steps:  16%|█▌        | 1612/10000 [8:41:47<43:58:20, 18.87s/it, loss_gram=0, loss_l2=0.00722, loss_lpips=0.0898]\n",
      "Steps:  16%|█▌        | 1613/10000 [8:42:06<43:37:07, 18.72s/it, loss_gram=0, loss_l2=0.00558, loss_lpips=0.166] \n",
      "Steps:  16%|█▌        | 1614/10000 [8:42:25<44:13:49, 18.99s/it, loss_gram=0, loss_l2=0.132, loss_lpips=0.218]  \n",
      "Steps:  16%|█▌        | 1615/10000 [8:42:43<43:05:21, 18.50s/it, loss_gram=0, loss_l2=0.00513, loss_lpips=0.083]\n",
      "Steps:  16%|█▌        | 1616/10000 [8:43:03<44:19:41, 19.03s/it, loss_gram=0, loss_l2=0.0099, loss_lpips=0.2]   \n",
      "Steps:  16%|█▌        | 1617/10000 [8:43:21<44:09:58, 18.97s/it, loss_gram=0, loss_l2=0.00734, loss_lpips=0.103]\n",
      "Steps:  16%|█▌        | 1618/10000 [8:43:41<43:46:04, 18.80s/it, loss_gram=0, loss_l2=0.00154, loss_lpips=0.0989]\n",
      "Steps:  16%|█▌        | 1619/10000 [8:43:59<44:16:44, 19.02s/it, loss_gram=0, loss_l2=0.0038, loss_lpips=0.159]  \n",
      "Steps:  16%|█▌        | 1620/10000 [8:44:19<44:33:31, 19.14s/it, loss_gram=0, loss_l2=0.00421, loss_lpips=0.102]\n",
      "Steps:  16%|█▌        | 1621/10000 [8:44:37<43:42:58, 18.78s/it, loss_gram=0, loss_l2=0.00185, loss_lpips=0.0745]\n",
      "Steps:  16%|█▌        | 1622/10000 [8:44:57<44:23:20, 19.07s/it, loss_gram=0, loss_l2=0.03, loss_lpips=0.21]     \n",
      "Steps:  16%|█▌        | 1623/10000 [8:45:14<43:25:13, 18.66s/it, loss_gram=0, loss_l2=0.0035, loss_lpips=0.0969]\n",
      "Steps:  16%|█▌        | 1624/10000 [8:45:35<44:37:20, 19.18s/it, loss_gram=0, loss_l2=0.00783, loss_lpips=0.127]\n",
      "Steps:  16%|█▋        | 1625/10000 [8:45:52<42:58:16, 18.47s/it, loss_gram=0, loss_l2=0.00785, loss_lpips=0.132]\n",
      "Steps:  16%|█▋        | 1626/10000 [8:46:12<44:02:47, 18.94s/it, loss_gram=0, loss_l2=0.201, loss_lpips=0.247]  \n",
      "Steps:  16%|█▋        | 1627/10000 [8:46:29<43:07:23, 18.54s/it, loss_gram=0, loss_l2=0.00471, loss_lpips=0.0886]\n",
      "Steps:  16%|█▋        | 1628/10000 [8:46:48<43:17:49, 18.62s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.223]  \n",
      "Steps:  16%|█▋        | 1629/10000 [8:47:06<42:57:47, 18.48s/it, loss_gram=0, loss_l2=0.0049, loss_lpips=0.0823]\n",
      "Steps:  16%|█▋        | 1630/10000 [8:47:27<44:19:20, 19.06s/it, loss_gram=0, loss_l2=0.00347, loss_lpips=0.105]\n",
      "Steps:  16%|█▋        | 1631/10000 [8:47:44<43:28:16, 18.70s/it, loss_gram=0, loss_l2=0.00752, loss_lpips=0.0706]\n",
      "Steps:  16%|█▋        | 1632/10000 [8:48:03<43:21:38, 18.65s/it, loss_gram=0, loss_l2=0.0061, loss_lpips=0.0742] \n",
      "Steps:  16%|█▋        | 1633/10000 [8:48:21<42:43:49, 18.39s/it, loss_gram=0, loss_l2=0.0158, loss_lpips=0.1]   \n",
      "Steps:  16%|█▋        | 1634/10000 [8:48:39<42:43:05, 18.38s/it, loss_gram=0, loss_l2=0.0137, loss_lpips=0.179]\n",
      "Steps:  16%|█▋        | 1635/10000 [8:48:57<41:59:24, 18.07s/it, loss_gram=0, loss_l2=0.00778, loss_lpips=0.0696]\n",
      "Steps:  16%|█▋        | 1636/10000 [8:49:16<43:09:38, 18.58s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.114]  \n",
      "Steps:  16%|█▋        | 1637/10000 [8:49:34<42:47:40, 18.42s/it, loss_gram=0, loss_l2=0.00948, loss_lpips=0.078]\n",
      "Steps:  16%|█▋        | 1638/10000 [8:49:55<44:28:14, 19.15s/it, loss_gram=0, loss_l2=0.00635, loss_lpips=0.146]\n",
      "Steps:  16%|█▋        | 1639/10000 [8:50:13<43:30:39, 18.73s/it, loss_gram=0, loss_l2=0.00853, loss_lpips=0.0923]\n",
      "Steps:  16%|█▋        | 1640/10000 [8:50:34<45:01:06, 19.39s/it, loss_gram=0, loss_l2=0.0218, loss_lpips=0.226]  \n",
      "Steps:  16%|█▋        | 1641/10000 [8:50:50<42:55:03, 18.48s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.224]\n",
      "Steps:  16%|█▋        | 1642/10000 [8:51:10<43:25:41, 18.71s/it, loss_gram=0, loss_l2=0.00675, loss_lpips=0.09]\n",
      "Steps:  16%|█▋        | 1643/10000 [8:51:28<42:59:32, 18.52s/it, loss_gram=0, loss_l2=0.00476, loss_lpips=0.0789]\n",
      "Steps:  16%|█▋        | 1644/10000 [8:51:46<43:20:38, 18.67s/it, loss_gram=0, loss_l2=0.0191, loss_lpips=0.143]  \n",
      "Steps:  16%|█▋        | 1645/10000 [8:52:05<43:05:35, 18.57s/it, loss_gram=0, loss_l2=0.0339, loss_lpips=0.138]\n",
      "Steps:  16%|█▋        | 1646/10000 [8:52:25<43:59:52, 18.96s/it, loss_gram=0, loss_l2=0.00266, loss_lpips=0.0764]\n",
      "Steps:  16%|█▋        | 1647/10000 [8:52:43<43:31:43, 18.76s/it, loss_gram=0, loss_l2=0.0155, loss_lpips=0.161]  \n",
      "Steps:  16%|█▋        | 1648/10000 [8:53:02<43:38:35, 18.81s/it, loss_gram=0, loss_l2=0.00975, loss_lpips=0.116]\n",
      "Steps:  16%|█▋        | 1649/10000 [8:53:20<42:59:55, 18.54s/it, loss_gram=0, loss_l2=0.00702, loss_lpips=0.0746]\n",
      "Steps:  16%|█▋        | 1650/10000 [8:53:39<43:32:54, 18.78s/it, loss_gram=0, loss_l2=0.00465, loss_lpips=0.0881]\n",
      "Steps:  17%|█▋        | 1651/10000 [8:53:57<42:51:24, 18.48s/it, loss_gram=0, loss_l2=0.0142, loss_lpips=0.0916] \n",
      "Steps:  17%|█▋        | 1652/10000 [8:54:15<42:57:57, 18.53s/it, loss_gram=0, loss_l2=0.0141, loss_lpips=0.0865]\n",
      "Steps:  17%|█▋        | 1653/10000 [8:54:34<42:44:34, 18.43s/it, loss_gram=0, loss_l2=0.00419, loss_lpips=0.0858]\n",
      "Steps:  17%|█▋        | 1654/10000 [8:54:54<43:33:22, 18.79s/it, loss_gram=0, loss_l2=0.00778, loss_lpips=0.0918]\n",
      "Steps:  17%|█▋        | 1655/10000 [8:55:11<42:58:16, 18.54s/it, loss_gram=0, loss_l2=0.0108, loss_lpips=0.075]  \n",
      "Steps:  17%|█▋        | 1656/10000 [8:55:32<44:40:33, 19.28s/it, loss_gram=0, loss_l2=0.0562, loss_lpips=0.215]\n",
      "Steps:  17%|█▋        | 1657/10000 [8:55:49<42:58:54, 18.55s/it, loss_gram=0, loss_l2=0.00275, loss_lpips=0.082]\n",
      "Steps:  17%|█▋        | 1658/10000 [8:56:08<43:16:04, 18.67s/it, loss_gram=0, loss_l2=0.0643, loss_lpips=0.206] \n",
      "Steps:  17%|█▋        | 1659/10000 [8:56:26<42:34:36, 18.38s/it, loss_gram=0, loss_l2=0.0481, loss_lpips=0.187]\n",
      "Steps:  17%|█▋        | 1660/10000 [8:56:46<43:30:33, 18.78s/it, loss_gram=0, loss_l2=0.00461, loss_lpips=0.13]\n",
      "Steps:  17%|█▋        | 1661/10000 [8:57:03<42:49:04, 18.48s/it, loss_gram=0, loss_l2=0.0117, loss_lpips=0.11] \n",
      "Steps:  17%|█▋        | 1662/10000 [8:57:23<43:31:41, 18.79s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.183]\n",
      "Steps:  17%|█▋        | 1663/10000 [8:57:41<43:00:34, 18.57s/it, loss_gram=0, loss_l2=0.0174, loss_lpips=0.143]\n",
      "Steps:  17%|█▋        | 1664/10000 [8:58:02<44:21:05, 19.15s/it, loss_gram=0, loss_l2=0.023, loss_lpips=0.111] \n",
      "Steps:  17%|█▋        | 1665/10000 [8:58:19<43:21:50, 18.73s/it, loss_gram=0, loss_l2=0.0058, loss_lpips=0.085]\n",
      "Steps:  17%|█▋        | 1666/10000 [8:58:38<43:46:26, 18.91s/it, loss_gram=0, loss_l2=0.0177, loss_lpips=0.123]\n",
      "Steps:  17%|█▋        | 1667/10000 [8:58:56<42:35:05, 18.40s/it, loss_gram=0, loss_l2=0.0189, loss_lpips=0.155]\n",
      "Steps:  17%|█▋        | 1668/10000 [8:59:15<43:27:09, 18.77s/it, loss_gram=0, loss_l2=0.000896, loss_lpips=0.116]\n",
      "Steps:  17%|█▋        | 1669/10000 [8:59:34<43:00:15, 18.58s/it, loss_gram=0, loss_l2=0.0152, loss_lpips=0.176]  \n",
      "Steps:  17%|█▋        | 1670/10000 [8:59:53<43:31:15, 18.81s/it, loss_gram=0, loss_l2=0.0038, loss_lpips=0.117]\n",
      "Steps:  17%|█▋        | 1671/10000 [9:00:11<42:59:47, 18.58s/it, loss_gram=0, loss_l2=0.0596, loss_lpips=0.275]\n",
      "Steps:  17%|█▋        | 1672/10000 [9:00:31<43:58:34, 19.01s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.165]\n",
      "Steps:  17%|█▋        | 1673/10000 [9:00:50<43:31:57, 18.82s/it, loss_gram=0, loss_l2=0.0167, loss_lpips=0.126]\n",
      "Steps:  17%|█▋        | 1674/10000 [9:01:09<43:42:58, 18.90s/it, loss_gram=0, loss_l2=0.00871, loss_lpips=0.0977]\n",
      "Steps:  17%|█▋        | 1675/10000 [9:01:26<42:48:51, 18.51s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.129]  \n",
      "Steps:  17%|█▋        | 1676/10000 [9:01:46<43:43:08, 18.91s/it, loss_gram=0, loss_l2=0.00348, loss_lpips=0.0935]\n",
      "Steps:  17%|█▋        | 1677/10000 [9:02:04<43:11:45, 18.68s/it, loss_gram=0, loss_l2=0.00743, loss_lpips=0.0868]\n",
      "Steps:  17%|█▋        | 1678/10000 [9:02:23<43:42:06, 18.90s/it, loss_gram=0, loss_l2=0.00449, loss_lpips=0.0948]\n",
      "Steps:  17%|█▋        | 1679/10000 [9:02:43<43:48:43, 18.95s/it, loss_gram=0, loss_l2=0.0285, loss_lpips=0.173]  \n",
      "Steps:  17%|█▋        | 1680/10000 [9:03:03<45:12:49, 19.56s/it, loss_gram=0, loss_l2=0.00872, loss_lpips=0.182]\n",
      "Steps:  17%|█▋        | 1681/10000 [9:03:23<44:45:50, 19.37s/it, loss_gram=0, loss_l2=0.00438, loss_lpips=0.122]\n",
      "Steps:  17%|█▋        | 1682/10000 [9:03:44<46:06:19, 19.95s/it, loss_gram=0, loss_l2=0.0039, loss_lpips=0.11]  \n",
      "Steps:  17%|█▋        | 1683/10000 [9:04:04<46:34:16, 20.16s/it, loss_gram=0, loss_l2=0.00501, loss_lpips=0.121]\n",
      "Steps:  17%|█▋        | 1684/10000 [9:04:27<48:01:03, 20.79s/it, loss_gram=0, loss_l2=0.0258, loss_lpips=0.176] \n",
      "Steps:  17%|█▋        | 1685/10000 [9:04:46<47:14:32, 20.45s/it, loss_gram=0, loss_l2=0.0218, loss_lpips=0.137]\n",
      "Steps:  17%|█▋        | 1686/10000 [9:05:08<48:17:52, 20.91s/it, loss_gram=0, loss_l2=0.0431, loss_lpips=0.191]\n",
      "Steps:  17%|█▋        | 1687/10000 [9:05:27<46:46:22, 20.26s/it, loss_gram=0, loss_l2=0.00805, loss_lpips=0.113]\n",
      "Steps:  17%|█▋        | 1688/10000 [9:05:50<48:26:38, 20.98s/it, loss_gram=0, loss_l2=0.00332, loss_lpips=0.108]\n",
      "Steps:  17%|█▋        | 1689/10000 [9:06:09<47:29:45, 20.57s/it, loss_gram=0, loss_l2=0.0109, loss_lpips=0.0815]\n",
      "Steps:  17%|█▋        | 1690/10000 [9:06:31<48:31:45, 21.02s/it, loss_gram=0, loss_l2=0.0148, loss_lpips=0.17]  \n",
      "Steps:  17%|█▋        | 1691/10000 [9:06:51<47:45:51, 20.69s/it, loss_gram=0, loss_l2=0.0259, loss_lpips=0.248]\n",
      "Steps:  17%|█▋        | 1692/10000 [9:07:15<49:33:08, 21.47s/it, loss_gram=0, loss_l2=0.0041, loss_lpips=0.156]\n",
      "Steps:  17%|█▋        | 1693/10000 [9:07:36<49:26:07, 21.42s/it, loss_gram=0, loss_l2=0.00655, loss_lpips=0.12]\n",
      "Steps:  17%|█▋        | 1694/10000 [9:07:57<49:10:26, 21.31s/it, loss_gram=0, loss_l2=0.0292, loss_lpips=0.143]\n",
      "Steps:  17%|█▋        | 1695/10000 [9:08:16<47:32:46, 20.61s/it, loss_gram=0, loss_l2=0.00703, loss_lpips=0.137]\n",
      "Steps:  17%|█▋        | 1696/10000 [9:08:40<49:58:32, 21.67s/it, loss_gram=0, loss_l2=0.00893, loss_lpips=0.139]\n",
      "Steps:  17%|█▋        | 1697/10000 [9:09:01<49:15:10, 21.35s/it, loss_gram=0, loss_l2=0.0151, loss_lpips=0.128] \n",
      "Steps:  17%|█▋        | 1698/10000 [9:09:24<50:53:38, 22.07s/it, loss_gram=0, loss_l2=0.0184, loss_lpips=0.181]\n",
      "Steps:  17%|█▋        | 1699/10000 [9:09:44<48:59:32, 21.25s/it, loss_gram=0, loss_l2=0.00174, loss_lpips=0.153]\n",
      "Steps:  17%|█▋        | 1700/10000 [9:10:06<50:05:44, 21.73s/it, loss_gram=0, loss_l2=0.00411, loss_lpips=0.0703]\n",
      "Steps:  17%|█▋        | 1701/10000 [9:10:27<48:59:00, 21.25s/it, loss_gram=0, loss_l2=0.0179, loss_lpips=0.141]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  17%|█▋        | 1702/10000 [9:10:50<50:32:00, 21.92s/it, loss_gram=0, loss_l2=0.00307, loss_lpips=0.0676]\n",
      "Steps:  17%|█▋        | 1703/10000 [9:11:09<48:26:45, 21.02s/it, loss_gram=0, loss_l2=0.0326, loss_lpips=0.118]  \n",
      "Steps:  17%|█▋        | 1704/10000 [9:11:33<50:21:40, 21.85s/it, loss_gram=0, loss_l2=0.00132, loss_lpips=0.148]\n",
      "Steps:  17%|█▋        | 1705/10000 [9:11:52<48:33:10, 21.07s/it, loss_gram=0, loss_l2=0.00346, loss_lpips=0.0977]\n",
      "Steps:  17%|█▋        | 1706/10000 [9:12:12<47:36:02, 20.66s/it, loss_gram=0, loss_l2=0.000924, loss_lpips=0.0972]\n",
      "Steps:  17%|█▋        | 1707/10000 [9:12:28<44:51:42, 19.47s/it, loss_gram=0, loss_l2=0.00329, loss_lpips=0.152]  \n",
      "Steps:  17%|█▋        | 1708/10000 [9:12:51<47:02:08, 20.42s/it, loss_gram=0, loss_l2=0.0064, loss_lpips=0.109] \n",
      "Steps:  17%|█▋        | 1709/10000 [9:13:10<45:51:15, 19.91s/it, loss_gram=0, loss_l2=0.0136, loss_lpips=0.0836]\n",
      "Steps:  17%|█▋        | 1710/10000 [9:13:30<46:03:02, 20.00s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.0895]\n",
      "Steps:  17%|█▋        | 1711/10000 [9:13:50<45:53:20, 19.93s/it, loss_gram=0, loss_l2=0.00702, loss_lpips=0.0964]\n",
      "Steps:  17%|█▋        | 1712/10000 [9:14:12<47:34:01, 20.66s/it, loss_gram=0, loss_l2=0.00532, loss_lpips=0.0818]\n",
      "Steps:  17%|█▋        | 1713/10000 [9:14:33<47:15:11, 20.53s/it, loss_gram=0, loss_l2=0.0042, loss_lpips=0.0746] \n",
      "Steps:  17%|█▋        | 1714/10000 [9:14:54<47:56:51, 20.83s/it, loss_gram=0, loss_l2=0.0485, loss_lpips=0.236] \n",
      "Steps:  17%|█▋        | 1715/10000 [9:15:13<46:46:05, 20.32s/it, loss_gram=0, loss_l2=0.0144, loss_lpips=0.114]\n",
      "Steps:  17%|█▋        | 1716/10000 [9:15:35<47:50:09, 20.79s/it, loss_gram=0, loss_l2=0.00727, loss_lpips=0.0903]\n",
      "Steps:  17%|█▋        | 1717/10000 [9:15:56<47:29:06, 20.64s/it, loss_gram=0, loss_l2=0.00367, loss_lpips=0.105] \n",
      "Steps:  17%|█▋        | 1718/10000 [9:16:19<49:48:10, 21.65s/it, loss_gram=0, loss_l2=0.00819, loss_lpips=0.0696]\n",
      "Steps:  17%|█▋        | 1719/10000 [9:16:37<47:17:20, 20.56s/it, loss_gram=0, loss_l2=0.0333, loss_lpips=0.17]   \n",
      "Steps:  17%|█▋        | 1720/10000 [9:16:59<47:39:24, 20.72s/it, loss_gram=0, loss_l2=0.0111, loss_lpips=0.109]\n",
      "Steps:  17%|█▋        | 1721/10000 [9:17:17<46:30:25, 20.22s/it, loss_gram=0, loss_l2=0.0112, loss_lpips=0.102]\n",
      "Steps:  17%|█▋        | 1722/10000 [9:17:40<48:17:32, 21.00s/it, loss_gram=0, loss_l2=0.00733, loss_lpips=0.0814]\n",
      "Steps:  17%|█▋        | 1723/10000 [9:17:59<46:52:36, 20.39s/it, loss_gram=0, loss_l2=0.00357, loss_lpips=0.133] \n",
      "Steps:  17%|█▋        | 1724/10000 [9:18:21<48:07:44, 20.94s/it, loss_gram=0, loss_l2=0.00697, loss_lpips=0.103]\n",
      "Steps:  17%|█▋        | 1725/10000 [9:18:40<46:26:47, 20.21s/it, loss_gram=0, loss_l2=0.00302, loss_lpips=0.0864]\n",
      "Steps:  17%|█▋        | 1726/10000 [9:19:02<47:34:54, 20.70s/it, loss_gram=0, loss_l2=0.00533, loss_lpips=0.125] \n",
      "Steps:  17%|█▋        | 1727/10000 [9:19:21<46:33:41, 20.26s/it, loss_gram=0, loss_l2=0.0173, loss_lpips=0.232] \n",
      "Steps:  17%|█▋        | 1728/10000 [9:19:44<48:15:29, 21.00s/it, loss_gram=0, loss_l2=0.00608, loss_lpips=0.104]\n",
      "Steps:  17%|█▋        | 1729/10000 [9:20:04<47:28:21, 20.66s/it, loss_gram=0, loss_l2=0.00455, loss_lpips=0.0908]\n",
      "Steps:  17%|█▋        | 1730/10000 [9:20:26<48:15:37, 21.01s/it, loss_gram=0, loss_l2=0.0149, loss_lpips=0.141]  \n",
      "Steps:  17%|█▋        | 1731/10000 [9:20:45<47:14:32, 20.57s/it, loss_gram=0, loss_l2=0.00545, loss_lpips=0.128]\n",
      "Steps:  17%|█▋        | 1732/10000 [9:21:08<48:36:44, 21.17s/it, loss_gram=0, loss_l2=0.00552, loss_lpips=0.0862]\n",
      "Steps:  17%|█▋        | 1733/10000 [9:21:26<46:36:24, 20.30s/it, loss_gram=0, loss_l2=0.026, loss_lpips=0.105]   \n",
      "Steps:  17%|█▋        | 1734/10000 [9:21:49<48:30:58, 21.13s/it, loss_gram=0, loss_l2=0.00747, loss_lpips=0.0729]\n",
      "Steps:  17%|█▋        | 1735/10000 [9:22:09<47:45:48, 20.80s/it, loss_gram=0, loss_l2=0.01, loss_lpips=0.069]    \n",
      "Steps:  17%|█▋        | 1736/10000 [9:22:30<48:12:45, 21.00s/it, loss_gram=0, loss_l2=0.00428, loss_lpips=0.078]\n",
      "Steps:  17%|█▋        | 1737/10000 [9:22:50<47:00:32, 20.48s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.0981]\n",
      "Steps:  17%|█▋        | 1738/10000 [9:23:11<47:28:25, 20.69s/it, loss_gram=0, loss_l2=0.00761, loss_lpips=0.0657]\n",
      "Steps:  17%|█▋        | 1739/10000 [9:23:30<46:35:53, 20.31s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.075]  \n",
      "Steps:  17%|█▋        | 1740/10000 [9:23:53<48:13:59, 21.02s/it, loss_gram=0, loss_l2=0.0082, loss_lpips=0.12] \n",
      "Steps:  17%|█▋        | 1741/10000 [9:24:12<47:00:36, 20.49s/it, loss_gram=0, loss_l2=0.00657, loss_lpips=0.0969]\n",
      "Steps:  17%|█▋        | 1742/10000 [9:24:34<48:12:26, 21.02s/it, loss_gram=0, loss_l2=0.00109, loss_lpips=0.0906]\n",
      "Steps:  17%|█▋        | 1743/10000 [9:24:55<47:32:44, 20.73s/it, loss_gram=0, loss_l2=0.0113, loss_lpips=0.107]  \n",
      "Steps:  17%|█▋        | 1744/10000 [9:25:16<48:02:13, 20.95s/it, loss_gram=0, loss_l2=0.00805, loss_lpips=0.187]\n",
      "Steps:  17%|█▋        | 1745/10000 [9:25:36<47:33:47, 20.74s/it, loss_gram=0, loss_l2=0.00462, loss_lpips=0.0887]\n",
      "Steps:  17%|█▋        | 1746/10000 [9:25:57<47:46:16, 20.84s/it, loss_gram=0, loss_l2=0.00126, loss_lpips=0.147] \n",
      "Steps:  17%|█▋        | 1747/10000 [9:26:17<46:53:01, 20.45s/it, loss_gram=0, loss_l2=0.00794, loss_lpips=0.175]\n",
      "Steps:  17%|█▋        | 1748/10000 [9:26:38<47:15:21, 20.62s/it, loss_gram=0, loss_l2=0.0035, loss_lpips=0.0896]\n",
      "Steps:  17%|█▋        | 1749/10000 [9:26:59<47:16:30, 20.63s/it, loss_gram=0, loss_l2=0.00519, loss_lpips=0.131]\n",
      "Steps:  18%|█▊        | 1750/10000 [9:27:20<47:59:55, 20.94s/it, loss_gram=0, loss_l2=0.00713, loss_lpips=0.102]\n",
      "Steps:  18%|█▊        | 1751/10000 [9:27:40<47:26:07, 20.70s/it, loss_gram=0, loss_l2=0.00847, loss_lpips=0.082]\n",
      "Steps:  18%|█▊        | 1752/10000 [9:28:02<47:57:31, 20.93s/it, loss_gram=0, loss_l2=0.00666, loss_lpips=0.0962]\n",
      "Steps:  18%|█▊        | 1753/10000 [9:28:21<46:40:04, 20.37s/it, loss_gram=0, loss_l2=0.00585, loss_lpips=0.12]  \n",
      "Steps:  18%|█▊        | 1754/10000 [9:28:43<47:42:00, 20.82s/it, loss_gram=0, loss_l2=0.00641, loss_lpips=0.114]\n",
      "Steps:  18%|█▊        | 1755/10000 [9:29:03<46:58:32, 20.51s/it, loss_gram=0, loss_l2=0.00591, loss_lpips=0.122]\n",
      "Steps:  18%|█▊        | 1756/10000 [9:29:26<48:34:53, 21.21s/it, loss_gram=0, loss_l2=0.00674, loss_lpips=0.0931]\n",
      "Steps:  18%|█▊        | 1757/10000 [9:29:44<47:04:35, 20.56s/it, loss_gram=0, loss_l2=0.00603, loss_lpips=0.0952]\n",
      "Steps:  18%|█▊        | 1758/10000 [9:30:08<48:56:01, 21.37s/it, loss_gram=0, loss_l2=0.00929, loss_lpips=0.104] \n",
      "Steps:  18%|█▊        | 1759/10000 [9:30:27<47:44:23, 20.85s/it, loss_gram=0, loss_l2=0.0107, loss_lpips=0.104] \n",
      "Steps:  18%|█▊        | 1760/10000 [9:30:50<48:32:17, 21.21s/it, loss_gram=0, loss_l2=0.00764, loss_lpips=0.108]\n",
      "Steps:  18%|█▊        | 1761/10000 [9:31:11<48:42:57, 21.29s/it, loss_gram=0, loss_l2=0.0138, loss_lpips=0.134] \n",
      "Steps:  18%|█▊        | 1762/10000 [9:31:32<48:27:31, 21.18s/it, loss_gram=0, loss_l2=0.0123, loss_lpips=0.103]\n",
      "Steps:  18%|█▊        | 1763/10000 [9:31:52<47:53:52, 20.93s/it, loss_gram=0, loss_l2=0.00156, loss_lpips=0.0995]\n",
      "Steps:  18%|█▊        | 1764/10000 [9:32:14<48:36:01, 21.24s/it, loss_gram=0, loss_l2=0.00266, loss_lpips=0.09]  \n",
      "Steps:  18%|█▊        | 1765/10000 [9:32:32<46:41:27, 20.41s/it, loss_gram=0, loss_l2=0.00799, loss_lpips=0.0933]\n",
      "Steps:  18%|█▊        | 1766/10000 [9:32:55<47:56:04, 20.96s/it, loss_gram=0, loss_l2=0.00793, loss_lpips=0.0967]\n",
      "Steps:  18%|█▊        | 1767/10000 [9:33:13<46:07:51, 20.17s/it, loss_gram=0, loss_l2=0.00716, loss_lpips=0.144] \n",
      "Steps:  18%|█▊        | 1768/10000 [9:33:33<45:51:44, 20.06s/it, loss_gram=0, loss_l2=0.0153, loss_lpips=0.107] \n",
      "Steps:  18%|█▊        | 1769/10000 [9:33:53<46:01:34, 20.13s/it, loss_gram=0, loss_l2=0.00969, loss_lpips=0.0856]\n",
      "Steps:  18%|█▊        | 1770/10000 [9:34:14<46:37:32, 20.40s/it, loss_gram=0, loss_l2=0.0034, loss_lpips=0.0752] \n",
      "Steps:  18%|█▊        | 1771/10000 [9:34:33<45:24:36, 19.87s/it, loss_gram=0, loss_l2=0.00131, loss_lpips=0.178]\n",
      "Steps:  18%|█▊        | 1772/10000 [9:34:54<46:16:33, 20.25s/it, loss_gram=0, loss_l2=0.0104, loss_lpips=0.129] \n",
      "Steps:  18%|█▊        | 1773/10000 [9:35:14<45:46:01, 20.03s/it, loss_gram=0, loss_l2=0.00587, loss_lpips=0.0916]\n",
      "Steps:  18%|█▊        | 1774/10000 [9:35:37<48:06:51, 21.06s/it, loss_gram=0, loss_l2=0.00637, loss_lpips=0.0665]\n",
      "Steps:  18%|█▊        | 1775/10000 [9:35:56<46:27:00, 20.33s/it, loss_gram=0, loss_l2=0.0144, loss_lpips=0.102]  \n",
      "Steps:  18%|█▊        | 1776/10000 [9:36:17<47:06:29, 20.62s/it, loss_gram=0, loss_l2=0.0256, loss_lpips=0.159]\n",
      "Steps:  18%|█▊        | 1777/10000 [9:36:35<45:16:51, 19.82s/it, loss_gram=0, loss_l2=0.00351, loss_lpips=0.126]\n",
      "Steps:  18%|█▊        | 1778/10000 [9:36:57<46:39:57, 20.43s/it, loss_gram=0, loss_l2=0.00826, loss_lpips=0.0996]\n",
      "Steps:  18%|█▊        | 1779/10000 [9:37:14<44:44:18, 19.59s/it, loss_gram=0, loss_l2=0.00483, loss_lpips=0.0934]\n",
      "Steps:  18%|█▊        | 1780/10000 [9:37:33<44:22:27, 19.43s/it, loss_gram=0, loss_l2=0.035, loss_lpips=0.144]   \n",
      "Steps:  18%|█▊        | 1781/10000 [9:37:52<43:32:56, 19.07s/it, loss_gram=0, loss_l2=0.00976, loss_lpips=0.115]\n",
      "Steps:  18%|█▊        | 1782/10000 [9:38:13<44:54:40, 19.67s/it, loss_gram=0, loss_l2=0.0487, loss_lpips=0.215] \n",
      "Steps:  18%|█▊        | 1783/10000 [9:38:31<43:59:19, 19.27s/it, loss_gram=0, loss_l2=0.00722, loss_lpips=0.092]\n",
      "Steps:  18%|█▊        | 1784/10000 [9:38:53<46:07:44, 20.21s/it, loss_gram=0, loss_l2=0.00123, loss_lpips=0.118]\n",
      "Steps:  18%|█▊        | 1785/10000 [9:39:12<44:58:42, 19.71s/it, loss_gram=0, loss_l2=0.00486, loss_lpips=0.13] \n",
      "Steps:  18%|█▊        | 1786/10000 [9:39:32<45:10:08, 19.80s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.102]\n",
      "Steps:  18%|█▊        | 1787/10000 [9:39:52<44:56:05, 19.70s/it, loss_gram=0, loss_l2=0.00837, loss_lpips=0.0729]\n",
      "Steps:  18%|█▊        | 1788/10000 [9:40:13<46:03:45, 20.19s/it, loss_gram=0, loss_l2=0.00678, loss_lpips=0.115] \n",
      "Steps:  18%|█▊        | 1789/10000 [9:40:32<45:12:10, 19.82s/it, loss_gram=0, loss_l2=0.005, loss_lpips=0.153]  \n",
      "Steps:  18%|█▊        | 1790/10000 [9:40:53<46:08:01, 20.23s/it, loss_gram=0, loss_l2=0.00628, loss_lpips=0.0839]\n",
      "Steps:  18%|█▊        | 1791/10000 [9:41:14<46:19:34, 20.32s/it, loss_gram=0, loss_l2=0.00198, loss_lpips=0.0726]\n",
      "Steps:  18%|█▊        | 1792/10000 [9:41:35<47:07:40, 20.67s/it, loss_gram=0, loss_l2=0.011, loss_lpips=0.137]   \n",
      "Steps:  18%|█▊        | 1793/10000 [9:41:53<45:20:26, 19.89s/it, loss_gram=0, loss_l2=0.00192, loss_lpips=0.113]\n",
      "Steps:  18%|█▊        | 1794/10000 [9:42:14<45:52:48, 20.13s/it, loss_gram=0, loss_l2=0.00442, loss_lpips=0.09] \n",
      "Steps:  18%|█▊        | 1795/10000 [9:42:32<44:16:36, 19.43s/it, loss_gram=0, loss_l2=0.00838, loss_lpips=0.17]\n",
      "Steps:  18%|█▊        | 1796/10000 [9:42:52<45:09:19, 19.81s/it, loss_gram=0, loss_l2=0.00958, loss_lpips=0.1] \n",
      "Steps:  18%|█▊        | 1797/10000 [9:43:10<43:59:43, 19.31s/it, loss_gram=0, loss_l2=0.0136, loss_lpips=0.112]\n",
      "Steps:  18%|█▊        | 1798/10000 [9:43:31<44:35:34, 19.57s/it, loss_gram=0, loss_l2=0.0163, loss_lpips=0.121]\n",
      "Steps:  18%|█▊        | 1799/10000 [9:43:50<44:31:32, 19.55s/it, loss_gram=0, loss_l2=0.00415, loss_lpips=0.114]\n",
      "Steps:  18%|█▊        | 1800/10000 [9:44:10<44:43:42, 19.64s/it, loss_gram=0, loss_l2=0.00846, loss_lpips=0.113]\n",
      "Steps:  18%|█▊        | 1801/10000 [9:44:28<43:51:45, 19.26s/it, loss_gram=0, loss_l2=0.00103, loss_lpips=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  18%|█▊        | 1802/10000 [9:44:50<45:46:24, 20.10s/it, loss_gram=0, loss_l2=0.0126, loss_lpips=0.0818]\n",
      "Steps:  18%|█▊        | 1803/10000 [9:45:08<44:20:07, 19.47s/it, loss_gram=0, loss_l2=0.00188, loss_lpips=0.127]\n",
      "Steps:  18%|█▊        | 1804/10000 [9:45:31<46:05:19, 20.24s/it, loss_gram=0, loss_l2=0.00832, loss_lpips=0.119]\n",
      "Steps:  18%|█▊        | 1805/10000 [9:45:51<46:22:12, 20.37s/it, loss_gram=0, loss_l2=0.0044, loss_lpips=0.0984]\n",
      "Steps:  18%|█▊        | 1806/10000 [9:46:14<47:39:58, 20.94s/it, loss_gram=0, loss_l2=0.00435, loss_lpips=0.0951]\n",
      "Steps:  18%|█▊        | 1807/10000 [9:46:31<45:31:15, 20.00s/it, loss_gram=0, loss_l2=0.0109, loss_lpips=0.13]   \n",
      "Steps:  18%|█▊        | 1808/10000 [9:46:53<46:46:23, 20.55s/it, loss_gram=0, loss_l2=0.0166, loss_lpips=0.114]\n",
      "Steps:  18%|█▊        | 1809/10000 [9:47:12<45:39:03, 20.06s/it, loss_gram=0, loss_l2=0.00587, loss_lpips=0.136]\n",
      "Steps:  18%|█▊        | 1810/10000 [9:47:32<45:39:39, 20.07s/it, loss_gram=0, loss_l2=0.0179, loss_lpips=0.181] \n",
      "Steps:  18%|█▊        | 1811/10000 [9:47:50<44:03:54, 19.37s/it, loss_gram=0, loss_l2=0.0144, loss_lpips=0.133]\n",
      "Steps:  18%|█▊        | 1812/10000 [9:48:14<47:21:42, 20.82s/it, loss_gram=0, loss_l2=0.00212, loss_lpips=0.1] \n",
      "Steps:  18%|█▊        | 1813/10000 [9:48:33<45:41:28, 20.09s/it, loss_gram=0, loss_l2=0.00554, loss_lpips=0.0813]\n",
      "Steps:  18%|█▊        | 1814/10000 [9:48:52<45:42:16, 20.10s/it, loss_gram=0, loss_l2=0.00349, loss_lpips=0.187] \n",
      "Steps:  18%|█▊        | 1815/10000 [9:49:11<44:53:49, 19.75s/it, loss_gram=0, loss_l2=0.0161, loss_lpips=0.159] \n",
      "Steps:  18%|█▊        | 1816/10000 [9:49:32<45:08:51, 19.86s/it, loss_gram=0, loss_l2=0.0133, loss_lpips=0.237]\n",
      "Steps:  18%|█▊        | 1817/10000 [9:49:51<44:49:39, 19.72s/it, loss_gram=0, loss_l2=0.0125, loss_lpips=0.113]\n",
      "Steps:  18%|█▊        | 1818/10000 [9:50:11<45:23:08, 19.97s/it, loss_gram=0, loss_l2=0.0127, loss_lpips=0.124]\n",
      "Steps:  18%|█▊        | 1819/10000 [9:50:31<45:14:17, 19.91s/it, loss_gram=0, loss_l2=0.00369, loss_lpips=0.0776]\n",
      "Steps:  18%|█▊        | 1820/10000 [9:50:53<46:32:29, 20.48s/it, loss_gram=0, loss_l2=0.00957, loss_lpips=0.0966]\n",
      "Steps:  18%|█▊        | 1821/10000 [9:51:13<46:05:57, 20.29s/it, loss_gram=0, loss_l2=0.00239, loss_lpips=0.105] \n",
      "Steps:  18%|█▊        | 1822/10000 [9:51:34<46:29:44, 20.47s/it, loss_gram=0, loss_l2=0.0185, loss_lpips=0.112] \n",
      "Steps:  18%|█▊        | 1823/10000 [9:51:53<45:28:32, 20.02s/it, loss_gram=0, loss_l2=0.00417, loss_lpips=0.0745]\n",
      "Steps:  18%|█▊        | 1824/10000 [9:52:12<45:12:42, 19.91s/it, loss_gram=0, loss_l2=0.00454, loss_lpips=0.17]  \n",
      "Steps:  18%|█▊        | 1825/10000 [9:52:31<44:02:36, 19.40s/it, loss_gram=0, loss_l2=0.00437, loss_lpips=0.114]\n",
      "Steps:  18%|█▊        | 1826/10000 [9:52:50<44:03:35, 19.40s/it, loss_gram=0, loss_l2=0.00677, loss_lpips=0.15] \n",
      "Steps:  18%|█▊        | 1827/10000 [9:53:08<43:05:19, 18.98s/it, loss_gram=0, loss_l2=0.0291, loss_lpips=0.123]\n",
      "Steps:  18%|█▊        | 1828/10000 [9:53:30<45:19:03, 19.96s/it, loss_gram=0, loss_l2=0.00941, loss_lpips=0.12]\n",
      "Steps:  18%|█▊        | 1829/10000 [9:53:48<43:50:36, 19.32s/it, loss_gram=0, loss_l2=0.00811, loss_lpips=0.0825]\n",
      "Steps:  18%|█▊        | 1830/10000 [9:54:08<44:09:09, 19.46s/it, loss_gram=0, loss_l2=0.018, loss_lpips=0.103]   \n",
      "Steps:  18%|█▊        | 1831/10000 [9:54:27<44:04:07, 19.42s/it, loss_gram=0, loss_l2=0.00417, loss_lpips=0.085]\n",
      "Steps:  18%|█▊        | 1832/10000 [9:54:49<45:40:02, 20.13s/it, loss_gram=0, loss_l2=0.00181, loss_lpips=0.0887]\n",
      "Steps:  18%|█▊        | 1833/10000 [9:55:07<44:06:04, 19.44s/it, loss_gram=0, loss_l2=0.0456, loss_lpips=0.225]  \n",
      "Steps:  18%|█▊        | 1834/10000 [9:55:28<44:58:38, 19.83s/it, loss_gram=0, loss_l2=0.0217, loss_lpips=0.148]\n",
      "Steps:  18%|█▊        | 1835/10000 [9:55:46<43:55:15, 19.37s/it, loss_gram=0, loss_l2=0.00775, loss_lpips=0.0999]\n",
      "Steps:  18%|█▊        | 1836/10000 [9:56:06<44:30:53, 19.63s/it, loss_gram=0, loss_l2=0.00971, loss_lpips=0.0999]\n",
      "Steps:  18%|█▊        | 1837/10000 [9:56:23<43:05:37, 19.00s/it, loss_gram=0, loss_l2=0.00912, loss_lpips=0.0941]\n",
      "Steps:  18%|█▊        | 1838/10000 [9:56:44<43:47:37, 19.32s/it, loss_gram=0, loss_l2=0.00655, loss_lpips=0.152] \n",
      "Steps:  18%|█▊        | 1839/10000 [9:57:04<44:25:17, 19.60s/it, loss_gram=0, loss_l2=0.0137, loss_lpips=0.094] \n",
      "Steps:  18%|█▊        | 1840/10000 [9:57:24<44:41:43, 19.72s/it, loss_gram=0, loss_l2=0.0132, loss_lpips=0.12] \n",
      "Steps:  18%|█▊        | 1841/10000 [9:57:42<43:29:25, 19.19s/it, loss_gram=0, loss_l2=0.00419, loss_lpips=0.118]\n",
      "Steps:  18%|█▊        | 1842/10000 [9:58:02<43:48:12, 19.33s/it, loss_gram=0, loss_l2=0.0151, loss_lpips=0.127] \n",
      "Steps:  18%|█▊        | 1843/10000 [9:58:20<43:37:11, 19.25s/it, loss_gram=0, loss_l2=0.00822, loss_lpips=0.105]\n",
      "Steps:  18%|█▊        | 1844/10000 [9:58:43<45:49:16, 20.23s/it, loss_gram=0, loss_l2=0.0119, loss_lpips=0.127] \n",
      "Steps:  18%|█▊        | 1845/10000 [9:59:02<44:53:22, 19.82s/it, loss_gram=0, loss_l2=0.00547, loss_lpips=0.0868]\n",
      "Steps:  18%|█▊        | 1846/10000 [9:59:21<44:47:11, 19.77s/it, loss_gram=0, loss_l2=0.0038, loss_lpips=0.0823] \n",
      "Steps:  18%|█▊        | 1847/10000 [9:59:40<43:49:50, 19.35s/it, loss_gram=0, loss_l2=0.00125, loss_lpips=0.164]\n",
      "Steps:  18%|█▊        | 1848/10000 [10:00:01<45:17:43, 20.00s/it, loss_gram=0, loss_l2=0.00921, loss_lpips=0.107]\n",
      "Steps:  18%|█▊        | 1849/10000 [10:00:19<43:27:19, 19.19s/it, loss_gram=0, loss_l2=0.00484, loss_lpips=0.107]\n",
      "Steps:  18%|█▊        | 1850/10000 [10:00:38<43:42:04, 19.30s/it, loss_gram=0, loss_l2=0.00654, loss_lpips=0.0869]\n",
      "Steps:  19%|█▊        | 1851/10000 [10:00:55<42:15:59, 18.67s/it, loss_gram=0, loss_l2=0.00401, loss_lpips=0.0637]\n",
      "Steps:  19%|█▊        | 1852/10000 [10:01:16<43:38:18, 19.28s/it, loss_gram=0, loss_l2=0.00579, loss_lpips=0.075] \n",
      "Steps:  19%|█▊        | 1853/10000 [10:01:33<41:47:13, 18.46s/it, loss_gram=0, loss_l2=0.00262, loss_lpips=0.105]\n",
      "Steps:  19%|█▊        | 1854/10000 [10:01:52<42:36:30, 18.83s/it, loss_gram=0, loss_l2=0.0173, loss_lpips=0.0853]\n",
      "Steps:  19%|█▊        | 1855/10000 [10:02:12<42:54:03, 18.96s/it, loss_gram=0, loss_l2=0.0043, loss_lpips=0.117] \n",
      "Steps:  19%|█▊        | 1856/10000 [10:02:33<44:11:56, 19.54s/it, loss_gram=0, loss_l2=0.00789, loss_lpips=0.0933]\n",
      "Steps:  19%|█▊        | 1857/10000 [10:02:52<44:06:11, 19.50s/it, loss_gram=0, loss_l2=0.00763, loss_lpips=0.114] \n",
      "Steps:  19%|█▊        | 1858/10000 [10:03:13<45:19:30, 20.04s/it, loss_gram=0, loss_l2=0.00135, loss_lpips=0.156]\n",
      "Steps:  19%|█▊        | 1859/10000 [10:03:32<43:50:20, 19.39s/it, loss_gram=0, loss_l2=0.00611, loss_lpips=0.108]\n",
      "Steps:  19%|█▊        | 1860/10000 [10:03:52<44:54:22, 19.86s/it, loss_gram=0, loss_l2=0.0152, loss_lpips=0.136] \n",
      "Steps:  19%|█▊        | 1861/10000 [10:04:10<43:35:36, 19.28s/it, loss_gram=0, loss_l2=0.00356, loss_lpips=0.0905]\n",
      "Steps:  19%|█▊        | 1862/10000 [10:04:31<44:29:52, 19.68s/it, loss_gram=0, loss_l2=0.00485, loss_lpips=0.0942]\n",
      "Steps:  19%|█▊        | 1863/10000 [10:04:49<43:43:50, 19.35s/it, loss_gram=0, loss_l2=0.00664, loss_lpips=0.0957]\n",
      "Steps:  19%|█▊        | 1864/10000 [10:05:10<44:48:41, 19.83s/it, loss_gram=0, loss_l2=0.0041, loss_lpips=0.107]  \n",
      "Steps:  19%|█▊        | 1865/10000 [10:05:29<44:10:45, 19.55s/it, loss_gram=0, loss_l2=0.00583, loss_lpips=0.101]\n",
      "Steps:  19%|█▊        | 1866/10000 [10:05:48<43:24:44, 19.21s/it, loss_gram=0, loss_l2=0.00923, loss_lpips=0.127]\n",
      "Steps:  19%|█▊        | 1867/10000 [10:06:07<43:20:12, 19.18s/it, loss_gram=0, loss_l2=0.00398, loss_lpips=0.0833]\n",
      "Steps:  19%|█▊        | 1868/10000 [10:06:26<43:38:31, 19.32s/it, loss_gram=0, loss_l2=0.00683, loss_lpips=0.117] \n",
      "Steps:  19%|█▊        | 1869/10000 [10:06:45<43:04:52, 19.07s/it, loss_gram=0, loss_l2=0.00181, loss_lpips=0.0725]\n",
      "Steps:  19%|█▊        | 1870/10000 [10:07:06<44:26:00, 19.68s/it, loss_gram=0, loss_l2=0.0153, loss_lpips=0.116]  \n",
      "Steps:  19%|█▊        | 1871/10000 [10:07:23<42:41:55, 18.91s/it, loss_gram=0, loss_l2=0.00477, loss_lpips=0.105]\n",
      "Steps:  19%|█▊        | 1872/10000 [10:07:42<42:37:09, 18.88s/it, loss_gram=0, loss_l2=0.00669, loss_lpips=0.0808]\n",
      "Steps:  19%|█▊        | 1873/10000 [10:08:01<42:52:10, 18.99s/it, loss_gram=0, loss_l2=0.00753, loss_lpips=0.0811]\n",
      "Steps:  19%|█▊        | 1874/10000 [10:08:22<44:11:44, 19.58s/it, loss_gram=0, loss_l2=0.00498, loss_lpips=0.0927]\n",
      "Steps:  19%|█▉        | 1875/10000 [10:08:40<42:57:33, 19.03s/it, loss_gram=0, loss_l2=0.0066, loss_lpips=0.08]   \n",
      "Steps:  19%|█▉        | 1876/10000 [10:09:00<43:57:35, 19.48s/it, loss_gram=0, loss_l2=0.00884, loss_lpips=0.11]\n",
      "Steps:  19%|█▉        | 1877/10000 [10:09:19<43:36:18, 19.33s/it, loss_gram=0, loss_l2=0.0353, loss_lpips=0.162]\n",
      "Steps:  19%|█▉        | 1878/10000 [10:09:39<44:04:05, 19.53s/it, loss_gram=0, loss_l2=0.000353, loss_lpips=0.09]\n",
      "Steps:  19%|█▉        | 1879/10000 [10:09:58<43:30:50, 19.29s/it, loss_gram=0, loss_l2=0.00708, loss_lpips=0.109]\n",
      "Steps:  19%|█▉        | 1880/10000 [10:10:19<44:27:39, 19.71s/it, loss_gram=0, loss_l2=0.00812, loss_lpips=0.0903]\n",
      "Steps:  19%|█▉        | 1881/10000 [10:10:37<43:42:00, 19.38s/it, loss_gram=0, loss_l2=0.00216, loss_lpips=0.0908]\n",
      "Steps:  19%|█▉        | 1882/10000 [10:10:59<45:06:45, 20.01s/it, loss_gram=0, loss_l2=0.00683, loss_lpips=0.0934]\n",
      "Steps:  19%|█▉        | 1883/10000 [10:11:17<44:00:04, 19.52s/it, loss_gram=0, loss_l2=0.0088, loss_lpips=0.0787] \n",
      "Steps:  19%|█▉        | 1884/10000 [10:11:36<43:32:14, 19.31s/it, loss_gram=0, loss_l2=0.0499, loss_lpips=0.147] \n",
      "Steps:  19%|█▉        | 1885/10000 [10:11:55<43:03:37, 19.10s/it, loss_gram=0, loss_l2=0.00598, loss_lpips=0.123]\n",
      "Steps:  19%|█▉        | 1886/10000 [10:12:14<43:14:13, 19.18s/it, loss_gram=0, loss_l2=0.0186, loss_lpips=0.143] \n",
      "Steps:  19%|█▉        | 1887/10000 [10:12:31<41:59:38, 18.63s/it, loss_gram=0, loss_l2=0.0167, loss_lpips=0.151]\n",
      "Steps:  19%|█▉        | 1888/10000 [10:12:53<44:12:26, 19.62s/it, loss_gram=0, loss_l2=0.00242, loss_lpips=0.107]\n",
      "Steps:  19%|█▉        | 1889/10000 [10:13:12<43:16:07, 19.20s/it, loss_gram=0, loss_l2=0.0128, loss_lpips=0.0842]\n",
      "Steps:  19%|█▉        | 1890/10000 [10:13:32<44:29:12, 19.75s/it, loss_gram=0, loss_l2=0.004, loss_lpips=0.0908] \n",
      "Steps:  19%|█▉        | 1891/10000 [10:13:50<43:00:32, 19.09s/it, loss_gram=0, loss_l2=0.0229, loss_lpips=0.102]\n",
      "Steps:  19%|█▉        | 1892/10000 [10:14:10<43:43:58, 19.42s/it, loss_gram=0, loss_l2=0.0117, loss_lpips=0.108]\n",
      "Steps:  19%|█▉        | 1893/10000 [10:14:28<42:38:03, 18.93s/it, loss_gram=0, loss_l2=0.0171, loss_lpips=0.118]\n",
      "Steps:  19%|█▉        | 1894/10000 [10:14:49<43:47:34, 19.45s/it, loss_gram=0, loss_l2=0.01, loss_lpips=0.13]   \n",
      "Steps:  19%|█▉        | 1895/10000 [10:15:07<42:51:37, 19.04s/it, loss_gram=0, loss_l2=0.00582, loss_lpips=0.111]\n",
      "Steps:  19%|█▉        | 1896/10000 [10:15:28<44:13:16, 19.64s/it, loss_gram=0, loss_l2=0.00346, loss_lpips=0.161]\n",
      "Steps:  19%|█▉        | 1897/10000 [10:15:47<43:38:31, 19.39s/it, loss_gram=0, loss_l2=0.0442, loss_lpips=0.18]  \n",
      "Steps:  19%|█▉        | 1898/10000 [10:16:06<43:56:38, 19.53s/it, loss_gram=0, loss_l2=0.0356, loss_lpips=0.235]\n",
      "Steps:  19%|█▉        | 1899/10000 [10:16:25<43:00:27, 19.11s/it, loss_gram=0, loss_l2=0.00371, loss_lpips=0.0967]\n",
      "Steps:  19%|█▉        | 1900/10000 [10:16:46<44:26:38, 19.75s/it, loss_gram=0, loss_l2=0.00708, loss_lpips=0.0836]\n",
      "Steps:  19%|█▉        | 1901/10000 [10:17:04<43:13:40, 19.21s/it, loss_gram=0, loss_l2=0.00485, loss_lpips=0.0852]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  19%|█▉        | 1902/10000 [10:17:25<44:05:08, 19.60s/it, loss_gram=0, loss_l2=0.00388, loss_lpips=0.12]  \n",
      "Steps:  19%|█▉        | 1903/10000 [10:17:43<43:41:15, 19.42s/it, loss_gram=0, loss_l2=0.00643, loss_lpips=0.0698]\n",
      "Steps:  19%|█▉        | 1904/10000 [10:18:05<45:18:24, 20.15s/it, loss_gram=0, loss_l2=0.0099, loss_lpips=0.0708] \n",
      "Steps:  19%|█▉        | 1905/10000 [10:18:24<44:20:16, 19.72s/it, loss_gram=0, loss_l2=0.00439, loss_lpips=0.0732]\n",
      "Steps:  19%|█▉        | 1906/10000 [10:18:45<45:22:21, 20.18s/it, loss_gram=0, loss_l2=0.00573, loss_lpips=0.0648]\n",
      "Steps:  19%|█▉        | 1907/10000 [10:19:03<43:54:46, 19.53s/it, loss_gram=0, loss_l2=0.00337, loss_lpips=0.0647]\n",
      "Steps:  19%|█▉        | 1908/10000 [10:19:24<44:41:15, 19.88s/it, loss_gram=0, loss_l2=0.0149, loss_lpips=0.0977] \n",
      "Steps:  19%|█▉        | 1909/10000 [10:19:42<43:31:22, 19.37s/it, loss_gram=0, loss_l2=0.00228, loss_lpips=0.0646]\n",
      "Steps:  19%|█▉        | 1910/10000 [10:20:01<43:21:05, 19.29s/it, loss_gram=0, loss_l2=0.00428, loss_lpips=0.074] \n",
      "Steps:  19%|█▉        | 1911/10000 [10:20:20<43:01:34, 19.15s/it, loss_gram=0, loss_l2=0.00415, loss_lpips=0.107]\n",
      "Steps:  19%|█▉        | 1912/10000 [10:20:40<43:35:25, 19.40s/it, loss_gram=0, loss_l2=0.0161, loss_lpips=0.107] \n",
      "Steps:  19%|█▉        | 1913/10000 [10:20:57<41:49:39, 18.62s/it, loss_gram=0, loss_l2=0.0292, loss_lpips=0.134]\n",
      "Steps:  19%|█▉        | 1914/10000 [10:21:19<44:04:40, 19.62s/it, loss_gram=0, loss_l2=0.00349, loss_lpips=0.0894]\n",
      "Steps:  19%|█▉        | 1915/10000 [10:21:36<42:33:07, 18.95s/it, loss_gram=0, loss_l2=0.00512, loss_lpips=0.103] \n",
      "Steps:  19%|█▉        | 1916/10000 [10:21:58<44:07:45, 19.65s/it, loss_gram=0, loss_l2=0.00263, loss_lpips=0.0636]\n",
      "Steps:  19%|█▉        | 1917/10000 [10:22:15<42:37:42, 18.99s/it, loss_gram=0, loss_l2=0.00826, loss_lpips=0.136] \n",
      "Steps:  19%|█▉        | 1918/10000 [10:22:35<43:37:47, 19.43s/it, loss_gram=0, loss_l2=0.00262, loss_lpips=0.103]\n",
      "Steps:  19%|█▉        | 1919/10000 [10:22:53<42:43:19, 19.03s/it, loss_gram=0, loss_l2=0.00878, loss_lpips=0.0636]\n",
      "Steps:  19%|█▉        | 1920/10000 [10:23:14<44:01:00, 19.61s/it, loss_gram=0, loss_l2=0.00272, loss_lpips=0.108] \n",
      "Steps:  19%|█▉        | 1921/10000 [10:23:33<43:12:15, 19.25s/it, loss_gram=0, loss_l2=0.00787, loss_lpips=0.1]  \n",
      "Steps:  19%|█▉        | 1922/10000 [10:23:54<44:21:24, 19.77s/it, loss_gram=0, loss_l2=0.00559, loss_lpips=0.0822]\n",
      "Steps:  19%|█▉        | 1923/10000 [10:24:12<43:19:13, 19.31s/it, loss_gram=0, loss_l2=0.00947, loss_lpips=0.0946]\n",
      "Steps:  19%|█▉        | 1924/10000 [10:24:32<44:08:02, 19.67s/it, loss_gram=0, loss_l2=0.0073, loss_lpips=0.0632] \n",
      "Steps:  19%|█▉        | 1925/10000 [10:24:49<42:14:37, 18.83s/it, loss_gram=0, loss_l2=0.00518, loss_lpips=0.115]\n",
      "Steps:  19%|█▉        | 1926/10000 [10:25:11<44:02:08, 19.63s/it, loss_gram=0, loss_l2=0.0124, loss_lpips=0.0964]\n",
      "Steps:  19%|█▉        | 1927/10000 [10:25:29<43:01:33, 19.19s/it, loss_gram=0, loss_l2=0.00416, loss_lpips=0.103]\n",
      "Steps:  19%|█▉        | 1928/10000 [10:25:50<44:08:14, 19.68s/it, loss_gram=0, loss_l2=0.00118, loss_lpips=0.0731]\n",
      "Steps:  19%|█▉        | 1929/10000 [10:26:09<43:25:00, 19.37s/it, loss_gram=0, loss_l2=0.00803, loss_lpips=0.105] \n",
      "Steps:  19%|█▉        | 1930/10000 [10:26:28<43:43:39, 19.51s/it, loss_gram=0, loss_l2=0.0306, loss_lpips=0.167] \n",
      "Steps:  19%|█▉        | 1931/10000 [10:26:46<42:43:21, 19.06s/it, loss_gram=0, loss_l2=0.0052, loss_lpips=0.0808]\n",
      "Steps:  19%|█▉        | 1932/10000 [10:27:08<44:23:54, 19.81s/it, loss_gram=0, loss_l2=0.00238, loss_lpips=0.068]\n",
      "Steps:  19%|█▉        | 1933/10000 [10:27:25<42:51:58, 19.13s/it, loss_gram=0, loss_l2=0.00479, loss_lpips=0.106]\n",
      "Steps:  19%|█▉        | 1934/10000 [10:27:46<43:35:21, 19.45s/it, loss_gram=0, loss_l2=0.0139, loss_lpips=0.151] \n",
      "Steps:  19%|█▉        | 1935/10000 [10:28:05<43:25:41, 19.39s/it, loss_gram=0, loss_l2=0.00962, loss_lpips=0.0812]\n",
      "Steps:  19%|█▉        | 1936/10000 [10:28:25<44:12:14, 19.73s/it, loss_gram=0, loss_l2=0.00648, loss_lpips=0.118] \n",
      "Steps:  19%|█▉        | 1937/10000 [10:28:44<43:36:33, 19.47s/it, loss_gram=0, loss_l2=0.00999, loss_lpips=0.101]\n",
      "Steps:  19%|█▉        | 1938/10000 [10:29:03<43:23:55, 19.38s/it, loss_gram=0, loss_l2=0.00104, loss_lpips=0.139]\n",
      "Steps:  19%|█▉        | 1939/10000 [10:29:22<42:51:56, 19.14s/it, loss_gram=0, loss_l2=0.0204, loss_lpips=0.173] \n",
      "Steps:  19%|█▉        | 1940/10000 [10:29:42<43:18:09, 19.34s/it, loss_gram=0, loss_l2=0.0258, loss_lpips=0.123]\n",
      "Steps:  19%|█▉        | 1941/10000 [10:30:00<42:44:23, 19.09s/it, loss_gram=0, loss_l2=0.00174, loss_lpips=0.0585]\n",
      "Steps:  19%|█▉        | 1942/10000 [10:30:21<44:04:20, 19.69s/it, loss_gram=0, loss_l2=0.00896, loss_lpips=0.137] \n",
      "Steps:  19%|█▉        | 1943/10000 [10:30:39<42:44:34, 19.10s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.192] \n",
      "Steps:  19%|█▉        | 1944/10000 [10:31:00<43:58:13, 19.65s/it, loss_gram=0, loss_l2=0.0369, loss_lpips=0.165]\n",
      "Steps:  19%|█▉        | 1945/10000 [10:31:20<43:39:36, 19.51s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.078]\n",
      "Steps:  19%|█▉        | 1946/10000 [10:31:39<43:45:40, 19.56s/it, loss_gram=0, loss_l2=0.00993, loss_lpips=0.115]\n",
      "Steps:  19%|█▉        | 1947/10000 [10:31:57<42:43:51, 19.10s/it, loss_gram=0, loss_l2=0.0153, loss_lpips=0.0974]\n",
      "Steps:  19%|█▉        | 1948/10000 [10:32:18<43:47:33, 19.58s/it, loss_gram=0, loss_l2=0.00285, loss_lpips=0.0834]\n",
      "Steps:  19%|█▉        | 1949/10000 [10:32:35<42:25:26, 18.97s/it, loss_gram=0, loss_l2=0.00341, loss_lpips=0.16]  \n",
      "Steps:  20%|█▉        | 1950/10000 [10:32:55<42:43:30, 19.11s/it, loss_gram=0, loss_l2=0.00399, loss_lpips=0.0941]\n",
      "Steps:  20%|█▉        | 1951/10000 [10:33:12<41:07:37, 18.39s/it, loss_gram=0, loss_l2=0.00227, loss_lpips=0.0945]\n",
      "Steps:  20%|█▉        | 1952/10000 [10:33:31<42:10:24, 18.86s/it, loss_gram=0, loss_l2=0.0864, loss_lpips=0.276]  \n",
      "Steps:  20%|█▉        | 1953/10000 [10:33:50<41:44:51, 18.68s/it, loss_gram=0, loss_l2=0.0333, loss_lpips=0.112]\n",
      "Steps:  20%|█▉        | 1954/10000 [10:34:09<42:29:13, 19.01s/it, loss_gram=0, loss_l2=0.00891, loss_lpips=0.115]\n",
      "Steps:  20%|█▉        | 1955/10000 [10:34:28<42:15:21, 18.91s/it, loss_gram=0, loss_l2=0.00797, loss_lpips=0.103]\n",
      "Steps:  20%|█▉        | 1956/10000 [10:34:48<43:08:18, 19.31s/it, loss_gram=0, loss_l2=0.00666, loss_lpips=0.11] \n",
      "Steps:  20%|█▉        | 1957/10000 [10:35:07<42:29:14, 19.02s/it, loss_gram=0, loss_l2=0.00444, loss_lpips=0.0881]\n",
      "Steps:  20%|█▉        | 1958/10000 [10:35:28<44:05:18, 19.74s/it, loss_gram=0, loss_l2=0.00108, loss_lpips=0.089] \n",
      "Steps:  20%|█▉        | 1959/10000 [10:35:47<43:20:21, 19.40s/it, loss_gram=0, loss_l2=0.00679, loss_lpips=0.0906]\n",
      "Steps:  20%|█▉        | 1960/10000 [10:36:05<42:52:39, 19.20s/it, loss_gram=0, loss_l2=0.00767, loss_lpips=0.115] \n",
      "Steps:  20%|█▉        | 1961/10000 [10:36:23<41:34:47, 18.62s/it, loss_gram=0, loss_l2=0.00473, loss_lpips=0.125]\n",
      "Steps:  20%|█▉        | 1962/10000 [10:36:43<42:30:40, 19.04s/it, loss_gram=0, loss_l2=0.0157, loss_lpips=0.14]  \n",
      "Steps:  20%|█▉        | 1963/10000 [10:37:01<42:21:32, 18.97s/it, loss_gram=0, loss_l2=0.0115, loss_lpips=0.0789]\n",
      "Steps:  20%|█▉        | 1964/10000 [10:37:21<42:42:02, 19.13s/it, loss_gram=0, loss_l2=0.00649, loss_lpips=0.134]\n",
      "Steps:  20%|█▉        | 1965/10000 [10:37:38<41:39:33, 18.66s/it, loss_gram=0, loss_l2=0.00954, loss_lpips=0.119]\n",
      "Steps:  20%|█▉        | 1966/10000 [10:37:57<41:27:13, 18.58s/it, loss_gram=0, loss_l2=0.00632, loss_lpips=0.0947]\n",
      "Steps:  20%|█▉        | 1967/10000 [10:38:16<41:50:55, 18.75s/it, loss_gram=0, loss_l2=0.00973, loss_lpips=0.114] \n",
      "Steps:  20%|█▉        | 1968/10000 [10:38:36<42:23:35, 19.00s/it, loss_gram=0, loss_l2=0.013, loss_lpips=0.0972] \n",
      "Steps:  20%|█▉        | 1969/10000 [10:38:53<41:23:16, 18.55s/it, loss_gram=0, loss_l2=0.019, loss_lpips=0.158] \n",
      "Steps:  20%|█▉        | 1970/10000 [10:39:13<42:31:21, 19.06s/it, loss_gram=0, loss_l2=0.0111, loss_lpips=0.157]\n",
      "Steps:  20%|█▉        | 1971/10000 [10:39:31<41:48:47, 18.75s/it, loss_gram=0, loss_l2=0.0813, loss_lpips=0.293]\n",
      "Steps:  20%|█▉        | 1972/10000 [10:39:52<42:41:54, 19.15s/it, loss_gram=0, loss_l2=0.00754, loss_lpips=0.0918]\n",
      "Steps:  20%|█▉        | 1973/10000 [10:40:10<41:57:22, 18.82s/it, loss_gram=0, loss_l2=0.00302, loss_lpips=0.112] \n",
      "Steps:  20%|█▉        | 1974/10000 [10:40:32<44:27:08, 19.94s/it, loss_gram=0, loss_l2=0.0315, loss_lpips=0.139] \n",
      "Steps:  20%|█▉        | 1975/10000 [10:40:50<42:56:47, 19.27s/it, loss_gram=0, loss_l2=0.00798, loss_lpips=0.114]\n",
      "Steps:  20%|█▉        | 1976/10000 [10:41:09<43:08:00, 19.35s/it, loss_gram=0, loss_l2=0.00999, loss_lpips=0.0857]\n",
      "Steps:  20%|█▉        | 1977/10000 [10:41:28<42:31:27, 19.08s/it, loss_gram=0, loss_l2=0.000982, loss_lpips=0.15] \n",
      "Steps:  20%|█▉        | 1978/10000 [10:41:49<43:47:38, 19.65s/it, loss_gram=0, loss_l2=0.00979, loss_lpips=0.153]\n",
      "Steps:  20%|█▉        | 1979/10000 [10:42:08<43:52:04, 19.69s/it, loss_gram=0, loss_l2=0.00608, loss_lpips=0.154]\n",
      "Steps:  20%|█▉        | 1980/10000 [10:42:31<45:54:46, 20.61s/it, loss_gram=0, loss_l2=0.00432, loss_lpips=0.0922]\n",
      "Steps:  20%|█▉        | 1981/10000 [10:42:53<46:57:06, 21.08s/it, loss_gram=0, loss_l2=0.00652, loss_lpips=0.0828]\n",
      "Steps:  20%|█▉        | 1982/10000 [10:43:16<47:39:26, 21.40s/it, loss_gram=0, loss_l2=0.00876, loss_lpips=0.0952]\n",
      "Steps:  20%|█▉        | 1983/10000 [10:43:35<46:14:16, 20.76s/it, loss_gram=0, loss_l2=0.0067, loss_lpips=0.1]    \n",
      "Steps:  20%|█▉        | 1984/10000 [10:43:57<47:04:58, 21.14s/it, loss_gram=0, loss_l2=0.00763, loss_lpips=0.139]\n",
      "Steps:  20%|█▉        | 1985/10000 [10:44:16<45:49:57, 20.59s/it, loss_gram=0, loss_l2=0.0145, loss_lpips=0.113] \n",
      "Steps:  20%|█▉        | 1986/10000 [10:44:38<46:36:50, 20.94s/it, loss_gram=0, loss_l2=0.00572, loss_lpips=0.0957]\n",
      "Steps:  20%|█▉        | 1987/10000 [10:44:57<45:19:49, 20.37s/it, loss_gram=0, loss_l2=0.000855, loss_lpips=0.167]\n",
      "Steps:  20%|█▉        | 1988/10000 [10:45:22<48:28:36, 21.78s/it, loss_gram=0, loss_l2=0.01, loss_lpips=0.077]    \n",
      "Steps:  20%|█▉        | 1989/10000 [10:45:42<47:09:14, 21.19s/it, loss_gram=0, loss_l2=0.00535, loss_lpips=0.0936]\n",
      "Steps:  20%|█▉        | 1990/10000 [10:46:03<47:14:26, 21.23s/it, loss_gram=0, loss_l2=0.0235, loss_lpips=0.127]  \n",
      "Steps:  20%|█▉        | 1991/10000 [10:46:23<46:13:16, 20.78s/it, loss_gram=0, loss_l2=0.00264, loss_lpips=0.0935]\n",
      "Steps:  20%|█▉        | 1992/10000 [10:46:46<47:58:47, 21.57s/it, loss_gram=0, loss_l2=0.00106, loss_lpips=0.157] \n",
      "Steps:  20%|█▉        | 1993/10000 [10:47:06<46:50:53, 21.06s/it, loss_gram=0, loss_l2=0.00249, loss_lpips=0.095]\n",
      "Steps:  20%|█▉        | 1994/10000 [10:47:29<47:49:47, 21.51s/it, loss_gram=0, loss_l2=0.00713, loss_lpips=0.0828]\n",
      "Steps:  20%|█▉        | 1995/10000 [10:47:48<46:19:21, 20.83s/it, loss_gram=0, loss_l2=0.0102, loss_lpips=0.195]  \n",
      "Steps:  20%|█▉        | 1996/10000 [10:48:12<48:16:28, 21.71s/it, loss_gram=0, loss_l2=0.00561, loss_lpips=0.0763]\n",
      "Steps:  20%|█▉        | 1997/10000 [10:48:33<47:50:52, 21.52s/it, loss_gram=0, loss_l2=0.00182, loss_lpips=0.0647]\n",
      "Steps:  20%|█▉        | 1998/10000 [10:48:53<46:35:44, 20.96s/it, loss_gram=0, loss_l2=0.00359, loss_lpips=0.108] \n",
      "Steps:  20%|█▉        | 1999/10000 [10:49:10<44:31:19, 20.03s/it, loss_gram=0, loss_l2=0.00496, loss_lpips=0.094]\n",
      "Steps:  20%|██        | 2000/10000 [10:49:31<44:46:09, 20.15s/it, loss_gram=0, loss_l2=0.0359, loss_lpips=0.105] \n",
      "Steps:  20%|██        | 2001/10000 [10:49:50<44:00:32, 19.81s/it, loss_gram=0, loss_l2=0.00698, loss_lpips=0.0928]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  20%|██        | 2002/10000 [10:53:49<190:08:11, 85.58s/it, loss_gram=1.04, loss_l2=0.0013, loss_lpips=0.0667]\n",
      "Steps:  20%|██        | 2003/10000 [10:54:11<147:30:10, 66.40s/it, loss_gram=2.28, loss_l2=0.00569, loss_lpips=0.0696]\n",
      "Steps:  20%|██        | 2004/10000 [10:54:34<119:11:59, 53.67s/it, loss_gram=61.5, loss_l2=0.0204, loss_lpips=0.134]  \n",
      "Steps:  20%|██        | 2005/10000 [10:54:51<94:37:20, 42.61s/it, loss_gram=1.62, loss_l2=0.00826, loss_lpips=0.0868]\n",
      "Steps:  20%|██        | 2006/10000 [10:55:17<83:14:54, 37.49s/it, loss_gram=11.8, loss_l2=0.0101, loss_lpips=0.0878] \n",
      "Steps:  20%|██        | 2007/10000 [10:55:34<70:01:28, 31.54s/it, loss_gram=0.773, loss_l2=0.00532, loss_lpips=0.101]\n",
      "Steps:  20%|██        | 2008/10000 [10:55:59<65:44:18, 29.61s/it, loss_gram=0.641, loss_l2=0.00126, loss_lpips=0.158]\n",
      "Steps:  20%|██        | 2009/10000 [10:56:16<57:08:50, 25.75s/it, loss_gram=1, loss_l2=0.00132, loss_lpips=0.0586]   \n",
      "Steps:  20%|██        | 2010/10000 [10:56:39<55:09:00, 24.85s/it, loss_gram=17.6, loss_l2=0.0337, loss_lpips=0.105]\n",
      "Steps:  20%|██        | 2011/10000 [10:56:57<50:44:41, 22.87s/it, loss_gram=5.25, loss_l2=0.00983, loss_lpips=0.0808]\n",
      "Steps:  20%|██        | 2012/10000 [10:57:22<51:47:31, 23.34s/it, loss_gram=14.1, loss_l2=0.0103, loss_lpips=0.106]  \n",
      "Steps:  20%|██        | 2013/10000 [10:57:38<47:15:43, 21.30s/it, loss_gram=5.75, loss_l2=0.00797, loss_lpips=0.104]\n",
      "Steps:  20%|██        | 2014/10000 [10:58:03<49:50:42, 22.47s/it, loss_gram=2.31, loss_l2=0.00373, loss_lpips=0.061]\n",
      "Steps:  20%|██        | 2015/10000 [10:58:21<46:27:04, 20.94s/it, loss_gram=8.12, loss_l2=0.0106, loss_lpips=0.106] \n",
      "Steps:  20%|██        | 2016/10000 [10:58:44<47:46:30, 21.54s/it, loss_gram=1.47, loss_l2=0.024, loss_lpips=0.212] \n",
      "Steps:  20%|██        | 2017/10000 [10:59:02<45:27:28, 20.50s/it, loss_gram=6.16, loss_l2=0.00967, loss_lpips=0.109]\n",
      "Steps:  20%|██        | 2018/10000 [10:59:27<48:28:13, 21.86s/it, loss_gram=1.41, loss_l2=0.00936, loss_lpips=0.108]\n",
      "Steps:  20%|██        | 2019/10000 [10:59:44<45:15:32, 20.42s/it, loss_gram=8.56, loss_l2=0.01, loss_lpips=0.198]   \n",
      "Steps:  20%|██        | 2020/10000 [11:00:08<47:55:49, 21.62s/it, loss_gram=27.6, loss_l2=0.036, loss_lpips=0.158]\n",
      "Steps:  20%|██        | 2021/10000 [11:00:26<45:05:00, 20.34s/it, loss_gram=0.352, loss_l2=0.000619, loss_lpips=0.0756]\n",
      "Steps:  20%|██        | 2022/10000 [11:00:51<48:05:32, 21.70s/it, loss_gram=1020.0, loss_l2=0.027, loss_lpips=0.153]   \n",
      "Steps:  20%|██        | 2023/10000 [11:01:08<45:31:34, 20.55s/it, loss_gram=34, loss_l2=0.0308, loss_lpips=0.315]   \n",
      "Steps:  20%|██        | 2024/10000 [11:01:35<49:38:47, 22.41s/it, loss_gram=3.72, loss_l2=0.00761, loss_lpips=0.137]\n",
      "Steps:  20%|██        | 2025/10000 [11:01:53<46:43:18, 21.09s/it, loss_gram=1.64, loss_l2=0.00459, loss_lpips=0.131]\n",
      "Steps:  20%|██        | 2026/10000 [11:02:19<49:47:49, 22.48s/it, loss_gram=3.27, loss_l2=0.0104, loss_lpips=0.106] \n",
      "Steps:  20%|██        | 2027/10000 [11:02:36<46:15:09, 20.88s/it, loss_gram=6.94, loss_l2=0.0105, loss_lpips=0.106]\n",
      "Steps:  20%|██        | 2028/10000 [11:03:01<49:06:04, 22.17s/it, loss_gram=1.97, loss_l2=0.0204, loss_lpips=0.146]\n",
      "Steps:  20%|██        | 2029/10000 [11:03:18<45:31:41, 20.56s/it, loss_gram=4.19, loss_l2=0.00424, loss_lpips=0.0684]\n",
      "Steps:  20%|██        | 2030/10000 [11:03:43<48:32:14, 21.92s/it, loss_gram=0.941, loss_l2=0.00625, loss_lpips=0.0964]\n",
      "Steps:  20%|██        | 2031/10000 [11:03:59<44:45:36, 20.22s/it, loss_gram=11.1, loss_l2=0.0147, loss_lpips=0.205]   \n",
      "Steps:  20%|██        | 2032/10000 [11:04:24<47:25:25, 21.43s/it, loss_gram=26.2, loss_l2=0.00281, loss_lpips=0.0919]\n",
      "Steps:  20%|██        | 2033/10000 [11:04:40<44:04:13, 19.91s/it, loss_gram=8.56, loss_l2=0.00982, loss_lpips=0.203] \n",
      "Steps:  20%|██        | 2034/10000 [11:05:04<47:05:01, 21.28s/it, loss_gram=114, loss_l2=0.0189, loss_lpips=0.135]  \n",
      "Steps:  20%|██        | 2035/10000 [11:05:23<45:10:51, 20.42s/it, loss_gram=7.25, loss_l2=0.00283, loss_lpips=0.224]\n",
      "Steps:  20%|██        | 2036/10000 [11:05:49<48:59:53, 22.15s/it, loss_gram=23.9, loss_l2=0.0246, loss_lpips=0.222] \n",
      "Steps:  20%|██        | 2037/10000 [11:06:07<46:09:20, 20.87s/it, loss_gram=8.56, loss_l2=0.0138, loss_lpips=0.124]\n",
      "Steps:  20%|██        | 2038/10000 [11:06:33<49:33:38, 22.41s/it, loss_gram=1.51, loss_l2=0.0249, loss_lpips=0.155]\n",
      "Steps:  20%|██        | 2039/10000 [11:06:49<45:12:56, 20.45s/it, loss_gram=1.55, loss_l2=0.00179, loss_lpips=0.108]\n",
      "Steps:  20%|██        | 2040/10000 [11:07:14<48:36:17, 21.98s/it, loss_gram=12.8, loss_l2=0.0319, loss_lpips=0.137] \n",
      "Steps:  20%|██        | 2041/10000 [11:07:31<44:49:52, 20.28s/it, loss_gram=4.56, loss_l2=0.0634, loss_lpips=0.167]\n",
      "Steps:  20%|██        | 2042/10000 [11:07:56<47:47:31, 21.62s/it, loss_gram=3.11, loss_l2=0.00702, loss_lpips=0.0938]\n",
      "Steps:  20%|██        | 2043/10000 [11:08:14<45:39:39, 20.66s/it, loss_gram=7.19, loss_l2=0.0128, loss_lpips=0.139]  \n",
      "Steps:  20%|██        | 2044/10000 [11:08:40<49:08:42, 22.24s/it, loss_gram=3.97, loss_l2=0.00963, loss_lpips=0.0759]\n",
      "Steps:  20%|██        | 2045/10000 [11:08:58<46:19:49, 20.97s/it, loss_gram=4.75, loss_l2=0.0218, loss_lpips=0.243]  \n",
      "Steps:  20%|██        | 2046/10000 [11:09:23<49:01:15, 22.19s/it, loss_gram=8.12, loss_l2=0.00507, loss_lpips=0.117]\n",
      "Steps:  20%|██        | 2047/10000 [11:09:41<46:25:36, 21.02s/it, loss_gram=12.8, loss_l2=0.00953, loss_lpips=0.113]\n",
      "Steps:  20%|██        | 2048/10000 [11:10:06<49:02:58, 22.21s/it, loss_gram=0.973, loss_l2=0.0345, loss_lpips=0.286]\n",
      "Steps:  20%|██        | 2049/10000 [11:10:23<45:20:25, 20.53s/it, loss_gram=118, loss_l2=0.00718, loss_lpips=0.0818]\n",
      "Steps:  20%|██        | 2050/10000 [11:10:47<47:56:56, 21.71s/it, loss_gram=1.96, loss_l2=0.00353, loss_lpips=0.114]\n",
      "Steps:  21%|██        | 2051/10000 [11:11:04<44:59:09, 20.37s/it, loss_gram=3.25, loss_l2=0.00548, loss_lpips=0.107]\n",
      "Steps:  21%|██        | 2052/10000 [11:11:30<48:13:50, 21.85s/it, loss_gram=7.25, loss_l2=0.0242, loss_lpips=0.111] \n",
      "Steps:  21%|██        | 2053/10000 [11:11:47<45:07:10, 20.44s/it, loss_gram=3.92, loss_l2=0.00794, loss_lpips=0.0697]\n",
      "Steps:  21%|██        | 2054/10000 [11:12:11<47:51:30, 21.68s/it, loss_gram=1.36, loss_l2=0.0194, loss_lpips=0.223]  \n",
      "Steps:  21%|██        | 2055/10000 [11:12:29<44:49:05, 20.31s/it, loss_gram=4.25, loss_l2=0.00892, loss_lpips=0.0973]\n",
      "Steps:  21%|██        | 2056/10000 [11:12:53<47:31:15, 21.54s/it, loss_gram=1.21, loss_l2=0.00268, loss_lpips=0.169] \n",
      "Steps:  21%|██        | 2057/10000 [11:13:10<44:41:09, 20.25s/it, loss_gram=2.62, loss_l2=0.00206, loss_lpips=0.122]\n",
      "Steps:  21%|██        | 2058/10000 [11:13:34<47:21:03, 21.46s/it, loss_gram=6.78, loss_l2=0.0074, loss_lpips=0.123] \n",
      "Steps:  21%|██        | 2059/10000 [11:13:52<44:39:20, 20.24s/it, loss_gram=6.94, loss_l2=0.0115, loss_lpips=0.148]\n",
      "Steps:  21%|██        | 2060/10000 [11:14:15<46:54:23, 21.27s/it, loss_gram=6.97, loss_l2=0.0042, loss_lpips=0.0933]\n",
      "Steps:  21%|██        | 2061/10000 [11:14:32<43:48:06, 19.86s/it, loss_gram=2.2, loss_l2=0.0146, loss_lpips=0.109]  \n",
      "Steps:  21%|██        | 2062/10000 [11:14:58<47:34:20, 21.57s/it, loss_gram=20.1, loss_l2=0.0153, loss_lpips=0.159]\n",
      "Steps:  21%|██        | 2063/10000 [11:15:15<44:59:46, 20.41s/it, loss_gram=5.12, loss_l2=0.011, loss_lpips=0.0885]\n",
      "Steps:  21%|██        | 2064/10000 [11:15:40<47:48:53, 21.69s/it, loss_gram=2.34, loss_l2=0.00723, loss_lpips=0.172]\n",
      "Steps:  21%|██        | 2065/10000 [11:15:58<45:34:06, 20.67s/it, loss_gram=5.84, loss_l2=0.00574, loss_lpips=0.109]\n",
      "Steps:  21%|██        | 2066/10000 [11:16:23<48:25:04, 21.97s/it, loss_gram=3.02, loss_l2=0.0115, loss_lpips=0.158] \n",
      "Steps:  21%|██        | 2067/10000 [11:16:40<44:58:36, 20.41s/it, loss_gram=7.19, loss_l2=0.0272, loss_lpips=0.162]\n",
      "Steps:  21%|██        | 2068/10000 [11:17:06<48:37:25, 22.07s/it, loss_gram=3.02, loss_l2=0.00674, loss_lpips=0.138]\n",
      "Steps:  21%|██        | 2069/10000 [11:17:25<46:10:55, 20.96s/it, loss_gram=18.4, loss_l2=0.00548, loss_lpips=0.102]\n",
      "Steps:  21%|██        | 2070/10000 [11:17:49<48:23:57, 21.97s/it, loss_gram=2.56, loss_l2=0.01, loss_lpips=0.105]   \n",
      "Steps:  21%|██        | 2071/10000 [11:18:06<45:36:48, 20.71s/it, loss_gram=1.37, loss_l2=0.0058, loss_lpips=0.142]\n",
      "Steps:  21%|██        | 2072/10000 [11:18:31<48:03:45, 21.82s/it, loss_gram=3.17, loss_l2=0.012, loss_lpips=0.0918]\n",
      "Steps:  21%|██        | 2073/10000 [11:18:48<44:54:03, 20.39s/it, loss_gram=11.1, loss_l2=0.00628, loss_lpips=0.109]\n",
      "Steps:  21%|██        | 2074/10000 [11:19:13<47:37:56, 21.63s/it, loss_gram=6.12, loss_l2=0.00934, loss_lpips=0.0876]\n",
      "Steps:  21%|██        | 2075/10000 [11:19:29<44:18:39, 20.13s/it, loss_gram=1.17, loss_l2=0.00441, loss_lpips=0.079] \n",
      "Steps:  21%|██        | 2076/10000 [11:19:55<47:53:30, 21.76s/it, loss_gram=1.45, loss_l2=0.00202, loss_lpips=0.104]\n",
      "Steps:  21%|██        | 2077/10000 [11:20:12<44:52:58, 20.39s/it, loss_gram=1.05, loss_l2=0.00666, loss_lpips=0.0764]\n",
      "Steps:  21%|██        | 2078/10000 [11:20:37<48:17:04, 21.94s/it, loss_gram=4.59, loss_l2=0.00979, loss_lpips=0.0966]\n",
      "Steps:  21%|██        | 2079/10000 [11:20:55<45:28:24, 20.67s/it, loss_gram=9.19, loss_l2=0.0135, loss_lpips=0.136]  \n",
      "Steps:  21%|██        | 2080/10000 [11:21:21<48:31:01, 22.05s/it, loss_gram=8.44, loss_l2=0.00808, loss_lpips=0.114]\n",
      "Steps:  21%|██        | 2081/10000 [11:21:38<45:23:05, 20.63s/it, loss_gram=2.05, loss_l2=0.00501, loss_lpips=0.0764]\n",
      "Steps:  21%|██        | 2082/10000 [11:22:02<47:52:27, 21.77s/it, loss_gram=6.12, loss_l2=0.0164, loss_lpips=0.111]  \n",
      "Steps:  21%|██        | 2083/10000 [11:22:20<45:10:17, 20.54s/it, loss_gram=3, loss_l2=0.00489, loss_lpips=0.0997] \n",
      "Steps:  21%|██        | 2084/10000 [11:22:45<48:22:16, 22.00s/it, loss_gram=868, loss_l2=0.022, loss_lpips=0.12]  \n",
      "Steps:  21%|██        | 2085/10000 [11:23:03<45:34:57, 20.73s/it, loss_gram=1.52, loss_l2=0.00747, loss_lpips=0.104]\n",
      "Steps:  21%|██        | 2086/10000 [11:23:28<48:21:14, 22.00s/it, loss_gram=1.9, loss_l2=0.0135, loss_lpips=0.184]  \n",
      "Steps:  21%|██        | 2087/10000 [11:23:45<45:25:45, 20.67s/it, loss_gram=0.582, loss_l2=0.0044, loss_lpips=0.111]\n",
      "Steps:  21%|██        | 2088/10000 [11:24:12<48:54:44, 22.26s/it, loss_gram=2.66, loss_l2=0.0334, loss_lpips=0.249] \n",
      "Steps:  21%|██        | 2089/10000 [11:24:28<45:22:54, 20.65s/it, loss_gram=1.48, loss_l2=0.0169, loss_lpips=0.12] \n",
      "Steps:  21%|██        | 2090/10000 [11:24:54<48:28:01, 22.06s/it, loss_gram=4, loss_l2=0.00309, loss_lpips=0.105] \n",
      "Steps:  21%|██        | 2091/10000 [11:25:11<45:31:14, 20.72s/it, loss_gram=6.09, loss_l2=0.01, loss_lpips=0.103]\n",
      "Steps:  21%|██        | 2092/10000 [11:25:36<47:55:25, 21.82s/it, loss_gram=2.98, loss_l2=0.00597, loss_lpips=0.0938]\n",
      "Steps:  21%|██        | 2093/10000 [11:25:54<45:36:32, 20.77s/it, loss_gram=2.69, loss_l2=0.0039, loss_lpips=0.0793] \n",
      "Steps:  21%|██        | 2094/10000 [11:26:19<48:21:07, 22.02s/it, loss_gram=1.88, loss_l2=0.00383, loss_lpips=0.133]\n",
      "Steps:  21%|██        | 2095/10000 [11:26:36<45:02:45, 20.51s/it, loss_gram=2.41, loss_l2=0.00182, loss_lpips=0.15] \n",
      "Steps:  21%|██        | 2096/10000 [11:27:01<47:54:10, 21.82s/it, loss_gram=14.6, loss_l2=0.00861, loss_lpips=0.147]\n",
      "Steps:  21%|██        | 2097/10000 [11:27:19<45:01:45, 20.51s/it, loss_gram=3.67, loss_l2=0.0128, loss_lpips=0.154] \n",
      "Steps:  21%|██        | 2098/10000 [11:27:44<48:19:02, 22.01s/it, loss_gram=0.547, loss_l2=0.00523, loss_lpips=0.0931]\n",
      "Steps:  21%|██        | 2099/10000 [11:28:02<45:26:34, 20.71s/it, loss_gram=1.86, loss_l2=0.00385, loss_lpips=0.0675] \n",
      "Steps:  21%|██        | 2100/10000 [11:28:26<47:54:27, 21.83s/it, loss_gram=1.09, loss_l2=0.00539, loss_lpips=0.0999]\n",
      "Steps:  21%|██        | 2101/10000 [11:28:43<44:54:24, 20.47s/it, loss_gram=3.03, loss_l2=0.0104, loss_lpips=0.0916] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  21%|██        | 2102/10000 [11:29:10<48:53:10, 22.28s/it, loss_gram=0.977, loss_l2=0.00277, loss_lpips=0.0842]\n",
      "Steps:  21%|██        | 2103/10000 [11:29:28<46:05:03, 21.01s/it, loss_gram=1.4, loss_l2=0.0103, loss_lpips=0.0869]   \n",
      "Steps:  21%|██        | 2104/10000 [11:29:52<48:19:28, 22.03s/it, loss_gram=3.16, loss_l2=0.00646, loss_lpips=0.107]\n",
      "Steps:  21%|██        | 2105/10000 [11:30:09<44:53:17, 20.47s/it, loss_gram=0.441, loss_l2=0.00182, loss_lpips=0.0961]\n",
      "Steps:  21%|██        | 2106/10000 [11:30:34<47:44:00, 21.77s/it, loss_gram=5.75, loss_l2=0.00673, loss_lpips=0.187]  \n",
      "Steps:  21%|██        | 2107/10000 [11:30:52<45:07:41, 20.58s/it, loss_gram=2.66, loss_l2=0.0114, loss_lpips=0.0984]\n",
      "Steps:  21%|██        | 2108/10000 [11:31:16<47:15:41, 21.56s/it, loss_gram=10.9, loss_l2=0.0157, loss_lpips=0.126] \n",
      "Steps:  21%|██        | 2109/10000 [11:31:32<44:08:19, 20.14s/it, loss_gram=0.185, loss_l2=0.00403, loss_lpips=0.101]\n",
      "Steps:  21%|██        | 2110/10000 [11:31:57<47:08:54, 21.51s/it, loss_gram=3.72, loss_l2=0.00957, loss_lpips=0.109] \n",
      "Steps:  21%|██        | 2111/10000 [11:32:14<44:24:17, 20.26s/it, loss_gram=1.39, loss_l2=0.019, loss_lpips=0.157]  \n",
      "Steps:  21%|██        | 2112/10000 [11:32:41<48:24:51, 22.10s/it, loss_gram=6.19, loss_l2=0.00624, loss_lpips=0.125]\n",
      "Steps:  21%|██        | 2113/10000 [11:32:59<45:47:06, 20.90s/it, loss_gram=1.38, loss_l2=0.00736, loss_lpips=0.0828]\n",
      "Steps:  21%|██        | 2114/10000 [11:33:24<48:35:21, 22.18s/it, loss_gram=1.05, loss_l2=0.0449, loss_lpips=0.186]  \n",
      "Steps:  21%|██        | 2115/10000 [11:33:41<45:02:17, 20.56s/it, loss_gram=10, loss_l2=0.0101, loss_lpips=0.102]  \n",
      "Steps:  21%|██        | 2116/10000 [11:34:05<47:23:48, 21.64s/it, loss_gram=0.436, loss_l2=0.00436, loss_lpips=0.11]\n",
      "Steps:  21%|██        | 2117/10000 [11:34:23<44:49:55, 20.47s/it, loss_gram=1.22, loss_l2=0.013, loss_lpips=0.232]  \n",
      "Steps:  21%|██        | 2118/10000 [11:34:50<49:10:01, 22.46s/it, loss_gram=1.62, loss_l2=0.00517, loss_lpips=0.0872]\n",
      "Steps:  21%|██        | 2119/10000 [11:35:08<45:57:11, 20.99s/it, loss_gram=16.9, loss_l2=0.00499, loss_lpips=0.104] \n",
      "Steps:  21%|██        | 2120/10000 [11:35:34<49:10:41, 22.47s/it, loss_gram=1.06, loss_l2=0.00414, loss_lpips=0.0962]\n",
      "Steps:  21%|██        | 2121/10000 [11:35:50<45:30:58, 20.80s/it, loss_gram=3.44, loss_l2=0.00874, loss_lpips=0.111] \n",
      "Steps:  21%|██        | 2122/10000 [11:36:14<47:35:56, 21.75s/it, loss_gram=93.5, loss_l2=0.0165, loss_lpips=0.139] \n",
      "Steps:  21%|██        | 2123/10000 [11:36:33<45:27:57, 20.78s/it, loss_gram=1.84, loss_l2=0.006, loss_lpips=0.0941]\n",
      "Steps:  21%|██        | 2124/10000 [11:36:57<47:53:26, 21.89s/it, loss_gram=7.19, loss_l2=0.00617, loss_lpips=0.131]\n",
      "Steps:  21%|██▏       | 2125/10000 [11:37:14<44:19:58, 20.27s/it, loss_gram=5.97, loss_l2=0.0104, loss_lpips=0.119] \n",
      "Steps:  21%|██▏       | 2126/10000 [11:37:39<47:51:43, 21.88s/it, loss_gram=4.5, loss_l2=0.0812, loss_lpips=0.31]  \n",
      "Steps:  21%|██▏       | 2127/10000 [11:37:57<44:55:13, 20.54s/it, loss_gram=11.1, loss_l2=0.0197, loss_lpips=0.162]\n",
      "Steps:  21%|██▏       | 2128/10000 [11:38:21<47:19:50, 21.65s/it, loss_gram=9.81, loss_l2=0.0124, loss_lpips=0.118]\n",
      "Steps:  21%|██▏       | 2129/10000 [11:38:37<43:39:59, 19.97s/it, loss_gram=1.77, loss_l2=0.00924, loss_lpips=0.0876]\n",
      "Steps:  21%|██▏       | 2130/10000 [11:39:01<46:07:21, 21.10s/it, loss_gram=8.31, loss_l2=0.00946, loss_lpips=0.174] \n",
      "Steps:  21%|██▏       | 2131/10000 [11:39:18<43:17:28, 19.81s/it, loss_gram=1.97, loss_l2=0.00526, loss_lpips=0.101]\n",
      "Steps:  21%|██▏       | 2132/10000 [11:39:42<46:20:33, 21.20s/it, loss_gram=1.64, loss_l2=0.0111, loss_lpips=0.0908]\n",
      "Steps:  21%|██▏       | 2133/10000 [11:39:58<43:04:45, 19.71s/it, loss_gram=1.56, loss_l2=0.00406, loss_lpips=0.0981]\n",
      "Steps:  21%|██▏       | 2134/10000 [11:40:23<46:29:53, 21.28s/it, loss_gram=6.22, loss_l2=0.0046, loss_lpips=0.0851] \n",
      "Steps:  21%|██▏       | 2135/10000 [11:40:41<44:10:42, 20.22s/it, loss_gram=0.504, loss_l2=0.00576, loss_lpips=0.103]\n",
      "Steps:  21%|██▏       | 2136/10000 [11:41:06<47:07:18, 21.57s/it, loss_gram=14, loss_l2=0.0136, loss_lpips=0.118]    \n",
      "Steps:  21%|██▏       | 2137/10000 [11:41:23<44:06:44, 20.20s/it, loss_gram=2.91, loss_l2=0.00791, loss_lpips=0.112]\n",
      "Steps:  21%|██▏       | 2138/10000 [11:41:48<47:21:41, 21.69s/it, loss_gram=0.27, loss_l2=0.00148, loss_lpips=0.155]\n",
      "Steps:  21%|██▏       | 2139/10000 [11:42:06<44:50:28, 20.54s/it, loss_gram=2.05, loss_l2=0.00221, loss_lpips=0.109]\n",
      "Steps:  21%|██▏       | 2140/10000 [11:42:31<48:05:47, 22.03s/it, loss_gram=1.92, loss_l2=0.00592, loss_lpips=0.104]\n",
      "Steps:  21%|██▏       | 2141/10000 [11:42:49<45:28:57, 20.83s/it, loss_gram=2.84, loss_l2=0.00741, loss_lpips=0.0925]\n",
      "Steps:  21%|██▏       | 2142/10000 [11:43:12<46:37:31, 21.36s/it, loss_gram=2.89, loss_l2=0.01, loss_lpips=0.111]    \n",
      "Steps:  21%|██▏       | 2143/10000 [11:43:29<43:45:15, 20.05s/it, loss_gram=0.766, loss_l2=0.00515, loss_lpips=0.156]\n",
      "Steps:  21%|██▏       | 2144/10000 [11:43:53<46:23:05, 21.26s/it, loss_gram=1.57, loss_l2=0.0056, loss_lpips=0.0602] \n",
      "Steps:  21%|██▏       | 2145/10000 [11:44:10<43:23:35, 19.89s/it, loss_gram=7.19, loss_l2=0.0147, loss_lpips=0.136] \n",
      "Steps:  21%|██▏       | 2146/10000 [11:44:33<45:52:36, 21.03s/it, loss_gram=0.291, loss_l2=0.00286, loss_lpips=0.0809]\n",
      "Steps:  21%|██▏       | 2147/10000 [11:44:51<43:31:18, 19.95s/it, loss_gram=6.69, loss_l2=0.0122, loss_lpips=0.0929]  \n",
      "Steps:  21%|██▏       | 2148/10000 [11:45:15<46:42:14, 21.41s/it, loss_gram=0.887, loss_l2=0.00525, loss_lpips=0.157]\n",
      "Steps:  21%|██▏       | 2149/10000 [11:45:32<43:26:53, 19.92s/it, loss_gram=3.03, loss_l2=0.00915, loss_lpips=0.259] \n",
      "Steps:  22%|██▏       | 2150/10000 [11:45:56<46:01:56, 21.11s/it, loss_gram=4.09, loss_l2=0.0436, loss_lpips=0.152] \n",
      "Steps:  22%|██▏       | 2151/10000 [11:46:13<43:24:52, 19.91s/it, loss_gram=1.2, loss_l2=0.0121, loss_lpips=0.0927]\n",
      "Steps:  22%|██▏       | 2152/10000 [11:46:38<46:35:19, 21.37s/it, loss_gram=7.97, loss_l2=0.0052, loss_lpips=0.117]\n",
      "Steps:  22%|██▏       | 2153/10000 [11:46:55<43:42:13, 20.05s/it, loss_gram=3.47, loss_l2=0.0107, loss_lpips=0.1]  \n",
      "Steps:  22%|██▏       | 2154/10000 [11:47:19<46:22:09, 21.28s/it, loss_gram=6.91, loss_l2=0.0441, loss_lpips=0.16]\n",
      "Steps:  22%|██▏       | 2155/10000 [11:47:38<44:51:39, 20.59s/it, loss_gram=11.6, loss_l2=0.00564, loss_lpips=0.0999]\n",
      "Steps:  22%|██▏       | 2156/10000 [11:48:02<47:05:45, 21.61s/it, loss_gram=2.48, loss_l2=0.0049, loss_lpips=0.11]   \n",
      "Steps:  22%|██▏       | 2157/10000 [11:48:18<43:46:45, 20.10s/it, loss_gram=6.56, loss_l2=0.0125, loss_lpips=0.0859]\n",
      "Steps:  22%|██▏       | 2158/10000 [11:48:41<45:33:49, 20.92s/it, loss_gram=5.88, loss_l2=0.0168, loss_lpips=0.142] \n",
      "Steps:  22%|██▏       | 2159/10000 [11:48:58<43:02:54, 19.76s/it, loss_gram=3.23, loss_l2=0.0148, loss_lpips=0.0984]\n",
      "Steps:  22%|██▏       | 2160/10000 [11:49:24<47:07:35, 21.64s/it, loss_gram=1.92, loss_l2=0.0049, loss_lpips=0.108] \n",
      "Steps:  22%|██▏       | 2161/10000 [11:49:41<43:53:36, 20.16s/it, loss_gram=7.16, loss_l2=0.00495, loss_lpips=0.0734]\n",
      "Steps:  22%|██▏       | 2162/10000 [11:50:06<46:44:55, 21.47s/it, loss_gram=1.86, loss_l2=0.006, loss_lpips=0.105]   \n",
      "Steps:  22%|██▏       | 2163/10000 [11:50:24<44:32:03, 20.46s/it, loss_gram=2.73, loss_l2=0.00572, loss_lpips=0.135]\n",
      "Steps:  22%|██▏       | 2164/10000 [11:50:49<47:37:11, 21.88s/it, loss_gram=2.19, loss_l2=0.0129, loss_lpips=0.12]  \n",
      "Steps:  22%|██▏       | 2165/10000 [11:51:05<43:52:49, 20.16s/it, loss_gram=2.39, loss_l2=0.00812, loss_lpips=0.136]\n",
      "Steps:  22%|██▏       | 2166/10000 [11:51:30<46:46:47, 21.50s/it, loss_gram=10, loss_l2=0.00879, loss_lpips=0.126]  \n",
      "Steps:  22%|██▏       | 2167/10000 [11:51:47<43:53:52, 20.18s/it, loss_gram=58.8, loss_l2=0.0386, loss_lpips=0.311]\n",
      "Steps:  22%|██▏       | 2168/10000 [11:52:12<47:14:02, 21.71s/it, loss_gram=10.6, loss_l2=0.0236, loss_lpips=0.188]\n",
      "Steps:  22%|██▏       | 2169/10000 [11:52:29<44:24:13, 20.41s/it, loss_gram=1.36, loss_l2=0.00742, loss_lpips=0.111]\n",
      "Steps:  22%|██▏       | 2170/10000 [11:52:55<48:04:32, 22.10s/it, loss_gram=2.34, loss_l2=0.0135, loss_lpips=0.114] \n",
      "Steps:  22%|██▏       | 2171/10000 [11:53:13<45:25:57, 20.89s/it, loss_gram=3.73, loss_l2=0.0272, loss_lpips=0.133]\n",
      "Steps:  22%|██▏       | 2172/10000 [11:53:37<47:16:04, 21.74s/it, loss_gram=10.6, loss_l2=0.00321, loss_lpips=0.108]\n",
      "Steps:  22%|██▏       | 2173/10000 [11:53:53<43:34:15, 20.04s/it, loss_gram=0.223, loss_l2=0.00173, loss_lpips=0.14]\n",
      "Steps:  22%|██▏       | 2174/10000 [11:54:18<46:35:36, 21.43s/it, loss_gram=1.12, loss_l2=0.00859, loss_lpips=0.094]\n",
      "Steps:  22%|██▏       | 2175/10000 [11:54:35<43:58:35, 20.23s/it, loss_gram=0.396, loss_l2=0.00229, loss_lpips=0.0847]\n",
      "Steps:  22%|██▏       | 2176/10000 [11:54:58<45:45:57, 21.06s/it, loss_gram=26.6, loss_l2=0.00717, loss_lpips=0.173]  \n",
      "Steps:  22%|██▏       | 2177/10000 [11:55:16<43:19:02, 19.93s/it, loss_gram=42, loss_l2=0.012, loss_lpips=0.108]    \n",
      "Steps:  22%|██▏       | 2178/10000 [11:55:40<45:52:10, 21.11s/it, loss_gram=1.89, loss_l2=0.0031, loss_lpips=0.116]\n",
      "Steps:  22%|██▏       | 2179/10000 [11:55:57<43:45:51, 20.14s/it, loss_gram=2.08, loss_l2=0.00962, loss_lpips=0.134]\n",
      "Steps:  22%|██▏       | 2180/10000 [11:56:21<45:52:51, 21.12s/it, loss_gram=0.809, loss_l2=0.00578, loss_lpips=0.153]\n",
      "Steps:  22%|██▏       | 2181/10000 [11:56:38<43:10:31, 19.88s/it, loss_gram=3.16, loss_l2=0.0106, loss_lpips=0.0914] \n",
      "Steps:  22%|██▏       | 2182/10000 [11:57:03<46:38:16, 21.48s/it, loss_gram=11.3, loss_l2=0.0115, loss_lpips=0.129] \n",
      "Steps:  22%|██▏       | 2183/10000 [11:57:20<43:52:49, 20.21s/it, loss_gram=1.73, loss_l2=0.00448, loss_lpips=0.0774]\n",
      "Steps:  22%|██▏       | 2184/10000 [11:57:44<45:54:00, 21.14s/it, loss_gram=0.672, loss_l2=0.0086, loss_lpips=0.112] \n",
      "Steps:  22%|██▏       | 2185/10000 [11:58:00<42:59:44, 19.81s/it, loss_gram=1.97, loss_l2=0.00701, loss_lpips=0.0892]\n",
      "Steps:  22%|██▏       | 2186/10000 [11:58:25<46:03:01, 21.22s/it, loss_gram=1.97, loss_l2=0.0028, loss_lpips=0.138]  \n",
      "Steps:  22%|██▏       | 2187/10000 [11:58:44<44:30:56, 20.51s/it, loss_gram=4.09, loss_l2=0.00656, loss_lpips=0.138]\n",
      "Steps:  22%|██▏       | 2188/10000 [11:59:10<47:56:12, 22.09s/it, loss_gram=7.44, loss_l2=0.0308, loss_lpips=0.214] \n",
      "Steps:  22%|██▏       | 2189/10000 [11:59:26<44:21:02, 20.44s/it, loss_gram=2.53, loss_l2=0.0142, loss_lpips=0.104]\n",
      "Steps:  22%|██▏       | 2190/10000 [11:59:54<48:58:06, 22.57s/it, loss_gram=2.16, loss_l2=0.00706, loss_lpips=0.0702]\n",
      "Steps:  22%|██▏       | 2191/10000 [12:00:06<42:26:53, 19.57s/it, loss_gram=3.16, loss_l2=0.00785, loss_lpips=0.0787]\n",
      "Steps:  22%|██▏       | 2192/10000 [12:00:43<46:14:40, 21.32s/it, loss_gram=1.05, loss_l2=0.00189, loss_lpips=0.16]  \n",
      "Steps:  22%|██▏       | 2193/10000 [12:04:24<144:40:15, 66.71s/it, loss_gram=0.719, loss_l2=0.00262, loss_lpips=0.0698]\n",
      "Steps:  22%|██▏       | 2194/10000 [12:07:30<246:53:03, 113.86s/it, loss_gram=30.8, loss_l2=0.0136, loss_lpips=0.113]   \n",
      "Steps:  22%|██▏       | 2195/10000 [12:10:52<309:20:14, 142.68s/it, loss_gram=0.355, loss_l2=0.00259, loss_lpips=0.0858]\n",
      "Steps:  22%|██▏       | 2196/10000 [12:15:14<380:26:37, 175.50s/it, loss_gram=4.81, loss_l2=0.00272, loss_lpips=0.0981] \n",
      "Steps:  22%|██▏       | 2197/10000 [12:18:16<387:31:40, 178.79s/it, loss_gram=1.07, loss_l2=0.0037, loss_lpips=0.0862] \n",
      "Steps:  22%|██▏       | 2198/10000 [12:21:26<403:06:37, 186.00s/it, loss_gram=1.15, loss_l2=0.00635, loss_lpips=0.108]\n",
      "Steps:  22%|██▏       | 2199/10000 [12:24:19<398:47:39, 184.04s/it, loss_gram=0.883, loss_l2=0.0012, loss_lpips=0.0798]\n",
      "Steps:  22%|██▏       | 2200/10000 [12:30:25<517:30:38, 238.85s/it, loss_gram=2.28, loss_l2=0.00707, loss_lpips=0.108] \n",
      "Steps:  22%|██▏       | 2201/10000 [12:33:28<471:44:45, 217.76s/it, loss_gram=0.516, loss_l2=0.0154, loss_lpips=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading viz images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps:  22%|██▏       | 2202/10000 [12:38:47<546:39:20, 252.37s/it, loss_gram=6.16, loss_l2=0.037, loss_lpips=0.123]  \n",
      "Steps:  22%|██▏       | 2203/10000 [12:38:55<387:31:12, 178.92s/it, loss_gram=20.5, loss_l2=0.0435, loss_lpips=0.236]\n",
      "Steps:  22%|██▏       | 2204/10000 [12:39:13<282:56:47, 130.66s/it, loss_gram=2.73, loss_l2=0.0117, loss_lpips=0.0969]\n",
      "Steps:  22%|██▏       | 2205/10000 [12:39:20<202:32:00, 93.54s/it, loss_gram=31.6, loss_l2=0.0265, loss_lpips=0.119]  \n",
      "Steps:  22%|██▏       | 2206/10000 [12:39:32<149:03:43, 68.85s/it, loss_gram=3.88, loss_l2=0.0189, loss_lpips=0.122]\n",
      "Steps:  22%|██▏       | 2207/10000 [12:39:45<113:41:03, 52.52s/it, loss_gram=3.31, loss_l2=0.0254, loss_lpips=0.134]\n",
      "Steps:  22%|██▏       | 2208/10000 [12:40:32<88:23:34, 40.84s/it, loss_gram=3.92, loss_l2=0.0149, loss_lpips=0.113] \n",
      "Steps:  22%|██▏       | 2209/10000 [12:44:28<236:50:52, 109.44s/it, loss_gram=0.151, loss_l2=0.0022, loss_lpips=0.14]\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "# start the training loop\n",
    "for epoch in tqdm(range(0, args.num_training_epochs)):\n",
    "    for step, batch in tqdm(enumerate(dl_train)):\n",
    "        l_acc = [net_difix]\n",
    "        with accelerator.accumulate(*l_acc):\n",
    "            x_src = batch[\"conditioning_pixel_values\"]\n",
    "            x_tgt = batch[\"output_pixel_values\"]\n",
    "            B, V, C, H, W = x_src.shape\n",
    "\n",
    "            # forward pass\n",
    "            x_tgt_pred = net_difix(x_src, prompt_tokens=batch[\"input_ids\"])       \n",
    "            \n",
    "            x_tgt = rearrange(x_tgt, 'b v c h w -> (b v) c h w')\n",
    "            x_tgt_pred = rearrange(x_tgt_pred, 'b v c h w -> (b v) c h w')\n",
    "                        \n",
    "            # Reconstruction loss\n",
    "            loss_l2 = F.mse_loss(x_tgt_pred.float(), x_tgt.float(), reduction=\"mean\") * args.lambda_l2\n",
    "            loss_lpips = net_lpips(x_tgt_pred.float(), x_tgt.float()).mean() * args.lambda_lpips\n",
    "            loss = loss_l2 + loss_lpips\n",
    "            \n",
    "            # Gram matrix loss\n",
    "            if args.lambda_gram > 0:\n",
    "                if global_step > args.gram_loss_warmup_steps:\n",
    "                    x_tgt_pred_renorm = t_vgg_renorm(x_tgt_pred * 0.5 + 0.5)\n",
    "                    crop_h, crop_w = 400, 400\n",
    "                    top, left = random.randint(0, H - crop_h), random.randint(0, W - crop_w)\n",
    "                    x_tgt_pred_renorm = crop(x_tgt_pred_renorm, top, left, crop_h, crop_w)\n",
    "                    \n",
    "                    x_tgt_renorm = t_vgg_renorm(x_tgt * 0.5 + 0.5)\n",
    "                    x_tgt_renorm = crop(x_tgt_renorm, top, left, crop_h, crop_w)\n",
    "                    \n",
    "                    loss_gram = gram_loss(x_tgt_pred_renorm.to(weight_dtype), x_tgt_renorm.to(weight_dtype), net_vgg) * args.lambda_gram\n",
    "                    loss += loss_gram\n",
    "                else:\n",
    "                    loss_gram = torch.tensor(0.0).to(weight_dtype)                    \n",
    "\n",
    "            accelerator.backward(loss, retain_graph=False)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(layers_to_opt, args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n",
    "            \n",
    "            x_tgt = rearrange(x_tgt, '(b v) c h w -> b v c h w', v=V)\n",
    "            x_tgt_pred = rearrange(x_tgt_pred, '(b v) c h w -> b v c h w', v=V)\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                logs = {}\n",
    "                # log all the losses\n",
    "                logs[\"loss_l2\"] = loss_l2.detach().item()\n",
    "                logs[\"loss_lpips\"] = loss_lpips.detach().item()\n",
    "                if args.lambda_gram > 0:\n",
    "                    logs[\"loss_gram\"] = loss_gram.detach().item()\n",
    "                progress_bar.set_postfix(**logs)\n",
    "\n",
    "                # viz some images\n",
    "                if global_step % args.viz_freq == 1:\n",
    "                    print('uploading viz images')\n",
    "                    log_dict = {\n",
    "                        \"train/source\": [wandb.Image(rearrange(x_src, \"b v c h w -> b c (v h) w\")[idx].float().detach().cpu(), caption=f\"idx={idx}\") for idx in range(B)],\n",
    "                        \"train/target\": [wandb.Image(rearrange(x_tgt, \"b v c h w -> b c (v h) w\")[idx].float().detach().cpu(), caption=f\"idx={idx}\") for idx in range(B)],\n",
    "                        \"train/model_output\": [wandb.Image(rearrange(x_tgt_pred, \"b v c h w -> b c (v h) w\")[idx].float().detach().cpu(), caption=f\"idx={idx}\") for idx in range(B)],\n",
    "                    }\n",
    "                    for k in log_dict:\n",
    "                        logs[k] = log_dict[k]\n",
    "\n",
    "                # checkpoint the model\n",
    "                if global_step % args.checkpointing_steps == 1:\n",
    "                    outf = os.path.join(args.output_dir, \"checkpoints\", f\"model_{global_step}.pkl\")\n",
    "                    # accelerator.unwrap_model(net_difix).save_model(outf)\n",
    "                    save_ckpt(accelerator.unwrap_model(net_difix), optimizer, outf)\n",
    "\n",
    "                # compute validation set L2, LPIPS\n",
    "                if args.eval_freq > 0 and global_step % args.eval_freq == 1:\n",
    "                    l_l2, l_lpips = [], []\n",
    "                    log_dict = {\"sample/source\": [], \"sample/target\": [], \"sample/model_output\": []}\n",
    "                    for step, batch_val in enumerate(dl_val):\n",
    "                        if step >= args.num_samples_eval:\n",
    "                            break\n",
    "                        x_src = batch_val[\"conditioning_pixel_values\"].to(accelerator.device, dtype=weight_dtype)\n",
    "                        x_tgt = batch_val[\"output_pixel_values\"].to(accelerator.device, dtype=weight_dtype)\n",
    "                        B, V, C, H, W = x_src.shape\n",
    "                        assert B == 1, \"Use batch size 1 for eval.\"\n",
    "                        with torch.no_grad():\n",
    "                            # forward pass\n",
    "                            x_tgt_pred = accelerator.unwrap_model(net_difix)(x_src, prompt_tokens=batch_val[\"input_ids\"].cuda())\n",
    "                            \n",
    "                            if step % 10 == 0:\n",
    "\n",
    "                                log_dict[\"sample/source\"].append(wandb.Image(rearrange(x_src, \"b v c h w -> b c (v h) w\")[0].float().detach().cpu(), caption=f\"idx={len(log_dict['sample/source'])}\"))\n",
    "                                log_dict[\"sample/target\"].append(wandb.Image(rearrange(x_tgt, \"b v c h w -> b c (v h) w\")[0].float().detach().cpu(), caption=f\"idx={len(log_dict['sample/source'])}\"))\n",
    "                                log_dict[\"sample/model_output\"].append(wandb.Image(rearrange(x_tgt_pred, \"b v c h w -> b c (v h) w\")[0].float().detach().cpu(), caption=f\"idx={len(log_dict['sample/source'])}\"))\n",
    "                            \n",
    "                            x_tgt = x_tgt[:, 0] # take the input view\n",
    "                            x_tgt_pred = x_tgt_pred[:, 0] # take the input view\n",
    "                            # compute the reconstruction losses\n",
    "                            loss_l2 = F.mse_loss(x_tgt_pred.float(), x_tgt.float(), reduction=\"mean\")\n",
    "                            loss_lpips = net_lpips(x_tgt_pred.float(), x_tgt.float()).mean()\n",
    "\n",
    "                            l_l2.append(loss_l2.item())\n",
    "                            l_lpips.append(loss_lpips.item())\n",
    "\n",
    "                    logs[\"val/l2\"] = np.mean(l_l2)\n",
    "                    logs[\"val/lpips\"] = np.mean(l_lpips)\n",
    "                    for k in log_dict:\n",
    "                        logs[k] = log_dict[k]\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                accelerator.log(logs, step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b793e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28835352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(net_lpips.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f40f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os, random, time, gc\n",
    "import numpy as np\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "from torchvision.transforms.functional import crop\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_losses(x_tgt_pred, x_tgt, args, net_lpips, weight_dtype):\n",
    "    loss_l2 = F.mse_loss(x_tgt_pred.float(), x_tgt.float(), reduction=\"mean\") * args.lambda_l2\n",
    "    loss_lpips = net_lpips(x_tgt_pred.float(), x_tgt.float()).mean() * args.lambda_lpips\n",
    "    total_loss = loss_l2 + loss_lpips\n",
    "    return total_loss, loss_l2, loss_lpips\n",
    "\n",
    "def compute_gram_loss(x_tgt_pred, x_tgt, args, global_step, t_vgg_renorm, net_vgg, weight_dtype):\n",
    "    if args.lambda_gram == 0 or global_step <= args.gram_loss_warmup_steps:\n",
    "        return torch.tensor(0.0, device=device, dtype=weight_dtype)\n",
    "\n",
    "    x_tgt_pred_renorm = t_vgg_renorm(x_tgt_pred * 0.5 + 0.5)\n",
    "    x_tgt_renorm = t_vgg_renorm(x_tgt * 0.5 + 0.5)\n",
    "\n",
    "    crop_h, crop_w = 400, 400\n",
    "    H, W = x_tgt_pred.shape[-2:]\n",
    "    top, left = random.randint(0, H - crop_h), random.randint(0, W - crop_w)\n",
    "    x_tgt_pred_renorm = crop(x_tgt_pred_renorm, top, left, crop_h, crop_w)\n",
    "    x_tgt_renorm = crop(x_tgt_renorm, top, left, crop_h, crop_w)\n",
    "\n",
    "    return gram_loss(x_tgt_pred_renorm.to(weight_dtype), x_tgt_renorm.to(weight_dtype), net_vgg) * args.lambda_gram\n",
    "\n",
    "def normalize_img_for_logging(img_tensor):\n",
    "    \"\"\"\n",
    "    Normalize an image tensor to [0, 255] and convert to uint8 for wandb.Image.\n",
    "    Handles input ranges of [-1, 1] or [0, 1]. Warns if out-of-range.\n",
    "    Assumes shape is (C, H, W).\n",
    "    \"\"\"\n",
    "    img = img_tensor.clone().detach().cpu().float()\n",
    "\n",
    "    min_val, max_val = img.min().item(), img.max().item()\n",
    "\n",
    "    if min_val >= -1.0 and max_val <= 1.0:\n",
    "        # Handle [-1, 1] range\n",
    "        img = ((img + 1) * 0.5).clamp(0, 1)\n",
    "    elif min_val >= 0.0 and max_val <= 1.0:\n",
    "        # Already in [0, 1]\n",
    "        img = img.clamp(0, 1)\n",
    "    else:\n",
    "        # Unexpected range\n",
    "        print(f\"[normalize_img_for_logging] ⚠️ Image values outside expected ranges: min={min_val}, max={max_val}. Clamping to [0, 1].\")\n",
    "        img = img.clamp(0, 1)\n",
    "\n",
    "    img = (img * 255).byte()\n",
    "    return img\n",
    "\n",
    "\n",
    "def log_images(x_src, x_tgt, x_tgt_pred, step, prefix=\"train\"):\n",
    "    B = x_src.size(0)\n",
    "\n",
    "    log_dict = {\n",
    "        f\"{prefix}/source\": [wandb.Image(normalize_img_for_logging(rearrange(x_src, \"b v c h w -> b c (v h) w\")[i]), caption=f\"idx={i}\") for i in range(B)],\n",
    "        f\"{prefix}/target\": [wandb.Image(normalize_img_for_logging(rearrange(x_tgt, \"b v c h w -> b c (v h) w\")[i]), caption=f\"idx={i}\") for i in range(B)],\n",
    "        f\"{prefix}/model_output\": [wandb.Image(normalize_img_for_logging(rearrange(x_tgt_pred, \"b v c h w -> b c (v h) w\")[i]), caption=f\"idx={i}\") for i in range(B)],\n",
    "    }\n",
    "\n",
    "    wandb.log(log_dict, step=step)\n",
    "\n",
    "    wandb.log(log_dict, step=step)\n",
    "\n",
    "def save_checkpoint(model, optimizer, path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(dl_val, model, args, net_lpips, weight_dtype, step):\n",
    "    model.eval()\n",
    "    l_l2, l_lpips = [], []\n",
    "    log_dict = {\"sample/source\": [], \"sample/target\": [], \"sample/model_output\": []}\n",
    "\n",
    "    for i, batch in enumerate(dl_val):\n",
    "        if i >= args.num_samples_eval:\n",
    "            break\n",
    "        x_src = batch[\"conditioning_pixel_values\"].to(device, dtype=weight_dtype)\n",
    "        x_tgt = batch[\"output_pixel_values\"].to(device, dtype=weight_dtype)\n",
    "        B, V, C, H, W = x_src.shape\n",
    "        assert B == 1, \"Use batch size 1 for eval.\"\n",
    "\n",
    "        x_tgt_pred = model(x_src, prompt_tokens=batch[\"input_ids\"].to(device))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            log_dict[\"sample/source\"].append(wandb.Image(rearrange(x_src, \"b v c h w -> b c (v h) w\")[0].float().cpu()))\n",
    "            log_dict[\"sample/target\"].append(wandb.Image(rearrange(x_tgt, \"b v c h w -> b c (v h) w\")[0].float().cpu()))\n",
    "            log_dict[\"sample/model_output\"].append(wandb.Image(rearrange(x_tgt_pred, \"b v c h w -> b c (v h) w\")[0].float().cpu()))\n",
    "\n",
    "        x_tgt = x_tgt[:, 0]\n",
    "        x_tgt_pred = x_tgt_pred[:, 0]\n",
    "        l2 = F.mse_loss(x_tgt_pred.float(), x_tgt.float(), reduction=\"mean\").item()\n",
    "        lpips = net_lpips(x_tgt_pred.float(), x_tgt.float()).mean().item()\n",
    "\n",
    "        l_l2.append(l2)\n",
    "        l_lpips.append(lpips)\n",
    "\n",
    "    logs = {\n",
    "        \"val/l2\": np.mean(l_l2),\n",
    "        \"val/lpips\": np.mean(l_lpips),\n",
    "        **log_dict\n",
    "    }\n",
    "    wandb.log(logs, step=step)\n",
    "    model.train()\n",
    "\n",
    "def train_one_epoch(dl_train, model, optimizer, scheduler, args, net_lpips, t_vgg_renorm, net_vgg, weight_dtype, global_step):\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(dl_train):\n",
    "        x_src = batch[\"conditioning_pixel_values\"].to(device)\n",
    "        x_tgt = batch[\"output_pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        B, V, C, H, W = x_src.shape\n",
    "\n",
    "        # Forward pass\n",
    "        x_tgt_pred = model(x_src, prompt_tokens=input_ids)\n",
    "\n",
    "        x_tgt = rearrange(x_tgt, 'b v c h w -> (b v) c h w')\n",
    "        x_tgt_pred = rearrange(x_tgt_pred, 'b v c h w -> (b v) c h w')\n",
    "\n",
    "        # Compute losses\n",
    "        loss, loss_l2, loss_lpips = compute_losses(x_tgt_pred, x_tgt, args, net_lpips, weight_dtype)\n",
    "        loss_gram = compute_gram_loss(x_tgt_pred, x_tgt, args, global_step, t_vgg_renorm, net_vgg, weight_dtype)\n",
    "        loss += loss_gram\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Logging\n",
    "        if global_step % args.viz_freq == 1:\n",
    "            log_images(x_src, rearrange(x_tgt, '(b v) c h w -> b v c h w', v=V), rearrange(x_tgt_pred, '(b v) c h w -> b v c h w', v=V), global_step)\n",
    "\n",
    "        if global_step % args.checkpointing_steps == 1:\n",
    "            ckpt_path = os.path.join(args.output_dir, \"checkpoints\", f\"model_{global_step}.pkl\")\n",
    "            save_checkpoint(model, optimizer, ckpt_path)\n",
    "\n",
    "        if args.eval_freq > 0 and global_step % args.eval_freq == 1:\n",
    "            evaluate(dl_val, model, args, net_lpips, weight_dtype, global_step)\n",
    "\n",
    "        wandb.log({\n",
    "            \"loss_l2\": loss_l2.item(),\n",
    "            \"loss_lpips\": loss_lpips.item(),\n",
    "            \"loss_gram\": loss_gram.item() if args.lambda_gram > 0 else 0.0\n",
    "        }, step=global_step)\n",
    "\n",
    "        global_step += 1\n",
    "    return global_step\n",
    "\n",
    "def train_loop(dl_train, dl_val, model, optimizer, scheduler, args, net_lpips, t_vgg_renorm, net_vgg, weight_dtype):\n",
    "    global_step = 0\n",
    "    for epoch in range(args.num_training_epochs):\n",
    "        global_step = train_one_epoch(dl_train, model, optimizer, scheduler, args, net_lpips, t_vgg_renorm, net_vgg, weight_dtype, global_step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difix_env_fixed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
