{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c16003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Go one level up\n",
    "parent_dir = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "\n",
    "# Add parent directory to Python path\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26cb527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import lpips\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import torchvision\n",
    "import transformers\n",
    "from torchvision.transforms.functional import crop\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from einops import rearrange\n",
    "\n",
    "import diffusers\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "import wandb\n",
    "\n",
    "from src.model import Difix, load_ckpt_from_state_dict, save_ckpt\n",
    "from src.dataset import PairedDataset\n",
    "from src.loss import gram_loss\n",
    "\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e6dc6",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d6e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"lambda_lpips\": 1.0,\n",
    "    \"lambda_l2\": 1.0,\n",
    "    \"lambda_gram\": 1.0,\n",
    "    \"gram_loss_warmup_steps\": 2000,\n",
    "    \"dataset_path\": \"data/converted_dataset_fixed.json\",\n",
    "    \"train_image_prep\": \"resized_crop_512\",\n",
    "    \"test_image_prep\": \"resized_crop_512\",\n",
    "    \"prompt\": None,\n",
    "    \"eval_freq\": 100,\n",
    "    \"num_samples_eval\": 100,\n",
    "    \"viz_freq\": 100,\n",
    "    \"tracker_project_name\": \"difix\",\n",
    "    \"tracker_run_name\": \"train\",\n",
    "    \"pretrained_model_name_or_path\": None,\n",
    "    \"revision\": None,\n",
    "    \"variant\": None,\n",
    "    \"tokenizer_name\": None,\n",
    "    \"lora_rank_vae\": 4,\n",
    "    \"timestep\": 199,\n",
    "    \"mv_unet\": False,\n",
    "    \"output_dir\": \"./outputs/difix/train\",\n",
    "    \"cache_dir\": None,\n",
    "    \"seed\": None,\n",
    "    \"resolution\": 512,\n",
    "    \"train_batch_size\": 1,\n",
    "    \"num_training_epochs\": 10,\n",
    "    \"max_train_steps\": 10000,\n",
    "    \"checkpointing_steps\": 500,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"lr_scheduler\": \"constant\",\n",
    "    \"lr_warmup_steps\": 500,\n",
    "    \"lr_num_cycles\": 1,\n",
    "    \"lr_power\": 1.0,\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.999,\n",
    "    \"adam_weight_decay\": 1e-2,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"allow_tf32\": False,\n",
    "    \"report_to\": \"wandb\",\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"enable_xformers_memory_efficient_attention\": True,\n",
    "    \"set_grads_to_none\": False,\n",
    "    \"resume\": None,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "args['output_dir'] = './outputs/difix/train'\n",
    "args['dataset_path'] = \"./data/converted_dataset_fixed.json\"\n",
    "args['max_train_steps'] = 10000 \n",
    "args['resolution'] = 512 \n",
    "args['learning_rate'] = 2e-5 \n",
    "args['train_batch_size'] = 1 \n",
    "args['dataloader_num_workers'] = 4 \n",
    "args['enable_xformers_memory_efficient_attention'] = True\n",
    "args['checkpointing_steps'] = 1000 \n",
    "args['eval_freq'] = 1000 \n",
    "args['viz_freq'] = 100 \n",
    "args['lambda_lpips'] = 1.0 \n",
    "args['lambda_l2'] = 1.0 \n",
    "args['lambda_gram'] =  1.0 \n",
    "args['gram_loss_warmup_steps'] = 2000\n",
    "args['report_to']= \"wandb\" \n",
    "args['tracker_project_name'] = \"difix\" \n",
    "args['tracker_run_name'] = \"train\" \n",
    "args['timestep'] = 199\n",
    "\n",
    "args = SimpleNamespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5e66b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with random weights\n",
      "==================================================\n",
      "Number of trainable parameters in UNet: 865.91M\n",
      "Number of trainable parameters in VAE: 0.52M\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "transformers.utils.logging.set_verbosity_error()\n",
    "diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "\n",
    "os.makedirs(os.path.join(args.output_dir, \"checkpoints\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(args.output_dir, \"eval\"), exist_ok=True)\n",
    "\n",
    "### Set Up Model ###\n",
    "net_difix = Difix(\n",
    "    lora_rank_vae=args.lora_rank_vae, \n",
    "    timestep=args.timestep,\n",
    "    mv_unet=args.mv_unet,\n",
    ")\n",
    "net_difix.set_train()\n",
    "\n",
    "if args.enable_xformers_memory_efficient_attention:\n",
    "    if is_xformers_available():\n",
    "        net_difix.unet.enable_xformers_memory_efficient_attention()\n",
    "    else:\n",
    "        raise ValueError(\"xformers is not available, please install it by running `pip install xformers`\")\n",
    "\n",
    "if args.gradient_checkpointing:\n",
    "    net_difix.unet.enable_gradient_checkpointing()\n",
    "\n",
    "if args.allow_tf32:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27fb44a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "### Set up metrics\n",
    "net_lpips = lpips.LPIPS(net='vgg').cuda()\n",
    "\n",
    "net_lpips.requires_grad_(False)\n",
    "\n",
    "net_vgg = torchvision.models.vgg16(pretrained=True).features\n",
    "for param in net_vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3cecf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cx24957/miniconda3/envs/difix_env_fixed/lib/python3.10/site-packages/torch/_compile.py:51: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#### make the optimizer\n",
    "layers_to_opt = []\n",
    "layers_to_opt += list(net_difix.unet.parameters())\n",
    "\n",
    "for n, _p in net_difix.vae.named_parameters():\n",
    "    if \"lora\" in n and \"vae_skip\" in n:\n",
    "        assert _p.requires_grad\n",
    "        layers_to_opt.append(_p)\n",
    "layers_to_opt = layers_to_opt + list(net_difix.vae.decoder.skip_conv_1.parameters()) + \\\n",
    "    list(net_difix.vae.decoder.skip_conv_2.parameters()) + \\\n",
    "    list(net_difix.vae.decoder.skip_conv_3.parameters()) + \\\n",
    "    list(net_difix.vae.decoder.skip_conv_4.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(layers_to_opt, lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,)\n",
    "lr_scheduler = get_scheduler(args.lr_scheduler, optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    "    num_cycles=args.lr_num_cycles, power=args.lr_power,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "386ce608",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up dataset\n",
    "dataset_train = PairedDataset(dataset_path='/mnt/e/Difix3d/data/converted_dataset_fixed.json',\n",
    "                              height=512,\n",
    "                              width=512,\n",
    "                              split=\"train\", tokenizer=net_difix.tokenizer)\n",
    "dl_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                       batch_size=args.train_batch_size,\n",
    "                                       shuffle=True, num_workers=args.dataloader_num_workers)\n",
    "dataset_val = PairedDataset(dataset_path='/mnt/e/Difix3d/data/converted_dataset_fixed.json',\n",
    "                            height=512,\n",
    "                            width=512,\n",
    "                            split=\"test\", tokenizer=net_difix.tokenizer)\n",
    "random.Random(42).shuffle(dataset_val.img_ids)\n",
    "dl_val = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25508187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]['output_pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40170aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "### Set up components on accelerator\n",
    "weight_dtype = torch.float32\n",
    "# if accelerator.mixed_precision == \"fp16\":\n",
    "#     weight_dtype = torch.float16\n",
    "# elif accelerator.mixed_precision == \"bf16\":\n",
    "#     weight_dtype = torch.bfloat16\n",
    "\n",
    "# Move al networksr to device and cast to weight_dtype\n",
    "net_difix.to(device, dtype=weight_dtype)\n",
    "net_lpips.to(device, dtype=weight_dtype)\n",
    "net_vgg.to(device, dtype=weight_dtype)\n",
    "\n",
    "# renorm with image net statistics\n",
    "t_vgg_renorm =  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71f40f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os, random, time, gc\n",
    "import numpy as np\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "from torchvision.transforms.functional import crop\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_losses(x_tgt_pred, x_tgt, args, net_lpips, weight_dtype):\n",
    "    print('l2')\n",
    "    print(x_tgt_pred.float().shape)\n",
    "    loss_l2 = F.mse_loss(x_tgt_pred.float(), x_tgt.float(), reduction=\"mean\") * args.lambda_l2\n",
    "    print('lpips')\n",
    "    loss_lpips = net_lpips(x_tgt_pred.float(), x_tgt.float()).mean() * args.lambda_lpips\n",
    "    total_loss = loss_l2 + loss_lpips\n",
    "    return total_loss, loss_l2, loss_lpips\n",
    "\n",
    "def compute_gram_loss(x_tgt_pred, x_tgt, args, global_step, t_vgg_renorm, net_vgg, weight_dtype):\n",
    "    if args.lambda_gram == 0 or global_step <= args.gram_loss_warmup_steps:\n",
    "        return torch.tensor(0.0, device=device, dtype=weight_dtype)\n",
    "\n",
    "    x_tgt_pred_renorm = t_vgg_renorm(x_tgt_pred * 0.5 + 0.5)\n",
    "    x_tgt_renorm = t_vgg_renorm(x_tgt * 0.5 + 0.5)\n",
    "\n",
    "    crop_h, crop_w = 400, 400\n",
    "    H, W = x_tgt_pred.shape[-2:]\n",
    "    top, left = random.randint(0, H - crop_h), random.randint(0, W - crop_w)\n",
    "    x_tgt_pred_renorm = crop(x_tgt_pred_renorm, top, left, crop_h, crop_w)\n",
    "    x_tgt_renorm = crop(x_tgt_renorm, top, left, crop_h, crop_w)\n",
    "\n",
    "    return gram_loss(x_tgt_pred_renorm.to(weight_dtype), x_tgt_renorm.to(weight_dtype), net_vgg) * args.lambda_gram\n",
    "\n",
    "def normalize_img_for_logging(img_tensor):\n",
    "    \"\"\"\n",
    "    Normalize an image tensor to [0, 255] and convert to uint8 for wandb.Image.\n",
    "    Handles input ranges of [-1, 1] or [0, 1]. Warns if out-of-range.\n",
    "    Assumes shape is (C, H, W).\n",
    "    \"\"\"\n",
    "    img = img_tensor.clone().detach().cpu().float()\n",
    "\n",
    "    min_val, max_val = img.min().item(), img.max().item()\n",
    "\n",
    "    if min_val >= -1.0 and max_val <= 1.0:\n",
    "        # Handle [-1, 1] range\n",
    "        img = ((img + 1) * 0.5).clamp(0, 1)\n",
    "    elif min_val >= 0.0 and max_val <= 1.0:\n",
    "        # Already in [0, 1]\n",
    "        img = img.clamp(0, 1)\n",
    "    else:\n",
    "        # Unexpected range\n",
    "        print(f\"[normalize_img_for_logging] ⚠️ Image values outside expected ranges: min={min_val}, max={max_val}. Clamping to [0, 1].\")\n",
    "        img = img.clamp(0, 1)\n",
    "\n",
    "    img = (img * 255).byte()\n",
    "    return img\n",
    "\n",
    "\n",
    "def log_images(x_src, x_tgt, x_tgt_pred, step, prefix=\"train\"):\n",
    "    B = x_src.size(0)\n",
    "\n",
    "    log_dict = {\n",
    "        f\"{prefix}/source\": [wandb.Image(normalize_img_for_logging(rearrange(x_src, \"b v c h w -> b c (v h) w\")[i]), caption=f\"idx={i}\") for i in range(B)],\n",
    "        f\"{prefix}/target\": [wandb.Image(normalize_img_for_logging(rearrange(x_tgt, \"b v c h w -> b c (v h) w\")[i]), caption=f\"idx={i}\") for i in range(B)],\n",
    "        f\"{prefix}/model_output\": [wandb.Image(normalize_img_for_logging(rearrange(x_tgt_pred, \"b v c h w -> b c (v h) w\")[i]), caption=f\"idx={i}\") for i in range(B)],\n",
    "    }\n",
    "\n",
    "    wandb.log(log_dict, step=step)\n",
    "\n",
    "    wandb.log(log_dict, step=step)\n",
    "\n",
    "def save_checkpoint(model, optimizer, path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(dl_val, model, args, net_lpips, weight_dtype, step):\n",
    "    model.eval()\n",
    "    l_l2, l_lpips = [], []\n",
    "    log_dict = {\"sample/source\": [], \"sample/target\": [], \"sample/model_output\": []}\n",
    "\n",
    "    for i, batch in enumerate(dl_val):\n",
    "        if i >= args.num_samples_eval:\n",
    "            break\n",
    "        x_src = batch[\"conditioning_pixel_values\"].to(device, dtype=weight_dtype)\n",
    "        x_tgt = batch[\"output_pixel_values\"].to(device, dtype=weight_dtype)\n",
    "        B, V, C, H, W = x_src.shape\n",
    "        assert B == 1, \"Use batch size 1 for eval.\"\n",
    "\n",
    "        x_tgt_pred = model(x_src, prompt_tokens=batch[\"input_ids\"].to(device))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            log_dict[\"sample/source\"].append(wandb.Image(rearrange(x_src, \"b v c h w -> b c (v h) w\")[0].float().cpu()))\n",
    "            log_dict[\"sample/target\"].append(wandb.Image(rearrange(x_tgt, \"b v c h w -> b c (v h) w\")[0].float().cpu()))\n",
    "            log_dict[\"sample/model_output\"].append(wandb.Image(rearrange(x_tgt_pred, \"b v c h w -> b c (v h) w\")[0].float().cpu()))\n",
    "\n",
    "        x_tgt = x_tgt[:, 0]\n",
    "        x_tgt_pred = x_tgt_pred[:, 0]\n",
    "        l2 = F.mse_loss(x_tgt_pred.float(), x_tgt.float(), reduction=\"mean\").item()\n",
    "        lpips = net_lpips(x_tgt_pred.float(), x_tgt.float()).mean().item()\n",
    "\n",
    "        l_l2.append(l2)\n",
    "        l_lpips.append(lpips)\n",
    "\n",
    "    logs = {\n",
    "        \"val/l2\": np.mean(l_l2),\n",
    "        \"val/lpips\": np.mean(l_lpips),\n",
    "        **log_dict\n",
    "    }\n",
    "    wandb.log(logs, step=step)\n",
    "    model.train()\n",
    "\n",
    "def train_one_epoch(dl_train, model, optimizer, scheduler, args, net_lpips, t_vgg_renorm, net_vgg, weight_dtype, global_step):\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in tqdm(enumerate(dl_train)):\n",
    "        x_src = batch[\"conditioning_pixel_values\"].to(device)\n",
    "        x_tgt = batch[\"output_pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        B, V, C, H, W = x_src.shape\n",
    "\n",
    "        # Forward pass\n",
    "        x_tgt_pred = model(x_src, prompt_tokens=input_ids)\n",
    "\n",
    "        x_tgt = rearrange(x_tgt, 'b v c h w -> (b v) c h w')\n",
    "        x_tgt_pred = rearrange(x_tgt_pred, 'b v c h w -> (b v) c h w')\n",
    "\n",
    "        # Compute losses\n",
    "        loss, loss_l2, loss_lpips = compute_losses(x_tgt_pred, x_tgt, args, net_lpips, weight_dtype)\n",
    "        loss_gram = compute_gram_loss(x_tgt_pred, x_tgt, args, global_step, t_vgg_renorm, net_vgg, weight_dtype)\n",
    "        loss += loss_gram\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Logging\n",
    "        if global_step % args.viz_freq == 1:\n",
    "            log_images(x_src, rearrange(x_tgt, '(b v) c h w -> b v c h w', v=V), rearrange(x_tgt_pred, '(b v) c h w -> b v c h w', v=V), global_step)\n",
    "\n",
    "        if global_step % args.checkpointing_steps == 1:\n",
    "            ckpt_path = os.path.join(args.output_dir, \"checkpoints\", f\"model_{global_step}.pkl\")\n",
    "            save_checkpoint(model, optimizer, ckpt_path)\n",
    "\n",
    "        if args.eval_freq > 0 and global_step % args.eval_freq == 1:\n",
    "            evaluate(dl_val, model, args, net_lpips, weight_dtype, global_step)\n",
    "\n",
    "        wandb.log({\n",
    "            \"loss_l2\": loss_l2.item(),\n",
    "            \"loss_lpips\": loss_lpips.item(),\n",
    "            \"loss_gram\": loss_gram.item() if args.lambda_gram > 0 else 0.0\n",
    "        }, step=global_step)\n",
    "\n",
    "        global_step += 1\n",
    "    return global_step\n",
    "\n",
    "def train_loop(dl_train, dl_val, model, optimizer, scheduler, args, net_lpips, t_vgg_renorm, net_vgg, weight_dtype):\n",
    "    global_step = 0\n",
    "    for epoch in tqdm(range(args.num_training_epochs)):\n",
    "        global_step = train_one_epoch(dl_train, model, optimizer, scheduler, args, net_lpips, t_vgg_renorm, net_vgg, weight_dtype, global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74997231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_loop(dl_train,\n",
    "           dl_val,\n",
    "           net_difix,\n",
    "           optimizer,\n",
    "           lr_scheduler,\n",
    "           args, \n",
    "           net_lpips, \n",
    "           t_vgg_renorm, \n",
    "           net_vgg, \n",
    "           weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bde22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dl_train, model, optimizer, scheduler, args, net_lpips, t_vgg_renorm, net_vgg, weight_dtype, global_step):\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in tqdm(enumerate(dl_train)):\n",
    "        print('loading data to device')\n",
    "        x_src = batch[\"conditioning_pixel_values\"].to(device)\n",
    "        x_tgt = batch[\"output_pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        B, V, C, H, W = x_src.shape\n",
    "\n",
    "        # Forward pass\n",
    "        print('forward pass')\n",
    "        x_tgt_pred = model(x_src, prompt_tokens=input_ids)\n",
    "\n",
    "        print('rearranging')\n",
    "        x_tgt = rearrange(x_tgt, 'b v c h w -> (b v) c h w')\n",
    "        x_tgt_pred = rearrange(x_tgt_pred, 'b v c h w -> (b v) c h w')\n",
    "\n",
    "        # Compute losses\n",
    "        print('computing first losses')\n",
    "        loss, loss_l2, loss_lpips = compute_losses(x_tgt_pred, x_tgt, args, net_lpips, weight_dtype)\n",
    "        print('computing gram loss')\n",
    "        loss_gram = compute_gram_loss(x_tgt_pred, x_tgt, args, global_step, t_vgg_renorm, net_vgg, weight_dtype)\n",
    "        loss += loss_gram\n",
    "\n",
    "        # Backward\n",
    "        print('gradient comp')\n",
    "        optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # # Logging\n",
    "        # if global_step % args.viz_freq == 1:\n",
    "        #     log_images(x_src, rearrange(x_tgt, '(b v) c h w -> b v c h w', v=V), rearrange(x_tgt_pred, '(b v) c h w -> b v c h w', v=V), global_step)\n",
    "\n",
    "        # if global_step % args.checkpointing_steps == 1:\n",
    "        #     ckpt_path = os.path.join(args.output_dir, \"checkpoints\", f\"model_{global_step}.pkl\")\n",
    "        #     save_checkpoint(model, optimizer, ckpt_path)\n",
    "\n",
    "        # if args.eval_freq > 0 and global_step % args.eval_freq == 1:\n",
    "        #     evaluate(dl_val, model, args, net_lpips, weight_dtype, global_step)\n",
    "\n",
    "        # wandb.log({\n",
    "        #     \"loss_l2\": loss_l2.item(),\n",
    "        #     \"loss_lpips\": loss_lpips.item(),\n",
    "        #     \"loss_gram\": loss_gram.item() if args.lambda_gram > 0 else 0.0\n",
    "        # }, step=global_step)\n",
    "\n",
    "        global_step += 1\n",
    "    return global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a8fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data to device\n",
      "forward pass\n",
      "rearranging\n",
      "computing first losses\n",
      "l2\n",
      "torch.Size([2, 3, 512, 512])\n",
      "lpips\n",
      "computing gram loss\n",
      "gradient comp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:13, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data to device\n",
      "forward pass\n",
      "rearranging\n",
      "computing first losses\n",
      "l2\n",
      "torch.Size([2, 3, 512, 512])\n",
      "lpips\n",
      "computing gram loss\n"
     ]
    }
   ],
   "source": [
    "train_one_epoch(dl_train, \n",
    "                net_difix, \n",
    "                optimizer, \n",
    "                lr_scheduler, \n",
    "                args, \n",
    "                net_lpips, t_vgg_renorm, net_vgg, weight_dtype, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112bd96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difix_env_fixed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
